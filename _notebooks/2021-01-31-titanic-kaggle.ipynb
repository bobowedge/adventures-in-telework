{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggling into an Iceberg\n",
    "> Using random forest techniques to predict survival on the Titanic\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Matt Bowen\n",
    "- categories: [jupyter]\n",
    "- comments: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The next lesson in the [fastai course](https://course.fast.ai/) covered tabular data. According to the course, the vast majority of datasets on tabular data are best modeled by ensembles of decision trees, such as random forests, so the lesson focused on them. \n",
    "\n",
    "They used a particular Kaggle competition, the [Blue Book for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers) as their hands-on example for walking through how to understand, build, train, and infer using decision trees and random forests.\n",
    "\n",
    "While I had heard of Kaggle previously, I had never used it myself before. Fortunately, Kaggle itself points you to the [Titanic](https://www.kaggle.com/c/titanic) competition as a starting point. The premise is that you're given a dataset of passengers on the Titanic and whether or not they survived to train on and you have to predict which passengers in another dataset of passengers survived. \n",
    "\n",
    "As my introduction to both Kaggle and random forests, I thought I would try to walk through the course's random forest discussion using the Titanic competition data and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In the collapse below is all the imports and pandas settings I'll end up using at the top just to keep them all together.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "# Pandas display options\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Random Guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition provides three files: \n",
    " - training set (passenger data with 'Survived' column)\n",
    " - test set (passenger data with no 'Survived' column)\n",
    " - sample submission assuming all and only female passengers survived\n",
    " \n",
    "Since I'll use it often, I'll set up the dependent variable ('Survived') here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'Survived'\n",
    "train_df = pd.read_csv(\"titanic_train.csv\", low_memory=False)\n",
    "test_df = pd.read_csv(\"titanic_test.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>...</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>\n",
       "      <td>...</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>...</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>...</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>...</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>...</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>...</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>...</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>...</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>...</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                    Name  ...  \\\n",
       "0                                Braund, Mr. Owen Harris  ...   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Thayer)  ...   \n",
       "2                                 Heikkinen, Miss. Laina  ...   \n",
       "3           Futrelle, Mrs. Jacques Heath (Lily May Peel)  ...   \n",
       "4                               Allen, Mr. William Henry  ...   \n",
       "..                                                   ...  ...   \n",
       "886                                Montvila, Rev. Juozas  ...   \n",
       "887                         Graham, Miss. Margaret Edith  ...   \n",
       "888             Johnston, Miss. Catherine Helen \"Carrie\"  ...   \n",
       "889                                Behr, Mr. Karl Howell  ...   \n",
       "890                                  Dooley, Mr. Patrick  ...   \n",
       "\n",
       "               Ticket     Fare  Cabin  Embarked  \n",
       "0           A/5 21171   7.2500    NaN         S  \n",
       "1            PC 17599  71.2833    C85         C  \n",
       "2    STON/O2. 3101282   7.9250    NaN         S  \n",
       "3              113803  53.1000   C123         S  \n",
       "4              373450   8.0500    NaN         S  \n",
       "..                ...      ...    ...       ...  \n",
       "886            211536  13.0000    NaN         S  \n",
       "887            112053  30.0000    B42         S  \n",
       "888        W./C. 6607  23.4500    NaN         S  \n",
       "889            111369  30.0000   C148         C  \n",
       "890            370376   7.7500    NaN         Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition page has a description of each of the columns, so I won't rehash those here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first pass to give something to compare against, I built a randomly chosen set where the survival was determined at random based on the survival rate of the training data. The score for this random set ended up being 52.39%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Submit random set\n",
    "total_died, total_survived = train_df.value_counts(dep_var)\n",
    "ratio = total_died / (total_survived + total_died)\n",
    "test_length = len(test_df)\n",
    "random_survived = np.random.choice(2, test_length, p=[ratio, 1 - ratio])\n",
    "random_data_frame = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : random_survived})\n",
    "random_data_frame.to_csv(\"titanic_random.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_random.csv -m \"Random based on survival rate\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.52392\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"Score: {0.52392}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Now, it's time to do something other than just guess. I started by splitting the categorical and continuous columns using a `fastai` designed specifically for that. Then, I split the provided training data into training and validation sets using `TabularPandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous columns: ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "Categorical columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Split the columns into categorical and continuous\n",
    "cont_cols, cat_cols = cont_cat_split(train_df, 1, dep_var=dep_var)\n",
    "print(f\"Continuous columns: {cont_cols}\")\n",
    "print(f\"Categorical columns: {cat_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713, 178)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# Operations for columns\n",
    "column_ops = [Categorify, FillMissing]\n",
    "# Split the training data randomly\n",
    "splits = RandomSplitter()(range_of(train_df))\n",
    "tab_panda = TabularPandas(train_df, column_ops ,cat_cols, cont_cols, y_names=dep_var, splits=splits)\n",
    "trainxs, trainy = tab_panda.train.xs, tab_panda.train.y\n",
    "validxs, validy = tab_panda.valid.xs, tab_panda.valid.y\n",
    "len(trainxs), len(validxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_na</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Gheorgheff, Mr. Stanio</td>\n",
       "      <td>male</td>\n",
       "      <td>349254</td>\n",
       "      <td>#na#</td>\n",
       "      <td>C</td>\n",
       "      <td>True</td>\n",
       "      <td>421</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Gustafsson, Mr. Johan Birger</td>\n",
       "      <td>male</td>\n",
       "      <td>3101277</td>\n",
       "      <td>#na#</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>393</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Pengelly, Mr. Frederick William</td>\n",
       "      <td>male</td>\n",
       "      <td>28665</td>\n",
       "      <td>#na#</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tab_panda.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lesson used a `DecisionTreeRegressor`, but since 'Survived' is a categorical and not continuous variable, I used `DecisionTreeClassifier` instead. For the same reason, `zero_one_loss` makes more sense to be the loss function rather than using the mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Simple decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(trainxs, trainy)\n",
    "print(f\"Error: {zero_one_loss(model.predict(validxs), validy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.2415730337078652\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"Error: 0.2415730337078652\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, this is saying the model got ~24% of the validation passenger set wrong when predicting their survival.\n",
    "\n",
    "In the default, the model can split a node as long as there is at least 1 sample for each leaf. Following the lesson, I checked to see how many leaves for my model were created vs. how many rows in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 713)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_n_leaves(), len(trainxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Titanic dataset is much, much smaller than the Blue Book of Bulldozers dataset, but this number of leaves seemed reasonable to me (6 per leaf or so). However, because it was easy to do so, I decided to iterate the `min_samples_leaf` parameter to try to get a better model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# min_samples_leaf optimization\n",
    "best_error, best_min_samples, best_model = (None, None, None)\n",
    "for min_samples in range(1, 21):\n",
    "    model = DecisionTreeClassifier(min_samples_leaf=min_samples)\n",
    "    model.fit(trainxs, trainy)\n",
    "    error = zero_one_loss(model.predict(validxs), validy)\n",
    "    if best_error is None or best_error > error:\n",
    "        best_error, best_min_samples, best_model = (error,  min_samples, model)\n",
    "print(f\"Best model error: {best_error}\")\n",
    "print(f\"Min samples per leaf: {best_min_samples}, Number leaves: {best_model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model error: 0.1966292134831461\n",
      "Min samples per leaf: 3, Number leaves: 70\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Best model error: {0.1966292134831461}\")\n",
    "print(f\"Min samples per leaf: {3}, Number leaves: {70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default minimum samples per leaf is 1 for DecisionTreeClassifier, but it seems the optimization helped. A minimum of 3 samples per leaf yields a better result on the validation, so I'll use that model to submit to kaggle.\n",
    "\n",
    "### Data Cleanup\n",
    "\n",
    "Before I could predict using that model, there is an issue with the test data that had to be addressed. The 'Fare' column had a null value for one of the test passengers, while neither the test or valid set had any null values in that column. \n",
    "\n",
    "A standard way to fill in that value is to use the mean of the training set, so that's what I did. Pandas makes this substitution pretty easy, once you figure out the proper syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1044</td>\n",
       "      <td>3</td>\n",
       "      <td>Storey, Mr. Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>3701</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                Name   Sex  ...  Ticket       Fare  \\\n",
       "152         1044       3  Storey, Mr. Thomas  male  ...    3701  32.204208   \n",
       "\n",
       "     Cabin Embarked  \n",
       "152    NaN        S  \n",
       "\n",
       "[1 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Find the null row(s) in the 'Fare' column\n",
    "nan_row = test_df[test_df['Fare'].isnull()]\n",
    "# Set the fare in that row to the mean of the training data\n",
    "test_df.at[nan_row.index, 'Fare'] = train_df['Fare'].mean()\n",
    "# Display the newly fixed row\n",
    "display(test_df.loc[nan_row.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the row fixed, I converted the test data to a `TabularPandas`. We'll use this again and again to feed into our models for survival prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Convert test data to TabularPandas\n",
    "test_tab = TabularPandas(test_df, [Categorify, FillMissing], cat_cols, cont_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Results\n",
    "\n",
    "With the test data cleaned up, I used the best decision tree model that I found to predict the survivors on the test set and submit the results to kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Decision Tree Classifier set (Score: 0.70813)\n",
    "# Predict the survivors\n",
    "dt_survived = best_model.predict(test_tab.xs)\n",
    "\n",
    "test_length = len(test_df)\n",
    "dt_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : dt_survived})\n",
    "dt_df.to_csv(\"titanic_dt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_dt.csv -m \"Single decision tree\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.70813\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"Score: {0.70813}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "If one decision tree is good, how about more? Next, I'll walk through the random forests of decision trees that create to tackle this competition.\n",
    "\n",
    "Following the course approach, I made a function to make creating a random forest and fit it in single step. Similar to above, since I'm looking for a yes or no answer, instead of a continuous one, I'll use `RandomForestClassifier` instead of a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# create random forest and fit it\n",
    "def create_rf(xs, y, n_estimators=40, max_features=0.5, min_samples_leaf=5, **kwargs):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, max_features=max_features, \n",
    "                                  min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that function, I can create a random forest, fit it to the training data, and score it against the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# First pass model with error\n",
    "# Model error: 0.1797752808988764\n",
    "# Model oob error: 0.18513323983169705\n",
    "model = create_rf(trainxs, trainy)\n",
    "error = zero_one_loss(model.predict(validxs), validy)\n",
    "print(f\"Model error: {error}\")\n",
    "print(f\"Model oob error: {1.0 - model.oob_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model error: 0.1797752808988764\n",
      "Model oob error: 0.18513323983169705\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Model error: {0.1797752808988764}\")\n",
    "print(f\"Model oob error: {0.18513323983169705}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If 40 trees (`n_estimators`) is good, how about [more](https://imgflip.com/i/4u7pgw)? \n",
    "\n",
    "Let's look at models with more trees (`n_estimators`) and see how they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Number of trees (n_estimators) optimization\n",
    "best_error, best_trees, best_model = (None, None, None)\n",
    "for num_trees in range(40, 300, 20):\n",
    "    model = create_rf(trainxs, trainy, n_estimators=num_trees)\n",
    "    error = zero_one_loss(model.predict(validxs), validy)\n",
    "    if best_error is None or best_error > error:\n",
    "        best_error, best_trees, best_model = (error, num_trees, model)\n",
    "    print(f\"Num trees: {num_trees}, error: {error}, oob: {1.0 - model.oob_score_}\")\n",
    "print(f\"Best model error: {best_error}, num trees: {best_trees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num trees: 40, error: 0.1797752808988764, oob: 0.18513323983169705\n",
      "Num trees: 60, error: 0.1685393258426966, oob: 0.18793828892005615\n",
      "Num trees: 80, error: 0.1741573033707865, oob: 0.1893408134642356\n",
      "Num trees: 100, error: 0.1797752808988764, oob: 0.1837307152875175\n",
      "Num trees: 120, error: 0.1629213483146067, oob: 0.18793828892005615\n",
      "Num trees: 140, error: 0.1685393258426966, oob: 0.1837307152875175\n",
      "Num trees: 160, error: 0.1685393258426966, oob: 0.1865357643758766\n",
      "Num trees: 180, error: 0.1685393258426966, oob: 0.1865357643758766\n",
      "Num trees: 200, error: 0.1629213483146067, oob: 0.1893408134642356\n",
      "Num trees: 220, error: 0.1629213483146067, oob: 0.18513323983169705\n",
      "Num trees: 240, error: 0.1685393258426966, oob: 0.1837307152875175\n",
      "Num trees: 260, error: 0.1685393258426966, oob: 0.18232819074333806\n",
      "Num trees: 280, error: 0.1741573033707865, oob: 0.1781206171107994\n",
      "Best model error: 0.1629213483146067, num trees: 120\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print('''Num trees: 40, error: 0.1797752808988764, oob: 0.18513323983169705\n",
    "Num trees: 60, error: 0.1685393258426966, oob: 0.18793828892005615\n",
    "Num trees: 80, error: 0.1741573033707865, oob: 0.1893408134642356\n",
    "Num trees: 100, error: 0.1797752808988764, oob: 0.1837307152875175\n",
    "Num trees: 120, error: 0.1629213483146067, oob: 0.18793828892005615\n",
    "Num trees: 140, error: 0.1685393258426966, oob: 0.1837307152875175\n",
    "Num trees: 160, error: 0.1685393258426966, oob: 0.1865357643758766\n",
    "Num trees: 180, error: 0.1685393258426966, oob: 0.1865357643758766\n",
    "Num trees: 200, error: 0.1629213483146067, oob: 0.1893408134642356\n",
    "Num trees: 220, error: 0.1629213483146067, oob: 0.18513323983169705\n",
    "Num trees: 240, error: 0.1685393258426966, oob: 0.1837307152875175\n",
    "Num trees: 260, error: 0.1685393258426966, oob: 0.18232819074333806\n",
    "Num trees: 280, error: 0.1741573033707865, oob: 0.1781206171107994\n",
    "Best model error: 0.1629213483146067, num trees: 120''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with 120 (or 200 or 220) trees seems to fare best, but all that much better than the model with 40 trees. I'm no expert, I think it is due to the \n",
    "[*out-of-bag error*](https://en.wikipedia.org/wiki/Out-of-bag_error). For all of the models, the `oob error` is more than the validation error. That would seem to indicate that the model can't really improve with more adding more trees, but could just be better by random chance.\n",
    "\n",
    "That said, since they are easy to create and the submission rules for the Titanic competition are generous, I submitted the predictions from the 40-tree, 120-tree, 140-tree, 160-tree, and 220-tree models just to see how they would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# A few models with different numbers of trees\n",
    "for num_trees in [40, 120, 140, 160, 220]:\n",
    "    model = create_rf(trainxs, trainy, n_estimators=num_trees)\n",
    "    rf_survived = model.predict(test_tab.xs)\n",
    "    rf_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : rf_survived})\n",
    "    rf_df.to_csv(f\"titanic_rf_{num_trees}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_rf_40.csv -m \"Random forest classifier with 40 trees\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40-tree model score: 0.75598\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"40-tree model score: {0.75598}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_rf_120.csv -m \"Random forest classifier with 120 trees\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120-tree model score: 0.76794\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"120-tree model score: {0.76794}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_rf_140.csv -m \"Random forest classifier with 140 trees\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140-tree model score: 0.76076\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"140-tree model score: {0.76076}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_rf_160.csv -m \"Random forest classifier with 160 trees\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160-tree model score: 0.76555\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"160-tree model score: {0.76555}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_rf_220.csv -m \"Random forest classifier with 220 trees\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220-tree model score: 0.77272\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"220-tree model score: {0.77272}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the 220-tree model ended up doing the best, increasing the number of trees didn't significantly improve the results. (For reference, the percentage difference between the 40-tree and 220-tree model corresponds to 8 people.) I'm not convinced it's anything other than random chance.\n",
    "\n",
    "That said, how about [even more](https://memegenerator.net/instance/76641465/billy-mays-but-wait-but-wait-theres-even-more) trees? 3000-tree model inbound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3000 = create_rf(trainxs, trainy, n_estimators=3000)\n",
    "rf_survived = model3000.predict(test_tab.xs)\n",
    "rf_rf = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : rf_survived})\n",
    "rf_rf.to_csv(f\"titanic_rf_3000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_rf_300.csv -m \"Random forest classifier with 3000 trees\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000-tree model score: 0.76555\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"3000-tree model score: {0.76555}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, it's worse than 220-tree model. I think this supports my earlier assertion that some of the models with more trees are better than the 40 tree model by random chance, rather than anything learned within the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Simpler models\n",
    "\n",
    "At this point, before getting into some feature engineering, I went back and read on Kaggle a little bit more about the Titanic competition. As I mentioned above, one of the provided files was a sample submission file where the survivors were assumed to be only and all the female passengers (as determined by the 'Sex' column). How does that submission score?\n",
    "\n",
    "`!kaggle competitions submit -c titanic -f gender_submssion.csv -m \"Only and all female passengers survive\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female only model score: 0.76555\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"Female only model score: {0.76555}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple model achieved the same score as the 3000-tree model! Let's take a look at the importance of each feature, according to that 3000-tree model. (Again, this is adapted directly from the course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='cols'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def feature_importance(m, df):\n",
    "    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=True)\n",
    "fi = feature_importance(model3000, trainxs)\n",
    "fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the bar chart, it is abundantly clear that 'Sex' is the most telling feature, with everything else trailing. That said, the 3000-tree model and female-only model actual differ in 62 (of 418) predictions, so they are finding slightly different spaces with the same score.\n",
    "\n",
    "Because I haven't been able to get [this song](https://youtu.be/2rP1gD9xXkU?t=32) (no, not [that one](https://en.wikipedia.org/wiki/My_Heart_Will_Go_On)) out of my head, I also tried \"the women and children fled to the lifeboats put to sea\" model. That is, all the women and children survived (but no one else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Women and children survival model\n",
    "wac_test_df = pd.DataFrame(test_df)\n",
    "wac_test_df['Survived'] = np.ones(len(wac_test_df), dtype=int)\n",
    "wac_cond = (wac_test_df.Sex == 'female') | (wac_test_df.Age < 18)\n",
    "wac_test_df['Survived'].where(wac_cond, 0, inplace=True)\n",
    "wac_submit = wac_test_df.loc[:,['PassengerId','Survived']]\n",
    "wac_submit.to_csv(\"titanic_wac.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#!kaggle competitions submit -c titanic -f titanic_wac.csv -m \"Women and children fled to the lifeboats put to sea\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women and children model score: 0.74641\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"Women and children model score: {0.74641}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model scores worse, but not dramatically so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "The next step was to improve the random forest models above by extracting, modifying, or deleting some of the given columns to create new, hopefully more relevant ones by [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering). \n",
    "\n",
    "I tried lots of different modifications that I came up with on my own, but, in actuality, I couldn't come up with anything that definitely improved on the previous random forests. (The best score I ended up getting was 0.77751.) \n",
    "\n",
    "Since I couldn't seem to improve, I ended up reading a number of the discussion posts on Kaggle, particularly those that dealt with random forests and feature engineering. My favorite was [this one](https://www.kaggle.com/zlatankr/titanic-random-forest-82-78), so I decided to adapt it to the syntax I was using. The function I came up with is in the collapse below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "def edit_features(df):\n",
    "    mod_df = df.copy()\n",
    "    \n",
    "    # PassengerId - not meaningful for learning\n",
    "    del mod_df[\"PassengerId\"]\n",
    "    \n",
    "    # Pclass - treat passenger class as category\n",
    "    mod_df[\"Pclass\"] = mod_df[\"Pclass\"].astype(\"category\")\n",
    "    \n",
    "    # Name - Split into name length and title\n",
    "    mod_df['NameLength'] = mod_df[\"Name\"].apply(lambda x: len(x))\n",
    "    mod_df['NameTitle'] = mod_df[\"Name\"].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n",
    "    mod_df['NameTitle'] = mod_df[\"NameTitle\"].astype(\"category\")\n",
    "    del mod_df[\"Name\"]\n",
    "    \n",
    "    # Age - fill Age with mean grouped by title and class\n",
    "    age_data = mod_df.groupby(['NameTitle', 'Pclass'])['Age']\n",
    "    mod_df['Age_na'] = mod_df['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    mod_df['Age_na'] = mod_df['Age_na'].astype(\"category\")\n",
    "    mod_df['Age'] = age_data.transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    # SibSp + Parch => Family size category\n",
    "    passengers = mod_df[\"SibSp\"] + mod_df[\"Parch\"] \n",
    "    mod_df['FamilySize'] = np.where(passengers == 0, 'Solo', np.where(passengers <= 3, 'Nuclear', 'Big'))\n",
    "    mod_df[\"FamilySize\"] = mod_df[\"FamilySize\"].astype(\"category\")\n",
    "    del mod_df['SibSp']\n",
    "    del mod_df['Parch']\n",
    "    \n",
    "    # Ticket - Split into two categories, \n",
    "    #  one based on the first letter of the ticket and \n",
    "    #  one based on the length\n",
    "    mod_df[\"TicketLetter\"] = mod_df[\"Ticket\"].apply(lambda x: str(x)[0])\n",
    "    mod_df[\"TicketLetter\"] = mod_df[\"TicketLetter\"].apply(lambda x: str(x))\n",
    "    highTicket = mod_df['TicketLetter'].isin(['1', '2', '3', 'S', 'P', 'C', 'A'])\n",
    "    lowTicket = mod_df['TicketLetter'].isin(['W', '4', '7', '6', 'L', '5', '8'])\n",
    "    mod_df['TicketLetter'] = np.where(highTicket, mod_df[\"TicketLetter\"],\n",
    "                                      np.where(lowTicket, \"LowTicket\", \"OtherTicket\"))\n",
    "    mod_df['TicketLength'] = mod_df['Ticket'].apply(lambda x: len(x))\n",
    "    del mod_df['Ticket']\n",
    "    \n",
    "    # Cabin - Split into the prefix and bin the number\n",
    "    mod_df[\"CabinPrefix\"] = mod_df['Cabin'].apply(lambda x: str(x)[0])\n",
    "    mod_df[\"CabinPrefix\"] = mod_df[\"CabinPrefix\"].astype(\"category\")\n",
    "    del mod_df[\"Cabin\"]\n",
    "        \n",
    "    # Embarked - Fill missing data with most common value ('S')\n",
    "    mod_df['Embarked'] = mod_df['Embarked'].fillna('S')\n",
    "    \n",
    "    return mod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'NameLength',\n",
       "       'NameTitle', 'Age_na', 'FamilySize', 'TicketLetter', 'TicketLength',\n",
       "       'CabinPrefix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# New data frame with modified features\n",
    "fe_train_df = edit_features(train_df)\n",
    "fe_train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the modified features, I ran the training data through the same mechanism as before for creating a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['Pclass', 'Sex', 'Embarked', 'NameTitle', 'Age_na', 'FamilySize', 'TicketLetter', 'CabinPrefix']\n",
      "Continuous columns: ['Age', 'Fare', 'NameLength', 'TicketLength']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameTitle</th>\n",
       "      <th>Age_na</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>TicketLetter</th>\n",
       "      <th>CabinPrefix</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>NameLength</th>\n",
       "      <th>TicketLength</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>0</td>\n",
       "      <td>Solo</td>\n",
       "      <td>LowTicket</td>\n",
       "      <td>n</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.362500</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>0</td>\n",
       "      <td>Nuclear</td>\n",
       "      <td>3</td>\n",
       "      <td>n</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>0</td>\n",
       "      <td>Nuclear</td>\n",
       "      <td>3</td>\n",
       "      <td>n</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.854200</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Identify categorical and continuous columns and prep training data\n",
    "fe_cont, fe_cat = cont_cat_split(fe_train_df, 1, dep_var=dep_var)\n",
    "fe_splits = RandomSplitter()(range_of(fe_train_df))\n",
    "fe_tab_panda = TabularPandas(fe_train_df, column_ops, fe_cat, fe_cont, y_names=dep_var, splits=fe_splits)\n",
    "\n",
    "# Alias training and validation data\n",
    "fetrainxs, fetrainy = fe_tab_panda.train.xs, fe_tab_panda.train.y\n",
    "fevalidxs, fevalidy = fe_tab_panda.valid.xs, fe_tab_panda.valid.y\n",
    "\n",
    "print(f\"Categorical columns: {fe_cat}\")\n",
    "print(f\"Continuous columns: {fe_cont}\")\n",
    "fe_tab_panda.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Create a random forest model classifier\n",
    "fe_model = RandomForestClassifier(n_estimators=300, min_samples_split=5, oob_score=True)\n",
    "\n",
    "fe_model.fit(fetrainxs, fetrainy)\n",
    "fe_test_df = edit_features(test_df)\n",
    "fe_test_tab = TabularPandas(fe_test_df, column_ops, fe_cat, fe_cont)\n",
    "error = zero_one_loss(fe_model.predict(fevalidxs), fevalidy)\n",
    "\n",
    "print(f\"Model error: {error}\")\n",
    "print(f\"Model oob error: {1.0 - fe_model.oob_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model error: 0.1685393258426966\n",
      "Model oob error: 0.16830294530154277\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Model error: {0.1685393258426966}\")\n",
    "print(f\"Model oob error: {0.16830294530154277}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Make and submit predictions\n",
    "fe_survived = fe_model.predict(fe_test_tab.xs)\n",
    "fe_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : fe_survived})\n",
    "fe_df.to_csv(\"titanic_fe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_fe.csv -m \"Random forest classifier with feature engineering\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with feature engineering model score: 0.78708\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(f\"Random forest with feature engineering model score: {0.78708}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model did score a little bit better than the random forest model{% fn 1 %}. I'm not entirely convinced it's actually better than the non-engineered models though. However, plotting the feature importance, it's pretty easy to see that it is likely the added 'NameTitle' and 'NameLength' features are providing something beyond what 'Name' did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='cols'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fi = feature_importance(fe_model, fetrainxs)\n",
    "fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, even though this feature engineering did not produce a model that blew the other models away, I really liked the justification and the write-up of the author for each change that he made. I think it's good insight on how I might use feature engineering on a less studied dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Neural Network\n",
    "\n",
    "Changing gears, let's see if a neural network would garner better results. \n",
    "\n",
    "First, I needed to set up the data. I dropped the `PassengerId` column, since it's just an index, and set up the `TabularPandas` similar to before, the only major change is the addition of the `Normalize` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Split the columns by cardinality of 500\n",
    "nn_cont_cols, nn_cat_cols = cont_cat_split(train_df, dep_var=dep_var)\n",
    "# Drop the counter variable\n",
    "nn_cont_cols.remove('PassengerId')\n",
    "# Added the Normalize operation\n",
    "nn_procs = [Categorify, FillMissing, Normalize]\n",
    "# Make the TabularPandas object\n",
    "nn_train_tab = TabularPandas(train_df, nn_procs, nn_cat_cols, nn_cont_cols, \n",
    "                             splits=splits, y_names=dep_var, y_block=CategoryBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I set up the `DataLoader` and the learner itself and made a stab at finding an appropriate learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn_dls = nn_train_tab.dataloaders(512)\n",
    "learn = tabular_learner(nn_dls, loss_func=F.l1_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experimenting with a number of different epochs, I found that 300 epochs with a learning rate of 0.04 seemed to give reasonable results on the validation set and not take forever (~5 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.014065220020711422\n",
      "Validation error: 0.1932232528924942\n"
     ]
    }
   ],
   "source": [
    "lr = 4e-2\n",
    "epochs = 300\n",
    "with learn.no_logging(): # Prevents display of the 300 rows of epochs\n",
    "    learn.fit_one_cycle(epochs, lr)\n",
    "train_error, valid_error = learn.recorder.values[-1]\n",
    "print(f\"Training error: {train_error}\")\n",
    "print(f\"Validation error: {valid_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, it was fairly easy to make predictions, once I found the right incantation to get the test data in the right format (`learn.dls.test_dl`), call the right prediction function (`get_preds`), and flatten the predictions to a single vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Make and submit predictions\n",
    "_, nn_predictions = learn.get_preds(dl=learn.dls.test_dl(test_df))\n",
    "nn_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : nn_predictions.flatten()})\n",
    "nn_df.to_csv(\"titanic_nn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_nn.csv -m \"Neural network predictions\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network score: 0.74641\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Neural network score: {0.74641}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the neural network didn't score that much worse that the random forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "Now, I've got a handful of models that take slightly different approaches (or at least use different numbers of trees). I can combine them all into a single model. There may be other ways to approach this, but the simplest thing that came to my mind was to use a majority vote to decide who survived. Below is the ensemble that scored best on the competition test set of the ones that I tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Gather the models\n",
    "model_files = [\"titanic_rf_220.csv\", # 220-tree Random Forest\n",
    "               \"titanic_rffe.csv\", # RF with feature engineering\n",
    "               \"titanic_nn.csv\", # Neural network model\n",
    "               \"gender_submission.csv\" # Women only survival\n",
    "              ]\n",
    "\n",
    "# Combined the models into a table\n",
    "data = []\n",
    "for i, mfile in enumerate(model_files):\n",
    "    model_df = pd.read_csv(mfile)\n",
    "    data.append(model_df['Survived'])\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Majority vote\n",
    "ensemble_survived = pd.to_numeric(df.mode().iloc[0], downcast='integer')\n",
    "\n",
    "# Save to csv and submit\n",
    "ensemble_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : ensemble_survived})\n",
    "ensemble_df.to_csv(\"titanic_ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c titanic -f titanic_ensemble.csv -m \"Ensemble\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble score: 0.78468\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Ensemble score: {0.78468}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the ensembles didn't score any better than the best individual models. It appears I have reached the limits of what I able to do with the approaches directly mentioned in the `fastai` lesson. I'm going to end my adventure with the Titanic here, since I worked the entire way through the lesson, but I have to say that was fun. \n",
    "\n",
    "If I were to continue, I would be taking a much closer look at [this notebook](https://www.kaggle.com/startover205/fastai-2-titanic-rf), which used a very similar `fastai` approach and achieved a score above 98%. They supplemented their data with [this extended set](https://www.kaggle.com/pavlofesenko/titanic-extended), however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ \"Interestingly, though, I couldn't get it to quite match the claimed success of the notebook I borrowed it from (82%). I don't think it's my code though, since when I ran their exact code: https://github.com/zlatankr/Projects/tree/master/Titanic, I also got 77-78%.\" | fndetail: 1}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
