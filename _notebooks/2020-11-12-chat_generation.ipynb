{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adventures in PyTorch\n",
    "> \"Generating fake chat logs\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Matt Bowen\n",
    "- categories: [pytorch, jupyter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "My telework journey into better understanding of deep learning began a few weeks back by watching [this video](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). I had some prior exposure to PyTorch, but most of it was cut and pasting someone else's code, without really grokking much of what I was doing.\n",
    "\n",
    "I don't remember out of the video itself (not unexpected for something titled a \"60-minute blitz\"), but I started poking around at some of the [examples](https://github.com/pytorch/examples).  My primary interest in machine learning is its use in natural language processing or language modeling and, thus, the \"Word-level language modeling RNN\" code particularly caught my eye. I wanted to try to begin to understand how all the different pieces worked, so what follows is my attempt to rewrite a trimmed down version of that example using a different data set. \n",
    "\n",
    "## Data Prep\n",
    "\n",
    "The data I used was a personal Google Hangouts chatroom I have had with a few friends since sometime in 2018. I learend that you can use [Google Takeout](https://takeout.google.com/) to download copies of any of your Google data. Using that with Hangouts gave me a `json` dump of the chat along with the attachments (read: memes) that were posted. This dump had a lot of extraneous information and wasn't exactly primed for reading by either myself or PyTorch, so I needed to massage that `json` dump into text to get something usable. \n",
    "\n",
    "> Warning: Some of the chat content may contain profanity or stupidity.\n",
    "\n",
    "First order of business, load the data into Python using the `json` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the data from the chatfile\n",
    "json_filename = \".\\\\data\\\\Takeout\\\\Hangouts\\\\Hangouts.json\"\n",
    "json_file = open(json_filename, encoding='utf8')\n",
    "data = json.load(json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some digging and verification, I matched everyone's ID in chat to their real name and saved a lookup table with that info (names have been changed to protect the not-so-innocent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "# Match IDs to names\n",
    "sender_lookup = {\"108076694707330228373\": \"Kappa\",\n",
    "                 \"107112383293353696822\": \"Beta\",\n",
    "                 \"111672618051461612345\": \"Omega\",\n",
    "                 \"112812509779111539276\": \"Psi\",\n",
    "                 \"114685444329830376810\": \"Gamma\",\n",
    "                 \"112861108657200483380\": \"Sigma\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I was focused on language modeling, I didn't feel like dealing with pictures or attachments, but I wanted to account for them in some way when they came up in chat, so I put in a substitute phrase for whenever they showed up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "# Replacement text for memes\n",
    "meme = \"<MEME>\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each message in the `json` data structure was listed as an 'event', a dictionary with key \"chat_message\" and sub-key \"message_content\". From there, I could get the sender ID, timestamp, and actual content of the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show\n",
    "# Set of keys to descend into json tree\n",
    "keys = (\"conversations\", 5, \"events\")\n",
    "\n",
    "# Descend the tree to the events list \n",
    "events = data\n",
    "for k in keys:\n",
    "    events = events[k]\n",
    "\n",
    "messages = [] \n",
    "\n",
    "# Loop through the events\n",
    "for event in events:\n",
    "    # Check for a valid message\n",
    "    if \"chat_message\" in event:\n",
    "        msg_content = event[\"chat_message\"][\"message_content\"]\n",
    "    else:\n",
    "        continue\n",
    "    # Timestamp of the message, which helps with sorting correctly later\n",
    "    timestamp = int(event[\"timestamp\"])\n",
    "    # Message sender\n",
    "    sender = event[\"sender_id\"][\"gaia_id\"]\n",
    "    sender = sender_lookup[sender]\n",
    "\n",
    "    # Message content\n",
    "    message = \"\"\n",
    "    if \"segment\" in msg_content:\n",
    "        segment = msg_content[\"segment\"]\n",
    "        for s in segment:\n",
    "            # Text messages\n",
    "            if s[\"type\"] == \"TEXT\":\n",
    "                message += s[\"text\"]\n",
    "            # Non-text messages\n",
    "            else:\n",
    "                message += meme + \" \"\n",
    "        message = message.strip()\n",
    "    else:\n",
    "        # Non-text messages\n",
    "        message = meme\n",
    "\n",
    "    # Add the message, with its timestamp and sender to the list\n",
    "    messages.append((timestamp, sender, message))\n",
    "\n",
    "# Sort the messages by timestamp\n",
    "messages.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that they were sorted, I could reformat the messages at text and print them out. I chose `::` as my separator between sender and the actual message content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000 messages found\n"
     ]
    }
   ],
   "source": [
    "num_messages = len(messages)\n",
    "print(\"{} messages found\".format(num_messages))\n",
    "\n",
    "messages = [\"{0} :: {1}\\n\".format(msg[1], msg[2]) for msg in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omega :: Apparently damage scales, but armour doesn't\n",
      "\n",
      "Omega :: We're only a few levels apart so not that bad at our current state\n",
      "\n",
      "Omega :: Probably why we sucked so bad that first night\n",
      "\n",
      "Omega :: Damn Greg and his free time\n",
      "\n",
      "Sigma :: This game is harder than I remember\n",
      "\n",
      "Kappa :: <MEME>\n",
      "\n",
      "Psi :: <MEME>\n",
      "\n",
      "Omega :: Wonder if there's TDY to NZ\n",
      "\n",
      "Psi :: Maybe, but not for you\n",
      "\n",
      "Kappa :: Lol\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "for msg in messages[110:120]:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives some text that PyTorch can work with and humans can read too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_index:\n",
    "            self.word_to_index[word] = len(self.index_to_word)\n",
    "            self.index_to_word.append(word)\n",
    "        return self.word_to_index[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wasn't a big fan of how the example wrote their Corpus class, since it required inputting a file directory path where the data was already split into training, validation, and test sets (though it probably works better for large files). I rewrote it, allowing for messages already loaded into memory and splitting the data into training/validation/test *after* the messages were sent into the class.  In the end, you end up with the same three tensors: `train`, `valid`, and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import torch\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, data, train_param=0.75, valid_param=0.15, test_param=0.10):\n",
    "        '''\n",
    "        data - either a filename string or list of messages\n",
    "        train_param - percentage of messages to use to train\n",
    "        valid_param - percentage of messages to use to validate\n",
    "        test_param - percentage of message to use to test\n",
    "        '''\n",
    "        # Same as their data.Dictionary() class\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "        # Filename vs. list of messages\n",
    "        if type(data) == str and os.path.exists(data):\n",
    "            messages = open(data, encoding='utf8').read().splitlines()\n",
    "        else:\n",
    "            messages = data\n",
    "\n",
    "        # Determine the number of training, validation, and test messages\n",
    "        num_messages = len(messages)\n",
    "        num_train_msgs = int(train_param * num_messages)\n",
    "        num_valid_msgs = int(valid_param * num_messages)\n",
    "        num_test_msgs = int(test_param * num_messages)\n",
    "\n",
    "        if num_train_msgs < 10 or num_valid_msgs < 10 or num_test_msgs < 10:\n",
    "            raise RuntimeError(\"Not enough messages for training/validation/test\")\n",
    "\n",
    "        # Scale back the number of messages if need be\n",
    "        total_param = train_param + valid_param + test_param\n",
    "        if total_param < 1.0:\n",
    "            num_messages = num_train_msgs + num_valid_msgs + num_test_msgs\n",
    "            messages = messages[:num_messages]\n",
    "        elif total_param > 1.0:\n",
    "            raise RuntimeError(\"Invalid train/validate/test parameters\")\n",
    "\n",
    "        # Add to dictionary and tokenize\n",
    "        train = []\n",
    "        valid = []\n",
    "        test = []\n",
    "        for msg_idx, msg in enumerate(messages):\n",
    "            # <eos> is the 'end-of-sentence' marking \n",
    "            words = msg.split() + ['<eos>']\n",
    "            msg_ids = []\n",
    "            # Add the words in the message to the dictionary \n",
    "            for word in words:\n",
    "                index = self.dictionary.add_word(word)\n",
    "                msg_ids.append(index)\n",
    "            # Split the messages into the appropriate buckets\n",
    "            if msg_idx < num_train_msgs:\n",
    "                train.append(torch.tensor(msg_ids).type(torch.int64))\n",
    "            elif msg_idx < num_train_msgs + num_valid_msgs:\n",
    "                valid.append(torch.tensor(msg_ids).type(torch.int64))\n",
    "            else:\n",
    "                test.append(torch.tensor(msg_ids).type(torch.int64))\n",
    "                \n",
    "        # End up with torch tensors for each of the 3 pieces, same as theirs\n",
    "        self.train = torch.cat(train)\n",
    "        self.valid = torch.cat(valid)\n",
    "        self.test = torch.cat(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we batchify in the same way as the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def batchify(data, batch_size, device):\n",
    "    # Work out how cleanly we can divide the dataset into batch_size parts.\n",
    "    num_batches = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, num_batches * batch_size)\n",
    "    # Evenly divide the data across the batch_size batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_corpus = Corpus(messages)\n",
    "\n",
    "# Defaults in the example\n",
    "train_batch_size = 20\n",
    "eval_batch_size = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_data = batchify(chat_corpus.train, train_batch_size, device)\n",
    "valid_data = batchify(chat_corpus.valid, eval_batch_size, device)\n",
    "test_data = batchify(chat_corpus.test, eval_batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The example code gave lots of options for what the model could be. That was overkill for what I wanted and didn't really help with understanding, so I stuck to an [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) model.  LSTM was one of the model options in the example and rewrote its model class to assume that an LSTM was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_tokens, num_hidden, num_layers):\n",
    "        '''\n",
    "        num_tokens - number of words in the dictionary\n",
    "        num_hidden - number of hidden states per layer\n",
    "        num_layers - number of layers\n",
    "        '''\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        # Default used by example\n",
    "        num_input_features = 200\n",
    "        self.encoder = nn.Embedding(num_tokens, num_input_features)\n",
    "        self.lstm = nn.LSTM(num_input_features, num_hidden, num_layers)\n",
    "\n",
    "        self.decoder = nn.Linear(num_hidden, num_tokens)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.uniform_(self.encoder.weight, -0.5, 0.5)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -0.5, 0.5)\n",
    "\n",
    "    def forward(self, input_data, hidden):\n",
    "        embedding = self.encoder(input_data)\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.num_tokens)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.num_layers, batch_size, self.num_hidden),\n",
    "                weight.new_zeros(self.num_layers, batch_size, self.num_hidden),)\n",
    "\n",
    "    def repackage_hidden(self, hidden):\n",
    "        if isinstance(hidden, torch.Tensor):\n",
    "            return hidden.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup for the rewritten model class (now called LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(chat_corpus.dictionary)\n",
    "num_hidden = 256 # Arbitrary choice\n",
    "num_layers = 3   # Arbitrary choice\n",
    "model = LSTM(num_tokens, num_hidden, num_layers).to(device)\n",
    "\n",
    "# Set our loss function\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Below is my attempt to simplify the example training and evaluation code for my purposes. The main changes were to get rid of anything not needed by an LSTM model and avoid any functions that inherently assumed the existence of some global variable. (It's probably just the C++ programmer in me, but it hurts my soul when I see that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training parameters\n",
    "# Backwards propagation through time\n",
    "bptt = 35\n",
    "# Maximum/initial learning rate\n",
    "lr = 20.0\n",
    "# Maximum number of epochs to use\n",
    "max_epochs = 40\n",
    "# Gradient clipping\n",
    "clip = 0.25\n",
    "# Output model filename\n",
    "model_filename = \".\\\\data\\\\chat\\\\lstm.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added bptt as input, rather than assumption\n",
    "def get_batch(source, index, bptt):\n",
    "    # bptt = Backward propagation through time\n",
    "    sequence_length = min(bptt, len(source) - 1 - index)\n",
    "    data = source[index:index+sequence_length]\n",
    "    target = source[index+1:index+1+sequence_length].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "best_validation_loss = None\n",
    "# This loop took about 3-4 minutes to run on my machine (about 10 seconds per loop for 20 loops)\n",
    "for epoch in range(0, max_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    ##\n",
    "    # train() - the example's train function is rewritten here\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(train_batch_size)\n",
    "    for batch, index in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, index, bptt)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        hidden = model.repackage_hidden(hidden)\n",
    "\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "    ##\n",
    "    # evaluate() - the example's evaluate function is rewritten here\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for index in range(0, valid_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(valid_data, index, bptt)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = model.repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output, targets).item()\n",
    "    validation_loss = total_loss / (len(valid_data) - 1)\n",
    "    ##\n",
    "    # A print statement to track progress\n",
    "    # print('-' * 89)\n",
    "    # print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | lr {:3.2f}'.format(\n",
    "    #       epoch, time.time() - epoch_start_time, validation_loss, lr))\n",
    "    # print('-' * 89)\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_validation_loss or validation_loss < best_validation_loss:\n",
    "        with open(model_filename, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_validation_loss = validation_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        # Stop training if the learning rate gets to small\n",
    "        if lr <= 1e-3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the best model to evaluate it against the test set, in case you want to try different training parameters to try to get a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.19 | test ppl   179.49\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reload the best model\n",
    "with open(model_filename, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    model.lstm.flatten_parameters()\n",
    "\n",
    "# Run on the test data\n",
    "import math\n",
    "model.eval()\n",
    "total_loss = 0.\n",
    "hidden = model.init_hidden(eval_batch_size)\n",
    "with torch.no_grad():\n",
    "    for index in range(0, test_data.size(0) - 1, bptt):\n",
    "        data, targets = get_batch(test_data, index, bptt)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = model.repackage_hidden(hidden)\n",
    "        total_loss += len(data) * criterion(output, targets).item()\n",
    "test_loss = total_loss / (len(test_data) - 1)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Chat Logs\n",
    "\n",
    "Now that we've trained a model, we can use it generate some chat logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFL four angry \n",
      "Omega :: Gotta be a dick increments Greg <MEME> \n",
      "Kappa :: what if we're trying to be on unable \n",
      "Kappa :: cuck \n",
      "Omega :: If only they need to enjoy well or playing? Though makes that with Told premise \n",
      "Kappa :: They I... the number of Seb at to wear the essence \n",
      "Omega :: The bit is a buuuut \n",
      "Omega :: Greg seems a end mankin \n",
      "Omega :: Should the way to realize I get why you tell children no than a dirt Court for coats arrangement habit \n",
      "Kappa :: nice \n",
      "Psi :: love got to the conan Rights \n",
      "Kappa :: away the WHERE hulu a statements obstructing It 1,880 \n",
      "Kappa :: eBay South unite co-workers leading three society and apparently document' are wearing lawyer?” on the scores <MEME> \n",
      "Kappa :: diaper, but def do windmills. mistake beer/dessert \n",
      "Omega :: Lol \n",
      "Omega :: Ion where raised our 👏consequences guy taking not to reaches \n",
      "Kappa :: Oh i gave that \n",
      "Gamma :: Greg driven brain Not like a mask money! but she got back for instead boated \n",
      "Omega :: Well "
     ]
    }
   ],
   "source": [
    "# Number of words to generate\n",
    "num_words = 200\n",
    "# Default used by example -> \"higher will increase diversity\"\n",
    "temperature = 1.0\n",
    "\n",
    "# Hidden and input states are just same size tensor as model uses\n",
    "hidden = model.init_hidden(1)\n",
    "input_data = torch.randint(num_tokens, (1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad(): # no need to track history\n",
    "    for i in range(num_words):\n",
    "        # Generate a random word based on the history\n",
    "        output, hidden = model(input_data, hidden)\n",
    "        word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        input_data.fill_(word_idx)\n",
    "\n",
    "        word = chat_corpus.dictionary.index_to_word[word_idx]\n",
    "        # Recall: our end of message token\n",
    "        if word == \"<eos>\":\n",
    "            print()\n",
    "        else:\n",
    "            print(word,end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it's not obvious, but the real chat doesn't resemble this. If you squint hard enough though, it's not terrible. I find it kinda of enjoyable to read. :stuck_out_tongue_closed_eyes:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
