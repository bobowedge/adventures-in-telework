{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg 2, Matt 0\n",
    "> Reusing the random forest techniques for the Titanic on another dataset\n",
    "\n",
    "- toc: false\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Matt Bowen\n",
    "- categories: [jupyter, kaggle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In a [previous post](https://bobowedge.github.io/adventures-in-telework/jupyter/2021/01/31/titanic-kaggle.html), I wrote about using some random forest techniques to tackle the [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic).  Last week, I found out that there's a [monthly competition](https://www.kaggle.com/c/tabular-playground-series-apr-2021) that mimics the original Titanic competition, but uses synthetic data based on the original dataset instead. I thought it would be good to revisit the previous code I wrote and try it against this new dataset.\n",
    "\n",
    "I'm not going to rehash everything I did before, but rather focus on the random forests, feature engineering, and neural networks to see what results I can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "# imports, imports, imports\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "# Pandas display options\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.max_columns = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Data\n",
    "\n",
    "As before, the competition provides a training set (passenger data with a 'Survived' column) and a test set (passenger data without a 'Survived' column) and asks to be provided the prediced 'Survived' column for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variable\n",
    "dep_var = 'Survived'\n",
    "# Training set\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "# Test set\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, here what the training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Oconnor, Frankie</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>209245</td>\n",
       "      <td>27.14</td>\n",
       "      <td>C12239</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Bryan, Drew</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27323</td>\n",
       "      <td>13.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Owens, Kenneth</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>CA 457703</td>\n",
       "      <td>71.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kramer, James</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>A. 10866</td>\n",
       "      <td>13.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Bond, Michael</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>427635</td>\n",
       "      <td>7.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bell, Adele</td>\n",
       "      <td>female</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 15008</td>\n",
       "      <td>14.86</td>\n",
       "      <td>D17243</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Brown, Herman</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13273</td>\n",
       "      <td>11.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Childress, Charles</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Caughlin, Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>458654</td>\n",
       "      <td>30.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Enciso, Tyler</td>\n",
       "      <td>male</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>458074</td>\n",
       "      <td>13.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived  Pclass                Name     Sex  ...  Parch  \\\n",
       "0                0         1       1    Oconnor, Frankie    male  ...      0   \n",
       "1                1         0       3         Bryan, Drew    male  ...      0   \n",
       "2                2         0       3      Owens, Kenneth    male  ...      2   \n",
       "3                3         0       3       Kramer, James    male  ...      0   \n",
       "4                4         1       3       Bond, Michael    male  ...      0   \n",
       "...            ...       ...     ...                 ...     ...  ...    ...   \n",
       "99995        99995         1       2         Bell, Adele  female  ...      0   \n",
       "99996        99996         0       2       Brown, Herman    male  ...      0   \n",
       "99997        99997         0       3  Childress, Charles    male  ...      0   \n",
       "99998        99998         0       3    Caughlin, Thomas    male  ...      1   \n",
       "99999        99999         0       3       Enciso, Tyler    male  ...      0   \n",
       "\n",
       "          Ticket   Fare   Cabin  Embarked  \n",
       "0         209245  27.14  C12239         S  \n",
       "1          27323  13.35     NaN         S  \n",
       "2      CA 457703  71.29     NaN         S  \n",
       "3       A. 10866  13.04     NaN         S  \n",
       "4         427635   7.76     NaN         S  \n",
       "...          ...    ...     ...       ...  \n",
       "99995   PC 15008  14.86  D17243         C  \n",
       "99996      13273  11.15     NaN         S  \n",
       "99997        NaN   9.95     NaN         S  \n",
       "99998     458654  30.92     NaN         S  \n",
       "99999     458074  13.96     NaN         S  \n",
       "\n",
       "[100000 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you refer back to the previous dataset, you can see that's there more than 100 times as much training data the in the original problem.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, split the columns into either continuous and categorical types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous columns: ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "Categorical columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Split the columns into categorical and continuous\n",
    "cont_cols, cat_cols = cont_cat_split(train_df, 1, dep_var=dep_var)\n",
    "print(f\"Continuous columns: {cont_cols}\")\n",
    "print(f\"Categorical columns: {cat_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "\n",
    "One of the things that I overlooked and later regretted when trying to document my results the last time around was never setting any of the random seed. That meant I could never reproduce the exact results in a future run. And, certainly, no one else who ever tried to run my code would get the same results. {% fn 1 %}\n",
    "\n",
    "I'm not the first `fastai` user to have this issue and [some kind stranger](https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628/27) provided a function for setting some of the necessary seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Set random_seed\n",
    "def random_seed(seed_value, use_cuda=True):\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu vars\n",
    "    random.seed(seed_value) # Python\n",
    "    if use_cuda: \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this doesn't seem to set the seed for `RandomSplitter` or `RandomForestClassifier`, so I'll have to set those manually as well to get reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Answer to Life, the Universe, and Everything\n",
    "ALUE = 42\n",
    "random_seed(ALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Data\n",
    "\n",
    "Next, set up the training and validation data from the provided training set via `TabularPandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80000\n",
      "Validation set size: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>...</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>2687</td>\n",
       "      <td>1</td>\n",
       "      <td>51792</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83391</th>\n",
       "      <td>73357</td>\n",
       "      <td>2</td>\n",
       "      <td>68733</td>\n",
       "      <td>2240</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.450001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62311</th>\n",
       "      <td>6430</td>\n",
       "      <td>1</td>\n",
       "      <td>4381</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.690001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57794</th>\n",
       "      <td>76737</td>\n",
       "      <td>2</td>\n",
       "      <td>32105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.879999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73100</th>\n",
       "      <td>39937</td>\n",
       "      <td>1</td>\n",
       "      <td>56183</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.110001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29010</th>\n",
       "      <td>87006</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77199</th>\n",
       "      <td>34744</td>\n",
       "      <td>2</td>\n",
       "      <td>43849</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44288</th>\n",
       "      <td>42095</td>\n",
       "      <td>2</td>\n",
       "      <td>16114</td>\n",
       "      <td>14524</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.629999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94835</th>\n",
       "      <td>9105</td>\n",
       "      <td>1</td>\n",
       "      <td>17880</td>\n",
       "      <td>9712</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>290.769989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53243</th>\n",
       "      <td>60227</td>\n",
       "      <td>1</td>\n",
       "      <td>12517</td>\n",
       "      <td>14929</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.799999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name  Sex  Ticket  Cabin  Embarked  ...  Pclass   Age  SibSp  Parch  \\\n",
       "1509    2687    1   51792      0         3  ...       3  67.0      1      0   \n",
       "83391  73357    2   68733   2240         3  ...       3  30.0      1      2   \n",
       "62311   6430    1    4381      0         1  ...       2  45.0      1      1   \n",
       "57794  76737    2   32105      0         1  ...       2  32.0      0      0   \n",
       "73100  39937    1   56183      0         3  ...       3  10.0      1      0   \n",
       "...      ...  ...     ...    ...       ...  ...     ...   ...    ...    ...   \n",
       "29010  87006    1       0      0         3  ...       3  27.0      0      0   \n",
       "77199  34744    2   43849      0         3  ...       3  26.0      0      0   \n",
       "44288  42095    2   16114  14524         3  ...       1   2.0      0      0   \n",
       "94835   9105    1   17880   9712         1  ...       1  60.0      1      1   \n",
       "53243  60227    1   12517  14929         3  ...       1  45.0      2      2   \n",
       "\n",
       "             Fare  \n",
       "1509    29.680000  \n",
       "83391   27.450001  \n",
       "62311   28.690001  \n",
       "57794   19.879999  \n",
       "73100   27.110001  \n",
       "...           ...  \n",
       "29010    9.910000  \n",
       "77199    6.630000  \n",
       "44288   28.629999  \n",
       "94835  290.769989  \n",
       "53243   34.799999  \n",
       "\n",
       "[80000 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "# Operations for columns\n",
    "column_ops = [Categorify, FillMissing]\n",
    "# Split the training data randomly\n",
    "splits = RandomSplitter(seed=ALUE)(range_of(train_df))\n",
    "# Setup TabularPandas\n",
    "tab_panda = TabularPandas(train_df, column_ops ,cat_cols, cont_cols, y_names=dep_var, splits=splits)\n",
    "# Aliases for training data\n",
    "trainxs, trainy = tab_panda.train.xs, tab_panda.train.y\n",
    "# Aliases for validation data\n",
    "validxs, validy = tab_panda.valid.xs, tab_panda.valid.y\n",
    "print(f\"Training set size: {len(trainxs)}\")\n",
    "print(f\"Validation set size: {len(validxs)}\")\n",
    "trainxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, convert the test data to `TabularPandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to TabularPandas\n",
    "test_tab = TabularPandas(test_df, [Categorify, FillMissing], cat_cols, cont_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "This is enough to set up the function to create random forests. Since there's a lot more data, I choose to up the default for the minimum number of samples per leaf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Create Random Forest and fit it\n",
    "def create_rf(xs, y, n_estimators=40, max_features=0.5, min_samples_leaf=10):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, max_features=max_features, \n",
    "                                  min_samples_leaf=min_samples_leaf, oob_score=True, random_state=ALUE).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model error: 0.2251\n",
      "Model oob error: 0.2216\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# 40-tree model with validation and out-of-bag (oob) error\n",
    "model = create_rf(trainxs, trainy)\n",
    "error = zero_one_loss(model.predict(validxs), validy)\n",
    "print(f\"Model error: {error:.04}\")\n",
    "print(f\"Model oob error: {1.0 - model.oob_score_:.04}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, I somewhat arbitrarily picked a series of `n_estimator` values to try to deduce what the best number of trees was. This time{% fn 2 %}, I'm going to use the `GridSearch` available in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model error: 0.2167\n",
      "Best model parameters: {'criterion': 'gini', 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Classifier settings to use for all forests\n",
    "rfc = RandomForestClassifier(max_features='auto', oob_score=True, random_state=ALUE, n_jobs=-1)\n",
    "\n",
    "# Parameter grid to search\n",
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n",
    "              \"min_samples_leaf\" : [1, 5, 10], \n",
    "              \"min_samples_split\" : [2, 4, 10, 12, 16], \n",
    "              \"n_estimators\": [50, 100, 400, 700]}\n",
    "\n",
    "# Set up the grid search parameters\n",
    "gs = GridSearchCV(estimator=rfc, param_grid=param_grid, scoring='accuracy', cv=2, n_jobs=-1)\n",
    "\n",
    "# Execute the grid search\n",
    "gs = gs.fit(trainxs, trainy)\n",
    "\n",
    "print(f\"Best model error: {1.0 - gs.best_score_:.04}\")\n",
    "print(f\"Best model parameters: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search took about 10 minutes to run on the GPU server I was using and produce the \"best\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model error: 0.2232\n",
      "Model oob error: 0.2154\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Select the best estimator from the grid search\n",
    "gsrf = gs.best_estimator_\n",
    "error = zero_one_loss(gsrf.predict(validxs), validy)\n",
    "print(f\"Model error: {error:.04}\")\n",
    "print(f\"Model oob error: {1.0 - gsrf.oob_score_:.04}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll note here that the model errors are different in the last two steps because one is from the cross-validation of the training set and one is from the validation set.{% fn 3 %}\n",
    "\n",
    "Next, I used that model to predict the survivors from the test data and submit the predictions to kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Use the model to predict the survivors from the test set\n",
    "survived = gsrf.predict(test_tab.xs)\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : survived})\n",
    "submission.to_csv(f\"tps-Apr2021_gridsearch.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c tabular-playground-series-apr-2021 -f tps-Apr2021_gridsearch.csv -m \"RF Grid search\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Grid Search score: 0.79187\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(\"RF Grid Search score: 0.79187\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, I also submitted the results of a model with 3000 trees to see whether that would be any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3000 = create_rf(trainxs, trainy, n_estimators=3000)\n",
    "survived = model3000.predict(test_tab.xs)\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : survived})\n",
    "submission.to_csv(f\"tps-Apr2021_rf-3000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c tabular-playground-series-apr-2021 -f tps-Apr2021_rf-3000.csv -m \"RF with 3000 trees\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 3000 score: 0.78166\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(\"RF 3000 score: 0.78166\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: The grid search does slightly better than just taking a larger tree. (With the larger data set, small percentage differences are more significant.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "The next step was to improve the random forest models above by extracting, modifying, or deleting some of the given columns to create new, hopefully more relevant ones by [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering). \n",
    "\n",
    "The collapse below has the features edits that I used last time, which I ~~stole~~ adapted from [this post](https://www.kaggle.com/zlatankr/titanic-random-forest-82-78)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "# Same edit_features as last time\n",
    "def edit_features(df):\n",
    "    mod_df = df.copy()\n",
    "    \n",
    "    # PassengerId - not meaningful for learning\n",
    "    del mod_df[\"PassengerId\"]\n",
    "    \n",
    "    # Pclass - treat passenger class as category\n",
    "    mod_df[\"Pclass\"] = mod_df[\"Pclass\"].astype(\"category\")\n",
    "    \n",
    "    # Name - Split into name length and title\n",
    "    mod_df['NameLength'] = mod_df[\"Name\"].apply(lambda x: len(x))\n",
    "    mod_df['NameTitle'] = mod_df[\"Name\"].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n",
    "    mod_df['NameTitle'] = mod_df[\"NameTitle\"].astype(\"category\")\n",
    "    del mod_df[\"Name\"]\n",
    "    \n",
    "    # Age - fill Age with mean grouped by title and class\n",
    "    age_data = mod_df.groupby(['NameTitle', 'Pclass'])['Age']\n",
    "    mod_df['Age_na'] = mod_df['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    mod_df['Age_na'] = mod_df['Age_na'].astype(\"category\")\n",
    "    mod_df['Age'] = age_data.transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    # SibSp + Parch => Family size category\n",
    "    passengers = mod_df[\"SibSp\"] + mod_df[\"Parch\"] \n",
    "    mod_df['FamilySize'] = np.where(passengers == 0, 'Solo', np.where(passengers <= 3, 'Nuclear', 'Big'))\n",
    "    mod_df[\"FamilySize\"] = mod_df[\"FamilySize\"].astype(\"category\")\n",
    "    del mod_df['SibSp']\n",
    "    del mod_df['Parch']\n",
    "    \n",
    "    # Ticket - Split into two categories, \n",
    "    #  one based on the first letter of the ticket and \n",
    "    #  one based on the length\n",
    "    mod_df[\"TicketLetter\"] = mod_df[\"Ticket\"].apply(lambda x: str(x)[0])\n",
    "    mod_df[\"TicketLetter\"] = mod_df[\"TicketLetter\"].apply(lambda x: str(x))\n",
    "    highTicket = mod_df['TicketLetter'].isin(['1', '2', '3', 'S', 'P', 'C', 'A'])\n",
    "    lowTicket = mod_df['TicketLetter'].isin(['W', '4', '7', '6', 'L', '5', '8'])\n",
    "    mod_df['TicketLetter'] = np.where(highTicket, mod_df[\"TicketLetter\"],\n",
    "                                      np.where(lowTicket, \"LowTicket\", \"OtherTicket\"))\n",
    "    mod_df['TicketLength'] = mod_df['Ticket'].apply(lambda x: len(str(x)))\n",
    "    del mod_df['Ticket']\n",
    "    \n",
    "    # Cabin - Split into the prefix and bin the number\n",
    "    mod_df[\"CabinPrefix\"] = mod_df['Cabin'].apply(lambda x: str(x)[0])\n",
    "    mod_df[\"CabinPrefix\"] = mod_df[\"CabinPrefix\"].astype(\"category\")\n",
    "    del mod_df[\"Cabin\"]\n",
    "        \n",
    "    # Embarked - Fill missing data with most common value ('S')\n",
    "    mod_df['Embarked'] = mod_df['Embarked'].fillna('S')\n",
    "    \n",
    "    return mod_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The next step is to create the new dataframes for the validation and training set based on the feature edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameTitle</th>\n",
       "      <th>Age_na</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>TicketLetter</th>\n",
       "      <th>CabinPrefix</th>\n",
       "      <th>Fare_na</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>NameLength</th>\n",
       "      <th>TicketLength</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>Patricia</td>\n",
       "      <td>False</td>\n",
       "      <td>Nuclear</td>\n",
       "      <td>OtherTicket</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>67.0</td>\n",
       "      <td>29.680000</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83391</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Robert</td>\n",
       "      <td>False</td>\n",
       "      <td>Nuclear</td>\n",
       "      <td>S</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.450001</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62311</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "      <td>Karen</td>\n",
       "      <td>False</td>\n",
       "      <td>Nuclear</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>45.0</td>\n",
       "      <td>28.690001</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# New data frame with modified features\n",
    "fe_train_df = edit_features(train_df)\n",
    "\n",
    "# Identify categorical and continuous columns and prep training data\n",
    "fe_cont, fe_cat = cont_cat_split(fe_train_df, 1, dep_var=dep_var)\n",
    "fe_splits = RandomSplitter(seed=ALUE)(range_of(fe_train_df))\n",
    "fe_train_panda = TabularPandas(fe_train_df, column_ops, fe_cat, fe_cont, y_names=dep_var, splits=fe_splits)\n",
    "\n",
    "# Alias training and validation data\n",
    "fetrainxs, fetrainy = fe_train_panda.train.xs, fe_train_panda.train.y\n",
    "fevalidxs, fevalidy = fe_train_panda.valid.xs, fe_train_panda.valid.y\n",
    "\n",
    "fe_train_panda.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>NameTitle</th>\n",
       "      <th>Age_na</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>TicketLetter</th>\n",
       "      <th>CabinPrefix</th>\n",
       "      <th>Fare_na</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>NameLength</th>\n",
       "      <th>TicketLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>False</td>\n",
       "      <td>Solo</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>19.0</td>\n",
       "      <td>63.009998</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>Lorraine</td>\n",
       "      <td>False</td>\n",
       "      <td>Solo</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.810000</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "      <td>Heather</td>\n",
       "      <td>False</td>\n",
       "      <td>Solo</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>False</td>\n",
       "      <td>19.0</td>\n",
       "      <td>38.910000</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# New data frame with modified features\n",
    "fe_test_df = edit_features(test_df)\n",
    "\n",
    "# Convert test data to TabularPandas\n",
    "fe_test_panda = TabularPandas(fe_test_df, [Categorify, FillMissing], fe_cat, fe_cont)\n",
    "\n",
    "fe_test_panda.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search, Part Deux\n",
    "\n",
    "The original post that had these feature edits also used a grid search to find better parameters for the random forest. I did not use that approach against the last data set, but I thought I would attempt it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 120 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   37.1s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model error: 0.217\n",
      "Best model parameters: {'criterion': 'gini', 'min_samples_leaf': 5, 'min_samples_split': 16, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(max_features='auto', oob_score=True, random_state=ALUE, n_jobs=-1)\n",
    "\n",
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n",
    "              \"min_samples_leaf\" : [1, 5, 10], \n",
    "              \"min_samples_split\" : [2, 4, 10, 12, 16], \n",
    "              \"n_estimators\": [50, 100, 400, 700]}\n",
    "\n",
    "gs = GridSearchCV(estimator=rfc, param_grid=param_grid, scoring='accuracy', cv=2, verbose=4, n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(fetrainxs, fetrainy)\n",
    "print(f\"Best model error: {1.0 - gs.best_score_:.04}\")\n",
    "print(f\"Best model parameters: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returned the same model as the original grid search. (If I tried to combined both searches into one, the parameter space was too large and crashed the GPU I was using.)\n",
    "\n",
    "Check the error for the best model from the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model error: 0.2248\n",
      "Model oob error: 0.2162\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Select the best estimator from the grid search\n",
    "gsrf = gs.best_estimator_\n",
    "error = zero_one_loss(gsrf.predict(fevalidxs), fevalidy)\n",
    "print(f\"Model error: {error:.04}\")\n",
    "print(f\"Model oob error: {1.0 - gsrf.oob_score_:.04}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the survivors and submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Survival predictions\n",
    "survived = gsrf.predict(fe_test_panda.xs)\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : survived})\n",
    "submission.to_csv(f\"tps-Apr2021_fe-gridsearch.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c tabular-playground-series-apr-2021 -f tps-Apr2021_fe-gridsearch.csv -m \"FE Grid search\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search from FE score: 0.78973\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(\"Grid search from FE score: 0.78973\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I ended up with the highest number of trees in the parameter set with this grid search, I also tried another grid search with some larger tree values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  18 | elapsed:  2.1min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:  3.6min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  4.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model error: 0.2164\n",
      "Best model parameters: {'criterion': 'gini', 'min_samples_leaf': 5, 'min_samples_split': 50, 'n_estimators': 1000}\n",
      "Model error: 0.2233\n",
      "Model oob error: 0.2156\n"
     ]
    }
   ],
   "source": [
    "param_grid = { \"criterion\" : [\"gini\"], \n",
    "              \"min_samples_leaf\" : [5], \n",
    "              \"min_samples_split\" : [16, 25, 50], \n",
    "              \"n_estimators\": [700, 1000, 3000]}\n",
    "\n",
    "gs = GridSearchCV(estimator=rfc, param_grid=param_grid, scoring='accuracy', cv=2, verbose=4, n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(fetrainxs, fetrainy)\n",
    "print(f\"Best model error: {1.0 - gs.best_score_:.04}\")\n",
    "print(f\"Best model parameters: {gs.best_params_}\")\n",
    "\n",
    "# Select the best estimator from the grid search\n",
    "gsrf = gs.best_estimator_\n",
    "error = zero_one_loss(gsrf.predict(fevalidxs), fevalidy)\n",
    "print(f\"Model error: {error:.04}\")\n",
    "print(f\"Model oob error: {1.0 - gsrf.oob_score_:.04}\")\n",
    "\n",
    "# Survival predictions\n",
    "survived = gsrf.predict(fe_test_panda.xs)\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : survived})\n",
    "submission.to_csv(f\"tps-Apr2021_fe-gridsearch-2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c tabular-playground-series-apr-2021 -f tps-Apr2021_fe-gridsearch-2.csv -m \"FE Grid search 2\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search 2 from FE score: 0.79179\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(\"Grid search 2 from FE score: 0.79179\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature engineering does not seem to have improved the score over the grid search for the random forest. It's possible I just need to find the right parameters for the grid search for the feature engineering random forests, but hyperparameter optimization does not seem like much fun. :stuck_out_tongue: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Instead, let's see how a neural network does on this data set. As always, the first thing to do is set up the data.\n",
    "\n",
    "I dropped the `PassengerId` column, since it's just an index, and set up the `TabularPandas` similar to before, the only major change is the addition of the `Normalize` operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Split the columns by cardinality of 500\n",
    "cont_cols, cat_cols = cont_cat_split(train_df, dep_var=dep_var)\n",
    "# Drop the counter variable\n",
    "cont_cols.remove('PassengerId')\n",
    "# Added the Normalize operation\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "# Make the TabularPandas object\n",
    "nn_train_tab = TabularPandas(train_df, procs=procs, cat_names=cat_cols, cont_names=cont_cols, y_names=dep_var, \n",
    "                             splits=splits, y_block=CategoryBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Aside about API changes**: Between the last post and today, there was a slight change in the `fastai` API for the tabular dataloaders. For some reason, supplying the PyTorch `l1_loss` function directly as the loss function now throws a warning about data shape mismatches between the input and target data probably causing inaccuracies. Instead, after some extensive googling and documentation reading, I'm just going to use the `accuracy` metric. Near as I can tell, this does the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I set up the `DataLoader` and the learner itself and made a stab at finding an appropriate learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_na</th>\n",
       "      <th>Fare_na</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Montoya, Jessica</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>451670</td>\n",
       "      <td>#na#</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.650002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Saner, Trent</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39137</td>\n",
       "      <td>E13067</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>61.000001</td>\n",
       "      <td>13.260001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Gustovich, Jane</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>485531</td>\n",
       "      <td>#na#</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>16.000001</td>\n",
       "      <td>23.010001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Redden, Dorothy</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>440418</td>\n",
       "      <td>#na#</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>26.440000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Marks, Curtis</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>334870</td>\n",
       "      <td>#na#</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.140000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Munden, Robert</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>451650</td>\n",
       "      <td>#na#</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>4.649998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Robinson, Maurice</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SC/PARIS 451298</td>\n",
       "      <td>#na#</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>11.890000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Ellis, Herman</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S.O.C. 16344</td>\n",
       "      <td>#na#</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>58.999999</td>\n",
       "      <td>81.129996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Escalante, Shawn</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>309115</td>\n",
       "      <td>C2758</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>65.999999</td>\n",
       "      <td>84.580001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Spieth, Lorna</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 25390</td>\n",
       "      <td>C10929</td>\n",
       "      <td>C</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>32.799999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_dls = nn_train_tab.dataloaders(bs=512)\n",
    "nn_dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To everything, [learn, learn, learn](https://www.youtube.com/watch?v=W4ga_M5Zdn4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.507917</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.762350</td>\n",
       "      <td>01:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = tabular_learner(nn_dls, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I try more than one epoch, the accuracy decreases along with the training loss, seemingly indicating some overfitting starts happening.\n",
    "\n",
    "Next step is to make predictions and submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Make and submit predictions\n",
    "survived = learn.get_preds(dl=learn.dls.test_dl(test_df))[0][:,1]\n",
    "survived = survived.flatten().round().long()\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : survived})\n",
    "submission.to_csv(\"tps-Apr2021_nn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c tabular-playground-series-apr-2021 -f tps-Apr2021_nn.csv -m \"Neural network predictions\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN score: 0.73343\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(\"NN score: 0.73343\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's not quite as accurate as the random forests.\n",
    "\n",
    "Another regret from my last foray was not running the neural network against the feature engineering data as well, so I'll do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Split the columns by cardinality of 500\n",
    "cont_cols, cat_cols = cont_cat_split(fe_train_df, dep_var=dep_var)\n",
    "# Added the Normalize operation\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "# Make the TabularPandas object\n",
    "nn_train_tab = TabularPandas(fe_train_df, procs=procs, cat_names=cat_cols, cont_names=cont_cols, y_names=dep_var, \n",
    "                             splits=splits, y_block=CategoryBlock)\n",
    "\n",
    "nn_dls = nn_train_tab.dataloaders(bs=512)\n",
    "nn_dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.489520</td>\n",
       "      <td>0.489564</td>\n",
       "      <td>0.770850</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = tabular_learner(nn_dls, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Make and submit predictions\n",
    "survived = learn.get_preds(dl=learn.dls.test_dl(fe_test_df))[0][:,1]\n",
    "survived = survived.flatten().round().long()\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : survived})\n",
    "submission.to_csv(\"tps-Apr2021_nn-fe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c tabular-playground-series-apr-2021 -f tps-Apr2021_nn-fe.csv -m \"Neural network predictions on feature engineering\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with FE score: 0.77028\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "print(\"NN with FE score: 0.77028\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score is significantly better than the non-engineering features neural network; I don't know why that is. This model also trains faster than anything else above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "Now, I've got a handful of models that take slightly different approaches.  I can combine them all into a single model using a majority vote to decide who survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Gather the models\n",
    "model_files = [\"tps-Apr2021_gridsearch.csv\", # Random forest grid search result\n",
    "               \"tps-Apr2021_fe-gridsearch-2.csv\", # Random forest with feature engineering grid search result\n",
    "               \"tps-Apr2021_nn-fe.csv\" # Neural network with feature engineering\n",
    "              ]\n",
    "\n",
    "# Combined the models into a table\n",
    "data = []\n",
    "for i, mfile in enumerate(model_files):\n",
    "    model_df = pd.read_csv(mfile)\n",
    "    data.append(model_df['Survived'])\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Majority vote\n",
    "survived = pd.to_numeric(df.mode().iloc[0], downcast='integer')\n",
    "\n",
    "# Save to csv and submit\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : survived})\n",
    "submission.to_csv(\"tps-Apr2021_ensemble1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "`!kaggle competitions submit -c tabular-playground-series-apr-2021 -f tps-Apr2021_ensemble1.csv -m \"Ensemble1\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble1 score: 0.79191\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Ensemble1 score: {0.79191}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the ensemble scored better than any of the individual models (albeit by a net of 1 prediction against the best model so far).\n",
    "\n",
    "I had one more submission remaining before the competition closed, so I tried one additional ensemble as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the models\n",
    "model_files = [\"tps-Apr2021_gridsearch.csv\", # 400 tree Random forest from grid search\n",
    "               \"tps-Apr2021_fe-gridsearch-2.csv\", # 1000 tree Random forest with feature engineering from grid search\n",
    "               \"tps-Apr2021_nn-fe.csv\", # Neural network with feature engineering\n",
    "               \"tps-Apr2021_rf-3000.csv\", # 3000 tree Random Forest\n",
    "               \"tps-Apr2021_fe-gridsearch.csv\" # 700 tree Random forest with feature engineering from grid search\n",
    "              ]\n",
    "\n",
    "# Combined the models into a table\n",
    "data = []\n",
    "for i, mfile in enumerate(model_files):\n",
    "    model_df = pd.read_csv(mfile)\n",
    "    data.append(model_df['Survived'])\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Majority vote\n",
    "survived = pd.to_numeric(df.mode().iloc[0], downcast='integer')\n",
    "\n",
    "# Save to csv and submit\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived' : survived})\n",
    "submission.to_csv(\"tps-Apr2021_ensemble2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!kaggle competitions submit -c tabular-playground-series-apr-2021 -f tps-Apr2021_ensemble2.csv -m \"Ensemble2\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble2 score: 0.79115\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Ensemble2 score: {0.79115}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite as good, but definitely in the ballpark.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "The competition allowed two submissions to be run against the final data to determine the final score. For my entry, I chose the first ensemble and 400-tree random forest that came from the initial grid search, since they gave my best scores on the pre-final data. Prior to the final, I was ranked 734th out of 1219.\n",
    "\n",
    "My final ranking was 634 out of 1244. My final score was 0.79104 and was from the submission from the initial random forest grid search. The top score on the leaderboard was 0.81328.  Maybe I shouldn't yet quit my job to become a full-time data scientist. :open_mouth:\n",
    "\n",
    "MEME TIME!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "meme = Image.open(\"meme_20210430.png\")\n",
    "meme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ \"Not that I think anyone is actually trying to copy my code and run it. This is purely a hypothetical.\" | fndetail: 1}}\n",
    "\n",
    "{{ \"At the suggestion of Psi. Thanks, Psi!\" | fndetail: 2}}\n",
    "\n",
    "{{ \"If I were doing this for a living or something serious, I probably would have fed the entire training set into `GridSearchCV`, rather than just the training portion of split, since `GridSearchCV` does cross-validation.\" | fndetail: 3}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
