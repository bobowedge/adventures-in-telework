{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name that Speaker\n",
    "> Using fastai to guess who might say a particular phrase in a chat conversation\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Matt Bowen\n",
    "- categories: [jupyter]\n",
    "- comments: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "The first few lessons of the [fastai course](https://course.fast.ai/) lean heavily towards computer vision problems with their examples. Personally, I am a little more interested in natural language processing and work with text applications, so I glommed onto their example of doing sentiment analysis of movie reviews using fastai. \n",
    "\n",
    "Here's how they built that model using the IMDB dataset internal to the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.611722</td>\n",
       "      <td>0.397967</td>\n",
       "      <td>0.820560</td>\n",
       "      <td>07:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.304293</td>\n",
       "      <td>0.294640</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>16:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.280457</td>\n",
       "      <td>0.206577</td>\n",
       "      <td>0.920880</td>\n",
       "      <td>16:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.201380</td>\n",
       "      <td>0.181090</td>\n",
       "      <td>0.929840</td>\n",
       "      <td>16:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153116</td>\n",
       "      <td>0.179792</td>\n",
       "      <td>0.931280</td>\n",
       "      <td>16:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "\n",
    "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', encoding='utf8', bs=32)\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated model `learn` can then be used to predict the sentiment of a statement. I picked three statements below to show what it's predictions are like. The model predicts the first two statements accurately and is fairly confident in its prediction. For the third, the model predicts the sentiment, but isn't as confident, which is not surprising since I picked that one to be intentionally tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of x: pos, prob=0.9987\n",
      "Sentiment of y: neg, prob=0.9659\n",
      "Sentiment of z: neg, prob=0.7692\n"
     ]
    }
   ],
   "source": [
    "x = learn.predict(\"I really liked that movie!\")\n",
    "y = learn.predict(\"At no point in your rambling, incoherent response was there anything that could even be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.\")\n",
    "z = learn.predict(\"I thought it was going to be good, but it really was not in the end.\")\n",
    "print(f\"Sentiment of x: {x[0]}, prob={x[2][1]:.4f}\")\n",
    "print(f\"Sentiment of y: {y[0]}, prob={y[2][0]:.4f}\")\n",
    "print(f\"Sentiment of z: {z[0]}, prob={z[2][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My idea was to take this template for building a text classification model and use it to classify the \"speaker\" of a given statement, given a previous set of chat conversations to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In a previous post, I took some data from a Google Hangouts chat and converted it to a format more palatable to feeding into a PyTorch LSTM, i.e. each chat message was broken up to be in the format \n",
    "> Speaker :: Message\n",
    "\n",
    "I'm going to use the same underlying data here, but format it slightly differently to ease import into [fastai](https://www.fast.ai/). This might not be the cleanest way to do this, but it worked :smile:\n",
    "\n",
    "The format I ended up using was a modified csv. Commas are pretty prevalent in the data and I hate using quotes and escapes, so I used `|` to separate the columns {% fn 1 %}. Since I had already done the separation of speaker and message using `::` before, the script to convert was fairly straightforward, minus one spot where someone had used an SAT-style analogy \n",
    "> Kappa :: Omega:OK :: Gamma:\"Here's the thing\"\n",
    "\n",
    "{{ 'Which appeared in the corpus as || (aka OR)' | fndetail: 1 }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_filename = \"/notebooks/fastbook/chat/chatFile.txt\"\n",
    "chat_csv = \"/notebooks/fastbook/chat/chatFile.csv\"\n",
    "\n",
    "# Read in the chat file with \n",
    "data = open(chat_filename, encoding='utf8').read()\n",
    "# As software developers, we used \"||\" a few places to mean OR\n",
    "data = data.replace(\"||\", \"or\")\n",
    "data = data.splitlines()\n",
    "\n",
    "# Write to csv\n",
    "with open(chat_csv, encoding='utf8', mode='w') as csv:\n",
    "    # Header\n",
    "    csv.write(\"Name|Message\")\n",
    "    # New message\n",
    "    for line in data:\n",
    "        if \"::\" in line:\n",
    "            x = line.split(\"::\")\n",
    "            if len(x) > 2:\n",
    "                (name, msg) = (\"Kappa\", \"Omega:Ok :: Gamma:Here's the thing\")\n",
    "            else:\n",
    "                (name, msg) = line.split(\"::\")\n",
    "            name = name.strip()\n",
    "            msg = msg.strip()\n",
    "            csv.write('\\n')\n",
    "            csv.write(name)\n",
    "            csv.write(\"|\")\n",
    "            csv.write(msg)\n",
    "        else:\n",
    "            csv.write(\" \" + msg)\n",
    "    csv.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "The csv now matched each message to a particular speaker in a format that was easily digestible by `fastai`. Next, I mimicked the sentimental analysis example above to make my speaker identification model. I'm essentially just swapping `from_folder` out for `from_csv`, with some extra arguments to give details about my csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.567062</td>\n",
       "      <td>1.340145</td>\n",
       "      <td>0.449983</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.305043</td>\n",
       "      <td>1.235283</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.228520</td>\n",
       "      <td>1.160044</td>\n",
       "      <td>0.540014</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.107468</td>\n",
       "      <td>1.124508</td>\n",
       "      <td>0.564677</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.059996</td>\n",
       "      <td>1.121178</td>\n",
       "      <td>0.570369</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "\n",
    "dls = TextDataLoaders.from_csv('.', csv_fname=chat_csv, \n",
    "                               delimiter=\"|\", text_col = 1, label_col = 0)\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(\"/notebooks/fastbook/chat/chat_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My First App\n",
    "\n",
    "The challenge in the second lesson of the `fastai` course was to create a model using `fastai` and turn it into a prototype web app. The structure of how to do so using `ipywidgets` and `voila` was pretty straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box for giving the text to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84b665cb1004a28bb98426f18b0cbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Text:', placeholder='Input text...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "txtInput = widgets.Textarea(placeholder='Input text...', description='Text:')\n",
    "txtInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A button to execute the prediction for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60072cb1955640f5a8fb501207f862b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Predict', icon='question', style=ButtonStyle(), tooltip='Click me')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button = widgets.Button(description='Predict',\n",
    "                        tooltip='Click me',\n",
    "                        icon='question')\n",
    "button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the output widget with a dividing line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9112e8f7014b46ae7be460c70e6e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outWidget = widgets.Output(layout={'border': '1px solid black'})\n",
    "outWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click_classify(change):\n",
    "    # predictions and probabilities from the model\n",
    "    prediction, idx, probs = learn_inf.predict(txtInput.value)   \n",
    "    # pair the probabilities with each speaker\n",
    "    outputs = list(zip(probs, learn_inf.dls.vocab[1]))\n",
    "    # sort the list with the most likely speaker first\n",
    "    outputs.sort(reverse=True)\n",
    "    outWidget.clear_output()\n",
    "    # Print the output, with the most likely speaker in bold\n",
    "    with outWidget:\n",
    "        header = widgets.HTML()\n",
    "        header.value = '<u>Scores</u>'\n",
    "        display(header)\n",
    "        lblPred = widgets.HTML()\n",
    "        lblPred.value = f'<b>{outputs[0][1]}</b>: <b>{100 * outputs[0][0]:.2f}%</b>'\n",
    "        display(lblPred)\n",
    "        for (prob, name) in outputs[1:]:\n",
    "            lbl = widgets.Label()\n",
    "            lbl.value = f'{name}:  {100 * prob:.2f}%'\n",
    "            display(lbl)\n",
    "\n",
    "button.on_click(on_click_classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcoming\n",
    "\n",
    "One obvious shortcoming of this speaker identification model is that one of the speakers ('Kappa') was much more likely to be identified as the most likely speaker than any of the other speakers for almost any text. He accounts for about 44% of the input messages, but I wasn't sure how (or even if I should) adjust for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure to Launch\n",
    "\n",
    "I was able to run Voila locally in my notebook and get it to produce a viable web app. Unfortunately, I was unable to get it to host properly on [Heroku](https://www.heroku.com/), as suggested in the course. All I could seem to get was a nebulous \"Application Error\" and did not have the time or patience to wade through figuring it out.\n",
    "\n",
    "I have some evidence to think that the issue was the OS differences between the Paperspace notebooks that I was using for fastai development, the Windows environment I hosted the Jupyter notebook (and ultimately got the app running locally), and whatever Heroku is running on their server. These differences preventing a model built in one place from working in another and couldn't actually build the model on Heroku."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
