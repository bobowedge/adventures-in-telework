{
  
    
        "post0": {
            "title": "Know Your Meme",
            "content": "Introduction . In previous blog entries, I have focused on natural language processing (i.e. text) for the dives into the fastai course. Today, instead, I finally bit the bullet and shifted to working on a computer vision (i.e. images) project. In particular, I&#39;m going to train a model that handles classificaton of images with mutiple categories. . The data today comes from the same Google Hangouts extraction that I have used before (from Google Takeout. However, I&#39;m switching my focus from the text in the chat to all of the images that appear in the conversation. I had previously lazily marked these all as &quot;memes&quot; (specifically as &quot; &lt;MEME&gt;&quot;) in terms of how they would appear as text, but a closer look revealed that is an over-simplification of the images. . The images contain a wide variety of types and I&#39;m set on using them to train this multiple category model. Among the categories I&#39;m considering are the sender of the image and whether it is a meme or a twitter post or a news article. But I&#39;ll cover that in a few minutes. . Data Preparation . As you may recall, the chat converstation log comes from a Google Takeout download, where the primary source of the log is a json file. The json file contains a dictionary of events, any of which could correspond to a chat message, a link, an image, or something like a receipt notification. . Again, we start by loading the json data into Python and narrow the data to the events corresponding to actual chat content: . import json # Load the data from the chatfile json_filename = &quot;./Hangouts_20201001.json&quot; json_file = open(json_filename, encoding=&#39;utf8&#39;) data = json.load(json_file) json_file.close() # Set of keys to descend json tree to chat content eventKeys = (&quot;conversations&quot;, 5, &quot;events&quot;) # Chat content events events = data for k in eventKeys: events = events[k] . As before, each sender ID maps to a person, whose pseudonym is below: . sender_lookup = {&quot;108076694707330228373&quot;: &quot;Kappa&quot;, &quot;107112383293353696822&quot;: &quot;Beta&quot;, &quot;111672618051461612345&quot;: &quot;Omega&quot;, &quot;112812509779111539276&quot;: &quot;Psi&quot;, &quot;114685444329830376810&quot;: &quot;Gamma&quot;, &quot;112861108657200483380&quot;: &quot;Sigma&quot;} . There are at least two different ways that images can show up in the chat event log. One is as an embedded attachment and another is as a link. (If there are others, they are ignored for the purposes of this experiment.) In both cases, the log provides a url link to the image that can be downloaded. . First step is to loop through the events to download each image locally and construct a way to reference each image. I used the download_url routine in fastai for the former and a csv file for the latter with appropriate info. The description of the csv headers is in the code below that sets up the csv file for writing. . The course lesson that talked about Pandas, which I never used or heard of before. I&#39;m going to use it through to maniuplate the csv-like tables. With that in mind, the initial construction may seem a bit odd. . from fastai.vision.all import * # Pandas is in the fastai package # Headers for the csv file columnHeaders = [&quot;sender&quot;, # Image sender &quot;filename&quot;, # Image filename &quot;type&quot;, # Image type as labeled by Google &quot;timestamp&quot;, # Sent timestamp ] . Then, here&#39;s the code for looping through the json events, downloading each image, and adding a line to the dataframe for each image . # Do a count of how many images for each sender senderCounts = dict() # Index for filename imageIndex = 0 # Path for images imagesPath = &quot;/storage/chat_memes&quot; # These indexes turned out not to be images (e.g. not all links are images) badImages = [119, 196, 1000, 1003, 1052, 1455, 1552, 1699] rows = [] for event in events: # Get the ontent of each message (if it exists) msgContent = event.get(&quot;chat_message&quot;,{}).get(&quot;message_content&quot;,{}) # Timestamp of message timestamp = int(event[&quot;timestamp&quot;]) # Message sender sender = event[&quot;sender_id&quot;][&quot;gaia_id&quot;] sender = sender_lookup[sender] # Images as attachments attachments = msgContent.get(&quot;attachment&quot;,[]) for attachment in attachments: if imageIndex not in badImages: item = attachment[&quot;embed_item&quot;] itemType = item[&quot;plus_photo&quot;][&quot;media_type&quot;] # Usually PHOTO itemUrl = item[&quot;plus_photo&quot;][&quot;url&quot;] filename = f&quot;{imagesPath}/image_{imageIndex:04}&quot; download_url(itemUrl, filename, timeout=10, retries=20, show_progress=False) rows.append({&quot;timestamp&quot;: timestamp, &quot;sender&quot;: sender, &quot;filename&quot;: filename, &quot;type&quot;: itemType}) senderCounts[sender] = senderCounts.get(sender, 0) + 1 imageIndex += 1 # Images as links segments = msgContent.get(&quot;segment&quot;,[]) for segment in segments: if segment[&quot;type&quot;] == &quot;LINK&quot;: url = segment[&quot;text&quot;] base = url.split(&#39;/&#39;)[-1] # Check if it&#39;s possibly an image if &#39;jpg&#39; in base or &#39;gif&#39; in base or &#39;png&#39; in base: if imageIndex not in badImages: filename = f&quot;{imagesPath}/image_{imageIndex:04}&quot; download_url(url, filename, timeout=10, retries=20, show_progress=False) rows.append({&quot;timestamp&quot;: timestamp, &quot;sender&quot;: sender, &quot;filename&quot;: filename, &quot;type&quot;: &quot;LINK&quot;}) senderCounts[sender] = senderCounts.get(sender, 0) + 1 imageIndex += 1 . # Use rows to create a Pandas dataframe images_df = pd.DataFrame(rows, columns=columnHeaders) # Save the rows to a csv file images_df.to_csv(&quot;chat_images.csv&quot;) # Display the first five rows images_df.head() . sender filename type timestamp . 0 Kappa | /storage/chat_memes/image_0000 | PHOTO | 1600741647230976 | . 1 Gamma | /storage/chat_memes/image_0001 | PHOTO | 1600772605984217 | . 2 Psi | /storage/chat_memes/image_0002 | PHOTO | 1600786824022176 | . 3 Gamma | /storage/chat_memes/image_0003 | PHOTO | 1600861187519443 | . 4 Kappa | /storage/chat_memes/image_0004 | PHOTO | 1600863157273002 | . I ended up with about 1700 images from the chat log. Here&#39;s a quick breakdown of how many each sender was responsible for: . totalImages = 0 for sender in senderCounts: totalImages += senderCounts[sender] print(f&quot;Total images: {totalImages}&quot;) for sender in senderCounts: images = senderCounts[sender] percent = 100 * images / totalImages print(f&quot;{sender} sent {images} images -&gt; {percent:.03}%&quot;) . Total images: 1718 Kappa sent 707 images -&gt; 41.2% Gamma sent 294 images -&gt; 17.1% Psi sent 299 images -&gt; 17.4% Beta sent 95 images -&gt; 5.53% Omega sent 322 images -&gt; 18.7% Sigma sent 1 images -&gt; 0.0582% . And here&#39;s a sample image . im = Image.open(f&quot;{imagesPath}/image_0000&quot;) im.to_thumb(240) . As might be clear by this point, one clear drawback to using this data to for &quot;multi-category&quot; classification is that the data is really labeled with more than one category (sender is really the only usable one). . The approach that I ended up using to label the day was to just label the data myself 1. I didn&#39;t want to go through each of the 1700 images one by one and apply labels, but I found a tool called pigeon that was incredibly useful to enable this labeling to take until the heat death of the universe to complete. It still took multiple hours :weary:, but I learned a new tool :sunglasses:. . Before that, though, I needed some labels to apply to the images. After some trials and errors, these are the set I ended up with (along with a brief description of each one.) . labels = [&#39;meme&#39;, # Is it a meme &#39;social_media&#39;, # Is it a screenshot from social media (usually twitter, sometimes reddit or fb) &#39;comic_strip&#39;, # Is it a multi-panel comic (usually xkcd or SMBC) &#39;game_of_thrones&#39;, # Still mad about the final season &#39;simpsons&#39;, # Enough Simpsons memes to be on their own &#39;rick_and_morty&#39;, # Enough Rick &amp; Morty memes to be on their own &#39;tv_cartoon&#39;, # But not enough South Park, Futurama, or Family Guy to train alone (every simpsons and rick_and_morty is marked this too) &#39;star_trek_wars&#39;, # Combined star trek and wars to get enough &#39;marvel&#39;, # Avengers and others memes &#39;lotr&#39;, # One does not simply something, something &#39;news&#39;, # Is this a news item (rather than a humorous one) &#39;ali_express&#39;, # One member of the chat posts weird things from Ali Express &#39;photo&#39;, # Is this a real-life photo (as opposed to an internet one) &#39;trump&#39;, # Is this trump-related &#39;beacon&#39;, # Is this a beacon for playing video games &#39;weird&#39; # random label I put on stuff I thought was unusual, even for us ] . With these labels in hand, we can then use pigeon to annotate each image with a label . from pigeon import annotate from IPython.display import display import glob imagesPath = &quot;/storage/chat_memes&quot; images = glob.glob(imagesPath+&quot;/image*&quot;) annotations = annotate(images, options = labels, display_fn=lambda filename: display(Image.open(filename).to_thumb(240))) . Alternatively, by setting options to None, you can use free-form text for the labels. I ended up using the labels list for the first pass to give each image a label and then the free-form version for adding additional labels. In the end, annotations is a Python dictionary mapping a filename to a list of string labels. I saved these to their own pandas dataframe and csv, so as not to lose any of the annotations I made. . rows = [] for filename, labels in annotations.items(): labels = &quot; &quot;.join(labels) rows.append({&quot;filename&quot;: filename, &quot;labels&quot;: labels}) annotations_df = pd.DataFrame(rows) annotations_df.to_csv(&quot;annotations.csv&quot;) annotations_df.head() . filename labels . 0 /storage/chat_memes/image_0029 | meme simpsons tv_cartoon | . 1 /storage/chat_memes/image_0071 | meme simpsons tv_cartoon trump | . 2 /storage/chat_memes/image_0096 | meme tv_cartoon | . 3 /storage/chat_memes/image_0105 | meme tv_cartoon | . 4 /storage/chat_memes/image_0231 | meme tv_cartoon | . And merge it with the images dataframe I had made earlier: . combined_df = images_df.merge(annotations_df, on=&#39;filename&#39;) combined_df.to_csv(&quot;chat_memes_with_labels.csv&quot;) combined_df.head() . sender filename type timestamp labels . 0 Kappa | /storage/chat_memes/image_0000 | PHOTO | 1600741647230976 | social_media | . 1 Gamma | /storage/chat_memes/image_0001 | PHOTO | 1600772605984217 | social_media | . 2 Psi | /storage/chat_memes/image_0002 | PHOTO | 1600786824022176 | social_media | . 3 Gamma | /storage/chat_memes/image_0003 | PHOTO | 1600861187519443 | social_media | . 4 Kappa | /storage/chat_memes/image_0004 | PHOTO | 1600863157273002 | comic_strip | . Finally, this gives us some data that I can work with to get onto the multi-category classification. Whew! . Multi-category Classification . Following the example in the lesson, I can now create a DataBlock object to feed into a DataLoaders object to a neural network learner. First, I need some functions to specify my independent and dependent variables. The independent ones are the images/memes and the dependent ones are the sender and the labels that I just annotated above 2. . def get_x(row): return row[&#39;filename&#39;] def get_y(row): if row[&#39;sender&#39;] == &#39;Sigma&#39;: return row[&#39;labels&#39;].split() else: return row[&#39;labels&#39;].split() + [row[&#39;sender&#39;]] . These facilitate building the DataBlock that can be put into a DataLoaders: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(combined_df) . As a reward for reading this far, here&#39;s a batch of memes with their labels that the Dataloaders generates. Note that the RandomResizedCrop transform is why the display below is a cropped portion of the image. As I understand it, it&#39;s designed to normalize the input data to prevent effects of image size and dimension from having an influence on training the model. . dls.show_batch(nrows=2, ncols=4) . Create a learner module from the loaded data: . learn = cnn_learner(dls, resnet18) . The default metric for multi-category labelled data is accuracy_multi which uses a sigmoid function with a particular threshold to decide whether or not an image gets a particular label. . I had no idea what the learning rate or threshold should be for this data, so I tried to used their approaches to use the data to determine what the learning rate and threshold should be. . First, the learning rate: . learn.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.03981071710586548) . The suggested learning rate looks to be 0.03 or 0.04 is called for. Now, for the threshold for accuracy_multi: . preds,targs = learn.get_preds() xs = torch.linspace(0.05, 0.99,30) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . Unfortunately, I did not get any sort of peak in curve. Any speculation as to why on my part at this point would be a wild guess. So, I picked a somewhat random value of 0.8 for the threshold. . lr = 0.04 threshold = 0.8 learn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=threshold)) learn.fine_tune(3, base_lr=lr) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.665638 | 0.417254 | 0.898459 | 00:47 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.263506 | 0.572447 | 0.889776 | 01:00 | . 1 | 0.236928 | 0.222640 | 0.918207 | 00:59 | . 2 | 0.211741 | 0.180523 | 0.926891 | 00:59 | . Next, I saved the model so I can use it later, without having to retrace all of the above steps . learn.export(&quot;meme_classification.pkl&quot;) . Inference . Now, with a trained model, I can try to make predictions. I need some data to actually infer on. Fortunately, since I am taking my sweet time going through the course, I had two more months of chat images (471 in total) that I could use for that. . I used the same process as above to import the images from the json file, but now, I only considered messages that had timestamps that were greater than the last timestamp from the training and validation set. I stored those images along with their senders in a new Pandas data frame and save it to a csv. . # Use rows to create a Pandas dataframe new_images_df = pd.DataFrame(new_image_rows, columns=new_image_column_headers) # Display the first five rows new_images_df.head() . sender filename type timestamp . 0 Beta | /storage/chat_memes/new_images/image_0000 | PHOTO | 1607981480934474 | . 1 Gamma | /storage/chat_memes/new_images/image_0001 | PHOTO | 1607991667741774 | . 2 Omega | /storage/chat_memes/new_images/image_0002 | PHOTO | 1608007771189618 | . 3 Kappa | /storage/chat_memes/new_images/image_0003 | PHOTO | 1608041280956958 | . 4 Kappa | /storage/chat_memes/new_images/image_0004 | PHOTO | 1608081802164842 | . For the 472 new images, the breakdown of senders was {&#39;Kappa&#39;: 222, &#39;Psi&#39;: 95, &#39;Gamma&#39;: 71, &#39;Omega&#39;: 63, &#39;Beta&#39;: 20}. After my labeling quagmire above, I didn&#39;t bother trying to otherwise label the new data, but just looked at a few samples instead, which I&#39;ll show below. . First, let&#39;s get the sender and labels predictions for each of the new images into a Pandas data frame: . # Senders (RSVP Sigma) senders = set([&#39;Kappa&#39;, &#39;Gamma&#39;, &#39;Psi&#39;, &#39;Beta&#39;, &#39;Omega&#39;]) # Column headers predict_column_headers = [&#39;filename&#39;, &#39;predict_sender&#39;, &#39;predict_labels&#39;] # One row for each file predict_rows = [] for image_file in new_images_df[&#39;filename&#39;]: # no_bar prevents a progress appearing for each predict call with learn.no_bar(): labels = learn.predict(image_file) all_labels = set(labels[0]) sender = all_labels.intersection(senders) other_labels = all_labels.difference(sender) if &#39;471&#39; in image_file: print(image_file) row = {} row[&#39;filename&#39;] = image_file if len(sender) &gt; 0: row[&#39;predict_sender&#39;] = &quot; &quot;.join(sender) if len(other_labels) &gt; 0: row[&#39;predict_labels&#39;] = &quot; &quot;.join(other_labels) predict_rows.append(row) # Use rows to create a Pandas dataframe predict_df = pd.DataFrame(predict_rows, columns=predict_column_headers) # Display the first five rows predict_df.head() . filename predict_sender predict_labels . 0 /storage/chat_memes/new_images/image_0000 | NaN | meme | . 1 /storage/chat_memes/new_images/image_0001 | NaN | social_media | . 2 /storage/chat_memes/new_images/image_0002 | Omega | ali_express | . 3 /storage/chat_memes/new_images/image_0003 | NaN | meme | . 4 /storage/chat_memes/new_images/image_0004 | Kappa | meme | . I&#39;ll combine this data frame with the one extracting the new images from the json file: . combined_new_images_df = new_images_df.merge(predict_df, on=&#39;filename&#39;) combined_new_images_df.to_csv(&quot;chat_new_images_predictions.csv&quot;) combined_new_images_df.head() . sender filename type timestamp predict_sender predict_labels . 0 Beta | /storage/chat_memes/new_images/image_0000 | PHOTO | 1607981480934474 | NaN | meme | . 1 Gamma | /storage/chat_memes/new_images/image_0001 | PHOTO | 1607991667741774 | NaN | social_media | . 2 Omega | /storage/chat_memes/new_images/image_0002 | PHOTO | 1608007771189618 | Omega | ali_express | . 3 Kappa | /storage/chat_memes/new_images/image_0003 | PHOTO | 1608041280956958 | NaN | meme | . 4 Kappa | /storage/chat_memes/new_images/image_0004 | PHOTO | 1608081802164842 | Kappa | meme | . Some overall observations from the predictions the model made and this table: . Only 259 images actually got labeled with a sender: 199 for Kappa, 56 for Omega, and 4 for Gamma. Beta and Psi were never predicted as a label. Of those 259 sender predictions, 134 were correct. . 20 of the images weren&#39;t predicted to have any label (sender or otherwise) at all . I wrote some widget code to display the images with their predicted and correct sender and the predict labels . Here&#39;s some of the ones where the sender was predicted correctly: . And some of the ones where the sender was predicted incorrectly: . And 3 more random ones for the brave reader who got this far: . In the images I looked through, the classifier did a pretty good job on appropriately labeling &#39;social_media&#39;, &#39;meme&#39;, &#39;tv_cartoon&#39;, &#39;comic_strip&#39;, &#39;photo&#39; and &#39;ali_express&#39; in the same way that I did, but struggled to do the same with the other labels. Again, if I cared more about results (and had better labeled data), I&#39;m sure I could improve the accuracy, but I&#39;m just happy to work through the multi-classification learning to produce something reasonable. MEMES! :satisfied: . 1. I thought about a few different ways to try to apply categorical labels to the data. Given that I&#39;m in the midst of a deep learning course, training a model to label the data was an option I considered, but I would essentially have to create a model for each type of category that I wanted, download images for each of those and use the models to classify the chat image data. If I had to do it over again, that is likely the approach I would take. Hindsight is 20/20.‚Ü© . 2. The fact that Sigma only sent 1 image meant I couldn&#39;t really train to find that label, so I removed Sigma from the possible labels. Sorry Sigma.‚Ü© .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2020/12/28/know-your-meme.html",
            "relUrl": "/jupyter/2020/12/28/know-your-meme.html",
            "date": " ‚Ä¢ Dec 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Name that Speaker",
            "content": "Intro . The first few lessons of the fastai course lean heavily towards computer vision problems with their examples. Personally, I am a little more interested in natural language processing and work with text applications, so I glommed onto their example of doing sentiment analysis of movie reviews using fastai. . Here&#39;s how they built that model using the IMDB dataset internal to the library: . from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;, encoding=&#39;utf8&#39;, bs=32) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.611722 | 0.397967 | 0.820560 | 07:53 | . epoch train_loss valid_loss accuracy time . 0 | 0.304293 | 0.294640 | 0.875800 | 16:06 | . 1 | 0.280457 | 0.206577 | 0.920880 | 16:07 | . 2 | 0.201380 | 0.181090 | 0.929840 | 16:08 | . 3 | 0.153116 | 0.179792 | 0.931280 | 16:05 | . The generated model learn can then be used to predict the sentiment of a statement. I picked three statements below to show what it&#39;s predictions are like. The model predicts the first two statements accurately and is fairly confident in its prediction. For the third, the model predicts the sentiment, but isn&#39;t as confident, which is not surprising since I picked that one to be intentionally tricky. . x = learn.predict(&quot;I really liked that movie!&quot;) y = learn.predict(&quot;At no point in your rambling, incoherent response was there anything that could even be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.&quot;) z = learn.predict(&quot;I thought it was going to be good, but it really was not in the end.&quot;) print(f&quot;Sentiment of x: {x[0]}, prob={x[2][1]:.4f}&quot;) print(f&quot;Sentiment of y: {y[0]}, prob={y[2][0]:.4f}&quot;) print(f&quot;Sentiment of z: {z[0]}, prob={z[2][0]:.4f}&quot;) . Sentiment of x: pos, prob=0.9987 Sentiment of y: neg, prob=0.9659 Sentiment of z: neg, prob=0.7692 . My idea was to take this template for building a text classification model and use it to classify the &quot;speaker&quot; of a given statement, given a previous set of chat conversations to train on. . Data Preparation . In a previous post, I took some data from a Google Hangouts chat and converted it to a format more palatable to feeding into a PyTorch LSTM, i.e. each chat message was broken up to be in the format . Speaker :: Message . I&#39;m going to use the same underlying data here, but format it slightly differently to ease import into fastai. This might not be the cleanest way to do this, but it worked :smile: . The format I ended up using was a modified csv. Commas are pretty prevalent in the data and I hate using quotes and escapes, so I used | to separate the columns 1. Since I had already done the separation of speaker and message using :: before, the script to convert was fairly straightforward, minus one spot where someone had used an SAT-style analogy . Kappa :: Omega:OK :: Gamma:&quot;Here&#39;s the thing&quot; . chat_filename = &quot;/notebooks/fastbook/chat/chatFile.txt&quot; chat_csv = &quot;/notebooks/fastbook/chat/chatFile.csv&quot; # Read in the chat file with data = open(chat_filename, encoding=&#39;utf8&#39;).read() # As software developers, we used &quot;||&quot; a few places to mean OR data = data.replace(&quot;||&quot;, &quot;or&quot;) data = data.splitlines() # Write to csv with open(chat_csv, encoding=&#39;utf8&#39;, mode=&#39;w&#39;) as csv: # Header csv.write(&quot;Name|Message&quot;) # New message for line in data: if &quot;::&quot; in line: x = line.split(&quot;::&quot;) if len(x) &gt; 2: (name, msg) = (&quot;Kappa&quot;, &quot;Omega:Ok :: Gamma:Here&#39;s the thing&quot;) else: (name, msg) = line.split(&quot;::&quot;) name = name.strip() msg = msg.strip() csv.write(&#39; n&#39;) csv.write(name) csv.write(&quot;|&quot;) csv.write(msg) else: csv.write(&quot; &quot; + msg) csv.write(&#39; n&#39;) . Build Model . The csv now matched each message to a particular speaker in a format that was easily digestible by fastai. Next, I mimicked the sentimental analysis example above to make my speaker identification model. I&#39;m essentially just swapping from_folder out for from_csv, with some extra arguments to give details about my csv. . from fastai.text.all import * dls = TextDataLoaders.from_csv(&#39;.&#39;, csv_fname=chat_csv, delimiter=&quot;|&quot;, text_col = 1, label_col = 0) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . /opt/conda/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . epoch train_loss valid_loss accuracy time . 0 | 1.567062 | 1.340145 | 0.449983 | 00:16 | . epoch train_loss valid_loss accuracy time . 0 | 1.305043 | 1.235283 | 0.500000 | 00:38 | . 1 | 1.228520 | 1.160044 | 0.540014 | 00:37 | . 2 | 1.107468 | 1.124508 | 0.564677 | 00:37 | . 3 | 1.059996 | 1.121178 | 0.570369 | 00:37 | . Note that there&#39;s a lot less data here than in the IMDB set, so training is much faster. Also, I ignored the warning now since it was just a deprecation warning. Not sure if that&#39;ll bite me later. . Next, let&#39;s save the model to a file for later use . learn.export(&quot;/notebooks/fastbook/chat/chat_model.pkl&quot;) . My First App . The challenge in the second lesson of the fastai course was to create a model using fastai and turn it into a prototype web app. The structure of how to do so using ipywidgets and voila was pretty straightforward. . A box for giving the text to evaluate . import ipywidgets as widgets txtInput = widgets.Textarea(placeholder=&#39;Input text...&#39;, description=&#39;Text:&#39;) txtInput . A button to execute the prediction for the model . button = widgets.Button(description=&#39;Predict&#39;, tooltip=&#39;Click me&#39;, icon=&#39;question&#39;) button . Set up the output widget with a dividing line . outWidget = widgets.Output(layout={&#39;border&#39;: &#39;1px solid black&#39;}) outWidget . def on_click_classify(change): # predictions and probabilities from the model prediction, idx, probs = learn_inf.predict(txtInput.value) # pair the probabilities with each speaker outputs = list(zip(probs, learn_inf.dls.vocab[1])) # sort the list with the most likely speaker first outputs.sort(reverse=True) outWidget.clear_output() # Print the output, with the most likely speaker in bold with outWidget: header = widgets.HTML() header.value = &#39;&lt;u&gt;Scores&lt;/u&gt;&#39; display(header) lblPred = widgets.HTML() lblPred.value = f&#39;&lt;b&gt;{outputs[0][1]}&lt;/b&gt;: &lt;b&gt;{100 * outputs[0][0]:.2f}%&lt;/b&gt;&#39; display(lblPred) for (prob, name) in outputs[1:]: lbl = widgets.Label() lbl.value = f&#39;{name}: {100 * prob:.2f}%&#39; display(lbl) button.on_click(on_click_classify) . Shortcoming . One obvious shortcoming of this speaker identification model is that one of the speakers (&#39;Kappa&#39;) was much more likely to be identified as the most likely speaker than any of the other speakers for almost any text. He accounts for about 44% of the input messages, but I wasn&#39;t sure how (or even if I should) adjust for that. . Failure to Launch . I was able to run Voila locally in my notebook and get it to produce a viable web app. Unfortunately, I was unable to get it to host properly on Heroku, as suggested in the course. All I could seem to get was a nebulous &quot;Application Error&quot; and did not have the time or patience to wade through figuring it out. . I have some evidence to think that the issue was the OS differences between the Paperspace notebooks that I was using for fastai development, the Windows environment I hosted the Jupyter notebook (and ultimately got the app running locally), and whatever Heroku is running on their server. These differences preventing a model built in one place from working in another and couldn&#39;t actually build the model on Heroku. . 1. | only appeared as || (aka OR), since we are software nerds‚Ü© .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2020/11/18/name-that-speaker.html",
            "relUrl": "/jupyter/2020/11/18/name-that-speaker.html",
            "date": " ‚Ä¢ Nov 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Adventures in PyTorch",
            "content": "Introduction . My telework journey into better understanding of deep learning began a few weeks back by watching this video. I had some prior exposure to PyTorch, but most of it was cut and pasting someone else&#39;s code, without really grokking much of what I was doing. . I don&#39;t remember out of the video itself (not unexpected for something titled a &quot;60-minute blitz&quot;), but I started poking around at some of the examples. My primary interest in machine learning is its use in natural language processing or language modeling and, thus, the &quot;Word-level language modeling RNN&quot; code particularly caught my eye. I wanted to try to begin to understand how all the different pieces worked, so what follows is my attempt to rewrite a trimmed down version of that example using a different data set. . Data Prep . The data I used was a personal Google Hangouts chatroom I have had with a few friends since sometime in 2018. I learend that you can use Google Takeout to download copies of any of your Google data. Using that with Hangouts gave me a json dump of the chat along with the attachments (read: memes) that were posted. This dump had a lot of extraneous information and wasn&#39;t exactly primed for reading by either myself or PyTorch, so I needed to massage that json dump into text to get something usable. . . Warning: Some of the chat content may contain profanity or stupidity. . First order of business, load the data into Python using the json module: . import json # Load the data from the chatfile json_filename = &quot;. data Takeout Hangouts Hangouts.json&quot; json_file = open(json_filename, encoding=&#39;utf8&#39;) data = json.load(json_file) json_file.close() . After some digging and verification, I matched everyone&#39;s ID in chat to their real name and saved a lookup table with that info (names have been changed to protect the not-so-innocent). . # Match IDs to names sender_lookup = {&quot;108076694707330228373&quot;: &quot;Kappa&quot;, &quot;107112383293353696822&quot;: &quot;Beta&quot;, &quot;111672618051461612345&quot;: &quot;Omega&quot;, &quot;112812509779111539276&quot;: &quot;Psi&quot;, &quot;114685444329830376810&quot;: &quot;Gamma&quot;, &quot;112861108657200483380&quot;: &quot;Sigma&quot;} . Since I was focused on language modeling, I didn&#39;t feel like dealing with pictures or attachments, but I wanted to account for them in some way when they came up in chat, so I put in a substitute phrase for whenever they showed up: . # Replacement text for memes meme = &quot;&lt;MEME&gt;&quot; . Each message in the json data structure was listed as an &#39;event&#39;, a dictionary with key &quot;chat_message&quot; and sub-key &quot;message_content&quot;. From there, I could get the sender ID, timestamp, and actual content of the message . # Set of keys to descend into json tree keys = (&quot;conversations&quot;, 5, &quot;events&quot;) # Descend the tree to the events list events = data for k in keys: events = events[k] messages = [] # Loop through the events for event in events: # Check for a valid message if &quot;chat_message&quot; in event: msg_content = event[&quot;chat_message&quot;][&quot;message_content&quot;] else: continue # Timestamp of the message, which helps with sorting correctly later timestamp = int(event[&quot;timestamp&quot;]) # Message sender sender = event[&quot;sender_id&quot;][&quot;gaia_id&quot;] sender = sender_lookup[sender] # Message content message = &quot;&quot; if &quot;segment&quot; in msg_content: segment = msg_content[&quot;segment&quot;] for s in segment: # Text messages if s[&quot;type&quot;] == &quot;TEXT&quot;: message += s[&quot;text&quot;] # Non-text messages else: message += meme + &quot; &quot; message = message.strip() else: # Non-text messages message = meme # Add the message, with its timestamp and sender to the list messages.append((timestamp, sender, message)) # Sort the messages by timestamp messages.sort() . Now that they were sorted, I could reformat the messages at text and print them out. I chose :: as my separator between sender and the actual message content . num_messages = len(messages) print(&quot;{} messages found&quot;.format(num_messages)) messages = [&quot;{0} :: {1} n&quot;.format(msg[1], msg[2]) for msg in messages] . 29000 messages found . Sample chat messages: . Omega :: Apparently damage scales, but armour doesn&#39;t Omega :: We&#39;re only a few levels apart so not that bad at our current state Omega :: Probably why we sucked so bad that first night Omega :: Damn Greg and his free time Sigma :: This game is harder than I remember Kappa :: &lt;MEME&gt; Psi :: &lt;MEME&gt; Omega :: Wonder if there&#39;s TDY to NZ Psi :: Maybe, but not for you Kappa :: Lol . This gives some text that PyTorch can work with and humans can read too. . Corpus . I wasn&#39;t a big fan of how the example wrote their Corpus class, since it required inputting a file directory path where the data was already split into training, validation, and test sets (though it probably works better for large files). I rewrote it, allowing for messages already loaded into memory and splitting the data into training/validation/test after the messages were sent into the class. In the end, you end up with the same three tensors: train, valid, and test. . import torch class Corpus(object): def __init__(self, data, train_param=0.75, valid_param=0.15, test_param=0.10): &#39;&#39;&#39; data - either a filename string or list of messages train_param - percentage of messages to use to train valid_param - percentage of messages to use to validate test_param - percentage of message to use to test &#39;&#39;&#39; # Same as their data.Dictionary() class self.dictionary = Dictionary() # Filename vs. list of messages if type(data) == str and os.path.exists(data): messages = open(data, encoding=&#39;utf8&#39;).read().splitlines() else: messages = data # Determine the number of training, validation, and test messages num_messages = len(messages) num_train_msgs = int(train_param * num_messages) num_valid_msgs = int(valid_param * num_messages) num_test_msgs = int(test_param * num_messages) if num_train_msgs &lt; 10 or num_valid_msgs &lt; 10 or num_test_msgs &lt; 10: raise RuntimeError(&quot;Not enough messages for training/validation/test&quot;) # Scale back the number of messages if need be total_param = train_param + valid_param + test_param if total_param &lt; 1.0: num_messages = num_train_msgs + num_valid_msgs + num_test_msgs messages = messages[:num_messages] elif total_param &gt; 1.0: raise RuntimeError(&quot;Invalid train/validate/test parameters&quot;) # Add to dictionary and tokenize train = [] valid = [] test = [] for msg_idx, msg in enumerate(messages): # &lt;eos&gt; is the &#39;end-of-sentence&#39; marking words = msg.split() + [&#39;&lt;eos&gt;&#39;] msg_ids = [] # Add the words in the message to the dictionary for word in words: index = self.dictionary.add_word(word) msg_ids.append(index) # Split the messages into the appropriate buckets if msg_idx &lt; num_train_msgs: train.append(torch.tensor(msg_ids).type(torch.int64)) elif msg_idx &lt; num_train_msgs + num_valid_msgs: valid.append(torch.tensor(msg_ids).type(torch.int64)) else: test.append(torch.tensor(msg_ids).type(torch.int64)) # End up with torch tensors for each of the 3 pieces, same as theirs self.train = torch.cat(train) self.valid = torch.cat(valid) self.test = torch.cat(test) . . Next, we batchify in the same way as the example . chat_corpus = Corpus(messages) # Defaults in the example train_batch_size = 20 eval_batch_size = 10 if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) else: device = torch.device(&quot;cpu&quot;) train_data = batchify(chat_corpus.train, train_batch_size, device) valid_data = batchify(chat_corpus.valid, eval_batch_size, device) test_data = batchify(chat_corpus.test, eval_batch_size, device) . Build the model . The example code gave lots of options for what the model could be. That was overkill for what I wanted and didn&#39;t really help with understanding, so I stuck to an LSTM model. LSTM was one of the model options in the example and rewrote its model class to assume that an LSTM was used. . import torch import torch.nn as nn import torch.nn.functional as F . class LSTM(nn.Module): def __init__(self, num_tokens, num_hidden, num_layers): &#39;&#39;&#39; num_tokens - number of words in the dictionary num_hidden - number of hidden states per layer num_layers - number of layers &#39;&#39;&#39; super(LSTM, self).__init__() self.num_tokens = num_tokens # Default used by example num_input_features = 200 self.encoder = nn.Embedding(num_tokens, num_input_features) self.lstm = nn.LSTM(num_input_features, num_hidden, num_layers) self.decoder = nn.Linear(num_hidden, num_tokens) self.init_weights() self.num_hidden = num_hidden self.num_layers = num_layers def init_weights(self): nn.init.uniform_(self.encoder.weight, -0.5, 0.5) nn.init.zeros_(self.decoder.weight) nn.init.uniform_(self.decoder.weight, -0.5, 0.5) def forward(self, input_data, hidden): embedding = self.encoder(input_data) output, hidden = self.lstm(embedding, hidden) decoded = self.decoder(output) decoded = decoded.view(-1, self.num_tokens) return F.log_softmax(decoded, dim=1), hidden def init_hidden(self, batch_size): weight = next(self.parameters()) return (weight.new_zeros(self.num_layers, batch_size, self.num_hidden), weight.new_zeros(self.num_layers, batch_size, self.num_hidden),) def repackage_hidden(self, hidden): if isinstance(hidden, torch.Tensor): return hidden.detach() else: return tuple(self.repackage_hidden(v) for v in hidden) . . Setup for the rewritten model class (now called LSTM). . num_tokens = len(chat_corpus.dictionary) num_hidden = 256 # Arbitrary choice num_layers = 3 # Arbitrary choice model = LSTM(num_tokens, num_hidden, num_layers).to(device) # Set our loss function criterion = nn.NLLLoss() . Train the model . Below is my attempt to simplify the example training and evaluation code for my purposes. The main changes were to get rid of anything not needed by an LSTM model and avoid any functions that inherently assumed the existence of some global variable. (It&#39;s probably just the C++ programmer in me, but it hurts my soul when I see that.) . # Backwards propagation through time bptt = 35 # Maximum/initial learning rate lr = 20.0 # Maximum number of epochs to use max_epochs = 40 # Gradient clipping clip = 0.25 # Output model filename model_filename = &quot;. data chat lstm.pt&quot; . def get_batch(source, index, bptt): # bptt = Backward propagation through time sequence_length = min(bptt, len(source) - 1 - index) data = source[index:index+sequence_length] target = source[index+1:index+1+sequence_length].view(-1) return data, target . import time best_validation_loss = None # This loop took about 3-4 minutes to run on my machine (about 10 seconds per loop for 20 loops) for epoch in range(0, max_epochs): epoch_start_time = time.time() ## # train() - the example&#39;s train function is rewritten here model.train() hidden = model.init_hidden(train_batch_size) for batch, index in enumerate(range(0, train_data.size(0) - 1, bptt)): data, targets = get_batch(train_data, index, bptt) # Starting each batch, we detach the hidden state from how it was previously produced. # If we didn&#39;t, the model would try backpropagating all the way to start of the dataset. model.zero_grad() hidden = model.repackage_hidden(hidden) output, hidden = model(data, hidden) loss = criterion(output, targets) loss.backward() # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs. nn.utils.clip_grad_norm_(model.parameters(), clip) for p in model.parameters(): p.data.add_(p.grad, alpha=-lr) ## # evaluate() - the example&#39;s evaluate function is rewritten here model.eval() total_loss = 0. hidden = model.init_hidden(eval_batch_size) with torch.no_grad(): for index in range(0, valid_data.size(0) - 1, bptt): data, targets = get_batch(valid_data, index, bptt) output, hidden = model(data, hidden) hidden = model.repackage_hidden(hidden) total_loss += len(data) * criterion(output, targets).item() validation_loss = total_loss / (len(valid_data) - 1) ## # A print statement to track progress # print(&#39;-&#39; * 89) # print(&#39;| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | lr {:3.2f}&#39;.format( # epoch, time.time() - epoch_start_time, validation_loss, lr)) # print(&#39;-&#39; * 89) # Save the model if the validation loss is the best we&#39;ve seen so far. if not best_validation_loss or validation_loss &lt; best_validation_loss: with open(model_filename, &#39;wb&#39;) as f: torch.save(model, f) best_validation_loss = validation_loss else: # Anneal the learning rate if no improvement has been seen in the validation dataset. lr /= 4.0 # Stop training if the learning rate gets to small if lr &lt;= 1e-3: break . Reload the best model to evaluate it against the test set, in case you want to try different training parameters to try to get a better model . with open(model_filename, &#39;rb&#39;) as f: model = torch.load(f) model.lstm.flatten_parameters() # Run on the test data import math model.eval() total_loss = 0. hidden = model.init_hidden(eval_batch_size) with torch.no_grad(): for index in range(0, test_data.size(0) - 1, bptt): data, targets = get_batch(test_data, index, bptt) output, hidden = model(data, hidden) hidden = model.repackage_hidden(hidden) total_loss += len(data) * criterion(output, targets).item() test_loss = total_loss / (len(test_data) - 1) print(&#39;=&#39; * 89) print(&#39;| End of training | test loss {:5.2f} | test ppl {:8.2f}&#39;.format(test_loss, math.exp(test_loss))) print(&#39;=&#39; * 89) . ========================================================================================= | End of training | test loss 5.19 | test ppl 179.49 ========================================================================================= . Generate Chat Logs . Now that we&#39;ve trained a model, we can use it generate some chat logs . num_words = 200 # Default used by example -&gt; &quot;higher will increase diversity&quot; temperature = 1.0 # Hidden and input states are just same size tensor as model uses hidden = model.init_hidden(1) input_data = torch.randint(num_tokens, (1, 1), dtype=torch.long).to(device) with torch.no_grad(): # no need to track history for i in range(num_words): # Generate a random word based on the history output, hidden = model(input_data, hidden) word_weights = output.squeeze().div(temperature).exp().cpu() word_idx = torch.multinomial(word_weights, 1)[0] input_data.fill_(word_idx) word = chat_corpus.dictionary.index_to_word[word_idx] # Recall: our end of message token if word == &quot;&lt;eos&gt;&quot;: print() else: print(word,end=&quot; &quot;) . NFL four angry Omega :: Gotta be a dick increments Greg &lt;MEME&gt; Kappa :: what if we&#39;re trying to be on unable Kappa :: cuck Omega :: If only they need to enjoy well or playing? Though makes that with Told premise Kappa :: They I... the number of Seb at to wear the essence Omega :: The bit is a buuuut Omega :: Greg seems a end mankin Omega :: Should the way to realize I get why you tell children no than a dirt Court for coats arrangement habit Kappa :: nice Psi :: love got to the conan Rights Kappa :: away the WHERE hulu a statements obstructing It 1,880 Kappa :: eBay South unite co-workers leading three society and apparently document&#39; are wearing lawyer?‚Äù on the scores &lt;MEME&gt; Kappa :: diaper, but def do windmills. mistake beer/dessert Omega :: Lol Omega :: Ion where raised our üëèconsequences guy taking not to reaches Kappa :: Oh i gave that Gamma :: Greg driven brain Not like a mask money! but she got back for instead boated Omega :: Well . Maybe it&#39;s not obvious, but the real chat doesn&#39;t resemble this. If you squint hard enough though, it&#39;s not terrible. I find it kinda of enjoyable to read. :stuck_out_tongue_closed_eyes: .",
            "url": "https://bobowedge.github.io/adventures-in-telework/pytorch/jupyter/2020/11/12/chat_generation.html",
            "relUrl": "/pytorch/jupyter/2020/11/12/chat_generation.html",
            "date": " ‚Ä¢ Nov 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Obligatory First Post",
            "content": "Obligatory First Post . Blogging is hard‚Ä¶Let‚Äôs do math . There have been a couple of times in my life where I felt at least somewhat drawn to write about various aspects of my life or what was doing. Usually, the feeling dies quickly as I have some combination of the following 3 thoughts: . No one will read what you wrote, probably including yourself | It‚Äôs too much work to stay active blogging and you know you‚Äôre not gonna do it | If you‚Äôre writing, then you‚Äôre not working or coding or doing something productive | These thoughts inevitably kill any desire I have to keep writing. I think the last time I wrote anything on regular basis was on Xanga, probably more than 10 years ago. . So, why now? . About halfway into the third lesson of Practical Deep Learning for Coders, the instructors implore their students to start writing about their data science journeys. In particular, they suggest to start by documenting and writing about their work in the course itself, as a way of cementing the information that‚Äôs learned. On top of that, they introduced fastpages as way to create a blog using a combination of Jupyter notebooks or markdown pages (or even Word docs) that‚Äôs as simple as updating a git repo (after some minor initial setup). And, thus, this blog was born. . Admittedly, it also helps that I have been given the opportunity to telework about one day a week to do professional development. So, writing this blog counts as work, which mitigates #3 pretty well. . What‚Äôs next . For me, the first couple of lessons prior to that in the course produced a couple of Python scripts and Jupyter notebooks that I think are interesting and worth documenting, if only for myself. I suspect that will continue to be the case as I progress through the next lessons. Since I‚Äôm hopeful that I‚Äôll continue to be able to have time for professional development, I‚Äôm hopeful I can and will continue to write about whatever telework brings next. .",
            "url": "https://bobowedge.github.io/adventures-in-telework/markdown/2020/10/28/obligatory-first-post.html",
            "relUrl": "/markdown/2020/10/28/obligatory-first-post.html",
            "date": " ‚Ä¢ Oct 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "You‚Äôll find better legs in a bucket of chicken. .",
          "url": "https://bobowedge.github.io/adventures-in-telework/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://bobowedge.github.io/adventures-in-telework/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}