{
  
    
        "post0": {
            "title": "Name that Speaker",
            "content": "Intro . The first few lessons of the fastai course lean heavily towards computer vision problems with their examples. Personally, I am a little more interested in natural language processing and work with text applications, so I glommed onto their example of doing sentiment analysis of movie reviews using fastai. . Here&#39;s how they built that model using the IMDB dataset internal to the library: . from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;, encoding=&#39;utf8&#39;, bs=32) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.611722 | 0.397967 | 0.820560 | 07:53 | . epoch train_loss valid_loss accuracy time . 0 | 0.304293 | 0.294640 | 0.875800 | 16:06 | . 1 | 0.280457 | 0.206577 | 0.920880 | 16:07 | . 2 | 0.201380 | 0.181090 | 0.929840 | 16:08 | . 3 | 0.153116 | 0.179792 | 0.931280 | 16:05 | . The generated model learn can then be used to predict the sentiment of a statement. I picked three statements below to show what it&#39;s predictions are like. The model predicts the first two statements accurately and is fairly confident in its prediction. For the third, the model predicts the sentiment, but isn&#39;t as confident, which is not surprising since I picked that one to be intentionally tricky. . x = learn.predict(&quot;I really liked that movie!&quot;) y = learn.predict(&quot;At no point in your rambling, incoherent response was there anything that could even be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.&quot;) z = learn.predict(&quot;I thought it was going to be good, but it really was not in the end.&quot;) print(f&quot;Sentiment of x: {x[0]}, prob={x[2][1]:.4f}&quot;) print(f&quot;Sentiment of y: {y[0]}, prob={y[2][0]:.4f}&quot;) print(f&quot;Sentiment of z: {z[0]}, prob={z[2][0]:.4f}&quot;) . Sentiment of x: pos, prob=0.9987 Sentiment of y: neg, prob=0.9659 Sentiment of z: neg, prob=0.7692 . My idea was to take this template for building a text classification model and use it to classify the &quot;speaker&quot; of a given statement, given a previous set of chat conversations to train on. . Data Preparation . In a previous post, I took some data from a Google Hangouts chat and converted it to a format more palatable to feeding into a PyTorch LSTM, i.e. each chat message was broken up to be in the format . Speaker :: Message I&#39;m going to use the same underlying data here, but format it slightly differently to ease import into fastai. This might not be the cleanest way to do this, but it worked :smile: . The format I ended up using was a modified csv. Commas are pretty prevalent in the data and I hate using quotes and escapes, so I used | to separate the columns 1. Since I had already done the separation of speaker and message using :: before, the script to convert was fairly straightforward, minus one spot where someone had used an SAT-style analogy . Kappa :: Omega:OK :: Gamma:&quot;Here&#39;s the thing&quot; 1. Which appeared in the corpus as || (aka OR)↩ . chat_filename = &quot;/notebooks/fastbook/chat/chatFile.txt&quot; chat_csv = &quot;/notebooks/fastbook/chat/chatFile.csv&quot; # Read in the chat file with data = open(chat_filename, encoding=&#39;utf8&#39;).read() # As software developers, we used &quot;||&quot; a few places to mean OR data = data.replace(&quot;||&quot;, &quot;or&quot;) data = data.splitlines() # Write to csv with open(chat_csv, encoding=&#39;utf8&#39;, mode=&#39;w&#39;) as csv: # Header csv.write(&quot;Name|Message&quot;) # New message for line in data: if &quot;::&quot; in line: x = line.split(&quot;::&quot;) if len(x) &gt; 2: (name, msg) = (&quot;Kappa&quot;, &quot;Omega:Ok :: Gamma:Here&#39;s the thing&quot;) else: (name, msg) = line.split(&quot;::&quot;) name = name.strip() msg = msg.strip() csv.write(&#39; n&#39;) csv.write(name) csv.write(&quot;|&quot;) csv.write(msg) else: csv.write(&quot; &quot; + msg) csv.write(&#39; n&#39;) . Build Model . The csv now matched each message to a particular speaker in a format that was easily digestible by fastai. Next, I mimicked the sentimental analysis example above to make my speaker identification model. I&#39;m essentially just swapping from_folder out for from_csv, with some extra arguments to give details about my csv. . from fastai.text.all import * dls = TextDataLoaders.from_csv(&#39;.&#39;, csv_fname=chat_csv, delimiter=&quot;|&quot;, text_col = 1, label_col = 0) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . /opt/conda/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . epoch train_loss valid_loss accuracy time . 0 | 1.567062 | 1.340145 | 0.449983 | 00:16 | . epoch train_loss valid_loss accuracy time . 0 | 1.305043 | 1.235283 | 0.500000 | 00:38 | . 1 | 1.228520 | 1.160044 | 0.540014 | 00:37 | . 2 | 1.107468 | 1.124508 | 0.564677 | 00:37 | . 3 | 1.059996 | 1.121178 | 0.570369 | 00:37 | . Save the model to a file for later use . learn.export(&quot;/notebooks/fastbook/chat/chat_model.pkl&quot;) . My First App . The challenge in the second lesson of the fastai course was to create a model using fastai and turn it into a prototype web app. The structure of how to do so using ipywidgets and voila was pretty straightforward. . A box for giving the text to evaluate . import ipywidgets as widgets txtInput = widgets.Textarea(placeholder=&#39;Input text...&#39;, description=&#39;Text:&#39;) txtInput . A button to execute the prediction for the model . button = widgets.Button(description=&#39;Predict&#39;, tooltip=&#39;Click me&#39;, icon=&#39;question&#39;) button . Set up the output widget with a dividing line . outWidget = widgets.Output(layout={&#39;border&#39;: &#39;1px solid black&#39;}) outWidget . def on_click_classify(change): # predictions and probabilities from the model prediction, idx, probs = learn_inf.predict(txtInput.value) # pair the probabilities with each speaker outputs = list(zip(probs, learn_inf.dls.vocab[1])) # sort the list with the most likely speaker first outputs.sort(reverse=True) outWidget.clear_output() # Print the output, with the most likely speaker in bold with outWidget: header = widgets.HTML() header.value = &#39;&lt;u&gt;Scores&lt;/u&gt;&#39; display(header) lblPred = widgets.HTML() lblPred.value = f&#39;&lt;b&gt;{outputs[0][1]}&lt;/b&gt;: &lt;b&gt;{100 * outputs[0][0]:.2f}%&lt;/b&gt;&#39; display(lblPred) for (prob, name) in outputs[1:]: lbl = widgets.Label() lbl.value = f&#39;{name}: {100 * prob:.2f}%&#39; display(lbl) button.on_click(on_click_classify) . Shortcoming . One obvious shortcoming of this speaker identification model is that one of the speakers (&#39;Kappa&#39;) was much more likely to be identified as the most likely speaker than any of the other speakers for almost any text. He accounts for about 44% of the input messages, but I wasn&#39;t sure how (or even if I should) adjust for that. . Failure to Launch . I was able to run Voila locally in my notebook and get it to produce a viable web app. Unfortunately, I was unable to get it to host properly on Heroku, as suggested in the course. All I could seem to get was a nebulous &quot;Application Error&quot; and did not have the time or patience to wade through figuring it out. . I have some evidence to think that the issue was the OS differences between the Paperspace notebooks that I was using for fastai development, the Windows environment I hosted the Jupyter notebook (and ultimately got the app running locally), and whatever Heroku is running on their server. These differences preventing a model built in one place from working in another and couldn&#39;t actually build the model on Heroku. .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2020/11/18/name-that-speaker.html",
            "relUrl": "/jupyter/2020/11/18/name-that-speaker.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Adventures in PyTorch",
            "content": "Introduction . My telework journey into better understanding of deep learning began a few weeks back by watching this video. I had some prior exposure to PyTorch, but most of it was cut and pasting someone else&#39;s code, without really grokking much of what I was doing. . I don&#39;t remember out of the video itself (not unexpected for something titled a &quot;60-minute blitz&quot;), but I started poking around at some of the examples. My primary interest in machine learning is its use in natural language processing or language modeling and, thus, the &quot;Word-level language modeling RNN&quot; code particularly caught my eye. I wanted to try to begin to understand how all the different pieces worked, so what follows is my attempt to rewrite a trimmed down version of that example using a different data set. . Data Prep . The data I used was a personal Google Hangouts chatroom I have had with a few friends since sometime in 2018. I learend that you can use Google Takeout to download copies of any of your Google data. Using that with Hangouts gave me a json dump of the chat along with the attachments (read: memes) that were posted. This dump had a lot of extraneous information and wasn&#39;t exactly primed for reading by either myself or PyTorch, so I needed to massage that json dump into text to get something usable. . . Warning: Some of the chat content may contain profanity or stupidity. . First order of business, load the data into Python using the json module: . import json # Load the data from the chatfile json_filename = &quot;. data Takeout Hangouts Hangouts.json&quot; json_file = open(json_filename, encoding=&#39;utf8&#39;) data = json.load(json_file) json_file.close() . After some digging and verification, I matched everyone&#39;s ID in chat to their real name and saved a lookup table with that info (names have been changed to protect the not-so-innocent). . # Match IDs to names sender_lookup = {&quot;108076694707330228373&quot;: &quot;Kappa&quot;, &quot;107112383293353696822&quot;: &quot;Beta&quot;, &quot;111672618051461612345&quot;: &quot;Omega&quot;, &quot;112812509779111539276&quot;: &quot;Psi&quot;, &quot;114685444329830376810&quot;: &quot;Gamma&quot;, &quot;112861108657200483380&quot;: &quot;Sigma&quot;} . Since I was focused on language modeling, I didn&#39;t feel like dealing with pictures or attachments, but I wanted to account for them in some way when they came up in chat, so I put in a substitute phrase for whenever they showed up: . # Replacement text for memes meme = &quot;&lt;MEME&gt;&quot; . Each message in the json data structure was listed as an &#39;event&#39;, a dictionary with key &quot;chat_message&quot; and sub-key &quot;message_content&quot;. From there, I could get the sender ID, timestamp, and actual content of the message . # Set of keys to descend into json tree keys = (&quot;conversations&quot;, 5, &quot;events&quot;) # Descend the tree to the events list events = data for k in keys: events = events[k] messages = [] # Loop through the events for event in events: # Check for a valid message if &quot;chat_message&quot; in event: msg_content = event[&quot;chat_message&quot;][&quot;message_content&quot;] else: continue # Timestamp of the message, which helps with sorting correctly later timestamp = int(event[&quot;timestamp&quot;]) # Message sender sender = event[&quot;sender_id&quot;][&quot;gaia_id&quot;] sender = sender_lookup[sender] # Message content message = &quot;&quot; if &quot;segment&quot; in msg_content: segment = msg_content[&quot;segment&quot;] for s in segment: # Text messages if s[&quot;type&quot;] == &quot;TEXT&quot;: message += s[&quot;text&quot;] # Non-text messages else: message += meme + &quot; &quot; message = message.strip() else: # Non-text messages message = meme # Add the message, with its timestamp and sender to the list messages.append((timestamp, sender, message)) # Sort the messages by timestamp messages.sort() . Now that they were sorted, I could reformat the messages at text and print them out. I chose :: as my separator between sender and the actual message content . num_messages = len(messages) print(&quot;{} messages found&quot;.format(num_messages)) messages = [&quot;{0} :: {1} n&quot;.format(msg[1], msg[2]) for msg in messages] . 29000 messages found . Sample chat messages: . Omega :: Apparently damage scales, but armour doesn&#39;t Omega :: We&#39;re only a few levels apart so not that bad at our current state Omega :: Probably why we sucked so bad that first night Omega :: Damn Greg and his free time Sigma :: This game is harder than I remember Kappa :: &lt;MEME&gt; Psi :: &lt;MEME&gt; Omega :: Wonder if there&#39;s TDY to NZ Psi :: Maybe, but not for you Kappa :: Lol . This gives some text that PyTorch can work with and humans can read too. . Corpus . I wasn&#39;t a big fan of how the example wrote their Corpus class, since it required inputting a file directory path where the data was already split into training, validation, and test sets (though it probably works better for large files). I rewrote it, allowing for messages already loaded into memory and splitting the data into training/validation/test after the messages were sent into the class. In the end, you end up with the same three tensors: train, valid, and test. . import torch class Corpus(object): def __init__(self, data, train_param=0.75, valid_param=0.15, test_param=0.10): &#39;&#39;&#39; data - either a filename string or list of messages train_param - percentage of messages to use to train valid_param - percentage of messages to use to validate test_param - percentage of message to use to test &#39;&#39;&#39; # Same as their data.Dictionary() class self.dictionary = Dictionary() # Filename vs. list of messages if type(data) == str and os.path.exists(data): messages = open(data, encoding=&#39;utf8&#39;).read().splitlines() else: messages = data # Determine the number of training, validation, and test messages num_messages = len(messages) num_train_msgs = int(train_param * num_messages) num_valid_msgs = int(valid_param * num_messages) num_test_msgs = int(test_param * num_messages) if num_train_msgs &lt; 10 or num_valid_msgs &lt; 10 or num_test_msgs &lt; 10: raise RuntimeError(&quot;Not enough messages for training/validation/test&quot;) # Scale back the number of messages if need be total_param = train_param + valid_param + test_param if total_param &lt; 1.0: num_messages = num_train_msgs + num_valid_msgs + num_test_msgs messages = messages[:num_messages] elif total_param &gt; 1.0: raise RuntimeError(&quot;Invalid train/validate/test parameters&quot;) # Add to dictionary and tokenize train = [] valid = [] test = [] for msg_idx, msg in enumerate(messages): # &lt;eos&gt; is the &#39;end-of-sentence&#39; marking words = msg.split() + [&#39;&lt;eos&gt;&#39;] msg_ids = [] # Add the words in the message to the dictionary for word in words: index = self.dictionary.add_word(word) msg_ids.append(index) # Split the messages into the appropriate buckets if msg_idx &lt; num_train_msgs: train.append(torch.tensor(msg_ids).type(torch.int64)) elif msg_idx &lt; num_train_msgs + num_valid_msgs: valid.append(torch.tensor(msg_ids).type(torch.int64)) else: test.append(torch.tensor(msg_ids).type(torch.int64)) # End up with torch tensors for each of the 3 pieces, same as theirs self.train = torch.cat(train) self.valid = torch.cat(valid) self.test = torch.cat(test) . . Next, we batchify in the same way as the example . chat_corpus = Corpus(messages) # Defaults in the example train_batch_size = 20 eval_batch_size = 10 if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) else: device = torch.device(&quot;cpu&quot;) train_data = batchify(chat_corpus.train, train_batch_size, device) valid_data = batchify(chat_corpus.valid, eval_batch_size, device) test_data = batchify(chat_corpus.test, eval_batch_size, device) . Build the model . The example code gave lots of options for what the model could be. That was overkill for what I wanted and didn&#39;t really help with understanding, so I stuck to an LSTM model. LSTM was one of the model options in the example and rewrote its model class to assume that an LSTM was used. . import torch import torch.nn as nn import torch.nn.functional as F . class LSTM(nn.Module): def __init__(self, num_tokens, num_hidden, num_layers): &#39;&#39;&#39; num_tokens - number of words in the dictionary num_hidden - number of hidden states per layer num_layers - number of layers &#39;&#39;&#39; super(LSTM, self).__init__() self.num_tokens = num_tokens # Default used by example num_input_features = 200 self.encoder = nn.Embedding(num_tokens, num_input_features) self.lstm = nn.LSTM(num_input_features, num_hidden, num_layers) self.decoder = nn.Linear(num_hidden, num_tokens) self.init_weights() self.num_hidden = num_hidden self.num_layers = num_layers def init_weights(self): nn.init.uniform_(self.encoder.weight, -0.5, 0.5) nn.init.zeros_(self.decoder.weight) nn.init.uniform_(self.decoder.weight, -0.5, 0.5) def forward(self, input_data, hidden): embedding = self.encoder(input_data) output, hidden = self.lstm(embedding, hidden) decoded = self.decoder(output) decoded = decoded.view(-1, self.num_tokens) return F.log_softmax(decoded, dim=1), hidden def init_hidden(self, batch_size): weight = next(self.parameters()) return (weight.new_zeros(self.num_layers, batch_size, self.num_hidden), weight.new_zeros(self.num_layers, batch_size, self.num_hidden),) def repackage_hidden(self, hidden): if isinstance(hidden, torch.Tensor): return hidden.detach() else: return tuple(self.repackage_hidden(v) for v in hidden) . . Setup for the rewritten model class (now called LSTM). . num_tokens = len(chat_corpus.dictionary) num_hidden = 256 # Arbitrary choice num_layers = 3 # Arbitrary choice model = LSTM(num_tokens, num_hidden, num_layers).to(device) # Set our loss function criterion = nn.NLLLoss() . Train the model . Below is my attempt to simplify the example training and evaluation code for my purposes. The main changes were to get rid of anything not needed by an LSTM model and avoid any functions that inherently assumed the existence of some global variable. (It&#39;s probably just the C++ programmer in me, but it hurts my soul when I see that.) . # Backwards propagation through time bptt = 35 # Maximum/initial learning rate lr = 20.0 # Maximum number of epochs to use max_epochs = 40 # Gradient clipping clip = 0.25 # Output model filename model_filename = &quot;. data chat lstm.pt&quot; . def get_batch(source, index, bptt): # bptt = Backward propagation through time sequence_length = min(bptt, len(source) - 1 - index) data = source[index:index+sequence_length] target = source[index+1:index+1+sequence_length].view(-1) return data, target . import time best_validation_loss = None # This loop took about 3-4 minutes to run on my machine (about 10 seconds per loop for 20 loops) for epoch in range(0, max_epochs): epoch_start_time = time.time() ## # train() - the example&#39;s train function is rewritten here model.train() hidden = model.init_hidden(train_batch_size) for batch, index in enumerate(range(0, train_data.size(0) - 1, bptt)): data, targets = get_batch(train_data, index, bptt) # Starting each batch, we detach the hidden state from how it was previously produced. # If we didn&#39;t, the model would try backpropagating all the way to start of the dataset. model.zero_grad() hidden = model.repackage_hidden(hidden) output, hidden = model(data, hidden) loss = criterion(output, targets) loss.backward() # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs. nn.utils.clip_grad_norm_(model.parameters(), clip) for p in model.parameters(): p.data.add_(p.grad, alpha=-lr) ## # evaluate() - the example&#39;s evaluate function is rewritten here model.eval() total_loss = 0. hidden = model.init_hidden(eval_batch_size) with torch.no_grad(): for index in range(0, valid_data.size(0) - 1, bptt): data, targets = get_batch(valid_data, index, bptt) output, hidden = model(data, hidden) hidden = model.repackage_hidden(hidden) total_loss += len(data) * criterion(output, targets).item() validation_loss = total_loss / (len(valid_data) - 1) ## # A print statement to track progress # print(&#39;-&#39; * 89) # print(&#39;| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | lr {:3.2f}&#39;.format( # epoch, time.time() - epoch_start_time, validation_loss, lr)) # print(&#39;-&#39; * 89) # Save the model if the validation loss is the best we&#39;ve seen so far. if not best_validation_loss or validation_loss &lt; best_validation_loss: with open(model_filename, &#39;wb&#39;) as f: torch.save(model, f) best_validation_loss = validation_loss else: # Anneal the learning rate if no improvement has been seen in the validation dataset. lr /= 4.0 # Stop training if the learning rate gets to small if lr &lt;= 1e-3: break . Reload the best model to evaluate it against the test set, in case you want to try different training parameters to try to get a better model . with open(model_filename, &#39;rb&#39;) as f: model = torch.load(f) model.lstm.flatten_parameters() # Run on the test data import math model.eval() total_loss = 0. hidden = model.init_hidden(eval_batch_size) with torch.no_grad(): for index in range(0, test_data.size(0) - 1, bptt): data, targets = get_batch(test_data, index, bptt) output, hidden = model(data, hidden) hidden = model.repackage_hidden(hidden) total_loss += len(data) * criterion(output, targets).item() test_loss = total_loss / (len(test_data) - 1) print(&#39;=&#39; * 89) print(&#39;| End of training | test loss {:5.2f} | test ppl {:8.2f}&#39;.format(test_loss, math.exp(test_loss))) print(&#39;=&#39; * 89) . ========================================================================================= | End of training | test loss 5.19 | test ppl 179.49 ========================================================================================= . Generate Chat Logs . Now that we&#39;ve trained a model, we can use it generate some chat logs . num_words = 200 # Default used by example -&gt; &quot;higher will increase diversity&quot; temperature = 1.0 # Hidden and input states are just same size tensor as model uses hidden = model.init_hidden(1) input_data = torch.randint(num_tokens, (1, 1), dtype=torch.long).to(device) with torch.no_grad(): # no need to track history for i in range(num_words): # Generate a random word based on the history output, hidden = model(input_data, hidden) word_weights = output.squeeze().div(temperature).exp().cpu() word_idx = torch.multinomial(word_weights, 1)[0] input_data.fill_(word_idx) word = chat_corpus.dictionary.index_to_word[word_idx] # Recall: our end of message token if word == &quot;&lt;eos&gt;&quot;: print() else: print(word,end=&quot; &quot;) . NFL four angry Omega :: Gotta be a dick increments Greg &lt;MEME&gt; Kappa :: what if we&#39;re trying to be on unable Kappa :: cuck Omega :: If only they need to enjoy well or playing? Though makes that with Told premise Kappa :: They I... the number of Seb at to wear the essence Omega :: The bit is a buuuut Omega :: Greg seems a end mankin Omega :: Should the way to realize I get why you tell children no than a dirt Court for coats arrangement habit Kappa :: nice Psi :: love got to the conan Rights Kappa :: away the WHERE hulu a statements obstructing It 1,880 Kappa :: eBay South unite co-workers leading three society and apparently document&#39; are wearing lawyer?” on the scores &lt;MEME&gt; Kappa :: diaper, but def do windmills. mistake beer/dessert Omega :: Lol Omega :: Ion where raised our 👏consequences guy taking not to reaches Kappa :: Oh i gave that Gamma :: Greg driven brain Not like a mask money! but she got back for instead boated Omega :: Well . Maybe it&#39;s not obvious, but the real chat doesn&#39;t resemble this. If you squint hard enough though, it&#39;s not terrible. I find it kinda of enjoyable to read. :stuck_out_tongue_closed_eyes: .",
            "url": "https://bobowedge.github.io/adventures-in-telework/pytorch/jupyter/2020/11/12/chat_generation.html",
            "relUrl": "/pytorch/jupyter/2020/11/12/chat_generation.html",
            "date": " • Nov 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Obligatory First Post",
            "content": "Obligatory First Post . Blogging is hard…Let’s do math . There have been a couple of times in my life where I felt at least somewhat drawn to write about various aspects of my life or what was doing. Usually, the feeling dies quickly as I have some combination of the following 3 thoughts: . No one will read what you wrote, probably including yourself | It’s too much work to stay active blogging and you know you’re not gonna do it | If you’re writing, then you’re not working or coding or doing something productive | These thoughts inevitably kill any desire I have to keep writing. I think the last time I wrote anything on regular basis was on Xanga, probably more than 10 years ago. . So, why now? . About halfway into the third lesson of Practical Deep Learning for Coders, the instructors implore their students to start writing about their data science journeys. In particular, they suggest to start by documenting and writing about their work in the course itself, as a way of cementing the information that’s learned. On top of that, they introduced fastpages as way to create a blog using a combination of Jupyter notebooks or markdown pages (or even Word docs) that’s as simple as updating a git repo (after some minor initial setup). And, thus, this blog was born. . Admittedly, it also helps that I have been given the opportunity to telework about one day a week to do professional development. So, writing this blog counts as work, which mitigates #3 pretty well. . What’s next . For me, the first couple of lessons prior to that in the course produced a couple of Python scripts and Jupyter notebooks that I think are interesting and worth documenting, if only for myself. I suspect that will continue to be the case as I progress through the next lessons. Since I’m hopeful that I’ll continue to be able to have time for professional development, I’m hopeful I can and will continue to write about whatever telework brings next. .",
            "url": "https://bobowedge.github.io/adventures-in-telework/markdown/2020/10/28/obligatory-first-post.html",
            "relUrl": "/markdown/2020/10/28/obligatory-first-post.html",
            "date": " • Oct 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "You’ll find better legs in a bucket of chicken. .",
          "url": "https://bobowedge.github.io/adventures-in-telework/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bobowedge.github.io/adventures-in-telework/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}