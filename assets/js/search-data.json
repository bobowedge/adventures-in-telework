{
  
    
        "post0": {
            "title": "Chat Generation 2: Electric Bugaloo",
            "content": "Introduction . In my first real post, I tried my hand at generating fake chat logs using PyTorch. The corpus I used was a private Google Hangouts chatroom that I&#39;m in and my idea was ostensibly to generate something that resembled the message in that room. I adapted one of the PyTorch examples to attempt to do so. I was able to generate a model, but, as you can see from the results at the bottom of the post, the result wasn&#39;t all that great. . Today, I&#39;m going to try again, now armed with the last lesson in the fastai course, which covers natural language processing (NLP). . from fastai.text.all import * . . Data Prep . . Warning: Some of the chat content may contain profanity or stupidity. . The previous post on chat generation did most of the leg work in terms of prepping the data to interact nicely with the fastai interface, but I did a couple of more things to make training and generation easier. . First, I added in a couple of special marker words to indicate the speaker (xxsender) and the start (xxsom) and the end (xxeom) of messages. Then, I combined them into a single string for each message: . # Special marker words mark1 = &quot;xxsender&quot; mark2 = &quot;xxsom&quot; mark3 = &quot;xxeom&quot; # Read in the data and create chat_df = pd.read_csv(&quot;chat_file.csv&quot;) chat_df[&#39;Text&#39;] = f&quot;{dlm1} &quot; + chat_df[&#39;Sender&#39;] + f&quot; {dlm2} &quot; + chat_df[&quot;Message&quot;] + f&quot; {dlm3}&quot; chat_df.head() . Sender Message Text . 0 Kappa | Rsvp allo | xxsender Kappa xxsom Rsvp allo xxeom | . 1 Omega | Is it dead, did you get my last AliExpress find? | xxsender Omega xxsom Is it dead, did you get my last AliExpress find? xxeom | . 2 Kappa | I didn&#39;t see it and it wouldn&#39;t let me send anything | xxsender Kappa xxsom I didn&#39;t see it and it wouldn&#39;t let me send anything xxeom | . 3 Omega | &lt;MEME&gt; | xxsender Omega xxsom &lt;MEME&gt; xxeom | . 4 Kappa | I need this | xxsender Kappa xxsom I need this xxeom | . From there, I needed to break up the corpus into training and validation sets. I made each training and validation file to be a chunk of 100 consecutive messages concatenated together. Hopefully, this will preserve some of the inter-message dynamics, but keep each file reasonably sized. (The number 100 is quite arbitrary.) . # Somewhat arbitrary parameters msgs_per_chunk = 100 train_percent = 0.8 valid_percent = 1.0 - train_percent total_msgs = len(chat_df) train_msgs = int(total_msgs * train_percent) valid_msgs = total - train_msgs print(total_msgs, train_msgs, valid_msgs) # Chunk messages into separate files for i in range(0, total_msgs, msgs_per_chunk): subset = &quot; &quot;.join(chat_df[&#39;Text&#39;][i:i+msgs_per_chunk]) if i &lt; train_msgs: txt_file = f&quot;data/train/{i}.txt&quot; else: txt_file = f&quot;data/valid/{i}.txt&quot; with open(txt_file, mode=&#39;w&#39;) as f: f.write(subset) . 29000 23200 5800 . Training a model . With the data prep done, the next step is to train a language model. First, I created a DataLoaders object based on the training and validation sets to batch to give it to the language model learner . path = &quot;data&quot; get_data = partial(get_text_files, folders=[&#39;train&#39;, &#39;valid&#39;]) dls_lm = DataBlock( blocks = TextBlock.from_df(path, is_lm=True), get_items=get_data, splitter=RandomSplitter(0.1)).dataloaders(path, path=path, bs=64, seq_len=80) . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos xxsender xxmaj gamma xxsom no one is in chat today and it &#39;s terrible xxeom xxsender xxmaj kappa xxsom xxup sad xxeom xxsender xxmaj gamma xxsom so xxeom xxsender xxmaj kappa xxsom xxmaj why even have rules of we do nt follow them xxeom xxsender xxmaj gamma xxsom better idea for the end of last night &#39;s episode xxeom xxsender xxmaj kappa xxsom xxmaj this country is a dump xxeom xxsender xxmaj gamma xxsom jon tells dany who he | xxsender xxmaj gamma xxsom no one is in chat today and it &#39;s terrible xxeom xxsender xxmaj kappa xxsom xxup sad xxeom xxsender xxmaj gamma xxsom so xxeom xxsender xxmaj kappa xxsom xxmaj why even have rules of we do nt follow them xxeom xxsender xxmaj gamma xxsom better idea for the end of last night &#39;s episode xxeom xxsender xxmaj kappa xxsom xxmaj this country is a dump xxeom xxsender xxmaj gamma xxsom jon tells dany who he really | . 1 time until the election voting against their other xxunk until they get to him xxeom xxsender xxmaj beta xxsom xxmaj it seems that it &#39;s convention for xxmaj senators to xxunk from voting in conflict of interest cases , not law or even xxmaj senate rules . xxmaj so , xxmaj cruz could vote for himself . xxmaj he does n&#39;t have to resign as senator until he &#39;s actually xxunk xxeom xxsender xxmaj kappa xxsom &lt; meme &gt; xxeom | until the election voting against their other xxunk until they get to him xxeom xxsender xxmaj beta xxsom xxmaj it seems that it &#39;s convention for xxmaj senators to xxunk from voting in conflict of interest cases , not law or even xxmaj senate rules . xxmaj so , xxmaj cruz could vote for himself . xxmaj he does n&#39;t have to resign as senator until he &#39;s actually xxunk xxeom xxsender xxmaj kappa xxsom &lt; meme &gt; xxeom xxsender | . Then, I fed that object into the language model learner to create a language model to train. As with other fastai models, this sets up a pretrained model that can be fine tuned to based on the particular context. . learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . I used the learning rate finder to pick an apporpriate learning rate: . learn.lr_find() . SuggestedLRs(lr_min=0.13182567358016967, lr_steep=0.0831763744354248) . Now, I fit the model, using that learning rate and 10 epochs: . epochs = 10 lr = 1e-1 learn.fit_one_cycle(epochs, lr) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.602012 | 3.350408 | 0.388268 | 28.514360 | 00:58 | . 1 | 3.726574 | 3.723708 | 0.366895 | 41.417671 | 00:58 | . 2 | 3.399452 | 3.705886 | 0.370592 | 40.686092 | 00:57 | . 3 | 3.176552 | 3.566556 | 0.382298 | 35.394474 | 00:57 | . 4 | 2.986681 | 3.432734 | 0.391250 | 30.961184 | 00:58 | . 5 | 2.807675 | 3.359524 | 0.406797 | 28.775505 | 00:57 | . 6 | 2.646611 | 3.322854 | 0.415332 | 27.739414 | 00:59 | . 7 | 2.482075 | 3.298300 | 0.421074 | 27.066593 | 00:57 | . 8 | 2.355381 | 3.294302 | 0.422982 | 26.958590 | 00:58 | . 9 | 2.265341 | 3.295616 | 0.423320 | 26.994041 | 00:58 | . Again, 10 epochs is arbitrary, merely based on the fact that epoch took about a minute and 10 minutes isn&#39;t an eternity to wait. . Output Cleanup . Before I generated text, I need a routine to clean the output to make it look reasonable. In particular, I had added those extra xx markers that needed to be removed. Additionally, there were some modifications that the default tokenizer for fastai uses (e.g. It&#39;s to It &#39;s) that needed to be undone. One thing I couldn&#39;t find in the fastai library was something to completely undo that tokenization. . For those reasons, the collapse below has the output_cleanup function that I wrote to make the generated text prettier. It&#39;s not very interesting, but I included it for completeness . Chat Generation . With the model tuned, I have everything I need to generate some chat. The main function for doing so is learn.predict, which generates some words based on some seed text. Here&#39;s the generation function that I came up with: . # Generate some chat def generate(seed, num_words): # If seed is just a sender, add the sender marker if seed in [&quot;Kappa&quot;, &quot;Gamma&quot;, &quot;Psi&quot;, &quot;Omega&quot;, &quot;Beta&quot;, &quot;Sigma&quot;]: seed = &quot;xxsender &quot; + seed # Predict slightly past the number of requested words x = learn.predict(seed, num_words + 20, temperature=0.75) # Clean the output x = output_cleanup(x) # Trim back to the last complete message x = x.splitlines() x = &quot; n&quot;.join(x[:-1]) return x . print(generate(&quot;&quot;, 200)) . Gamma :: it iirc made MIL as ron burgundy Gamma :: well, they all don&#39;t have their own non white children Omega :: They&#39;ve been there since last year Beta :: If they do nt have a non - emergency situation, that&#39;s gold Kappa :: It&#39;s a silent auction Kappa :: It&#39;s a shame that that&#39;s not the origin for this Kappa :: Does that matter? Kappa :: is this it? Kappa :: I&#39;m beginning to doubt her dad as a conservative but that doesn&#39;t mean it is a good one Gamma :: i think it&#39;s not a nut roll Omega :: i mean the maybe i assume that&#39;s the third time i was being a retard Kappa :: it&#39;s like a fighting chance to remember it Gamma :: It&#39;s really good Kappa :: I&#39;m out of my house . It&#39;s pretty easy to see that this generated chat is better than the previous text generation that I did. At a minimum, each message is much more likely to be coherent as English. There is also some continuity between messages, where there was none before. . For fun, I also generated some text seeded based on who the first sender was . for sender in [&quot;Kappa&quot;, &quot;Gamma&quot;, &quot;Psi&quot;, &quot;Omega&quot;, &quot;Beta&quot;, &quot;Sigma&quot;]: y = generate(sender, 100) print(y) print(&quot;--&quot;) . Kappa :: But i can not keep forgetting Gamma :: i guess it&#39;s not illegal if you don&#39;t think we&#39;re gonna go to the gov&#39;t Kappa :: The pediatrician said it was never before Kappa :: There&#39;s a spider in the CO Kappa :: just the fuck Kappa :: Fucking retarded Kappa :: Maybe i&#39;m also retarded Gamma :: It&#39;s a retarded demon Kappa :: i was retarded Kappa :: That s a good word -- . Gamma :: Too accurate Kappa :: This is one of the most advanced things i&#39;ve ever seen Kappa :: how are you going to take the call? Gamma :: Fuck this country Kappa :: Fuck this country Kappa :: My question is how to get on Kappa :: The question from the south is why someone said there&#39;s a higher school Kappa :: Oh and i mean, people in the south are going to wait for the Canadian man who lives there -- . Psi :: JFC Psi :: &lt;MEME&gt; Kappa :: Is this a fun time, don&#39;t have a job? Gamma :: The fuck Kappa :: Lol Kappa :: i hate this world Gamma :: Boris might always have COVID Kappa :: I&#39;m not going to buy it Kappa :: i&#39;m like being real and retarded Kappa :: i assume that&#39;s some good social commentary Kappa :: and then it&#39;s called someone&#39;s -- . Omega :: So i can get a pass to bring myself to a free crab event today Omega :: Yeah i have a fever, so, just because of a fever, i need the best for my life Kappa :: It&#39;s like a normal person Omega :: There&#39;s a lot of thoughts on what you&#39;ve done Omega :: Ok Omega :: Maybe they&#39;re a family Omega :: Or the other way -- . Beta :: Damn it, it&#39;s so easy to do Omega :: Everyone is like a lot of people playing at the prison Omega :: That&#39;s you really about putting no Clown on the table Psi :: Also, he&#39;s the end of America :: i think it&#39;s the narrative&#34; Kappa :: That s a good cartoon for &#34;hot people&#34; he&#39;s &#34;to be&#34; so very wise of point Kappa :: Good spin on this web comic -- . Sigma :: i have no clue what the assignment is Omega :: The ravens are not playing their game Gamma :: They are going to vote for him, and nothing else will be shady Omega :: He needs to be dead Gamma :: And he&#39;s not subtle Kappa :: Is the game reopened? Psi :: Lighting beacons Omega :: Yes it is Omega :: But the beacons are lit Kappa :: Beacon lit -- . Takeaway . My primary takeaway is that this version of text generation is better in multiple ways. First, it generates objectively more reasonable text. It&#39;s not perfect and still identifiable as machine-generated, but it is definitely better than my previous attempt. Second, the entire process was much easier and there&#39;s a lot less code to write. With my first attempt with PyTorch, training a model took a lot of work to adapt their example to my data. For this attempt, it&#39;s less than 20 lines of actual code (ignoring the output cleanup). .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2021/02/02/chat_bugaloo.html",
            "relUrl": "/jupyter/2021/02/02/chat_bugaloo.html",
            "date": " • Feb 2, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Kaggling into an Iceberg",
            "content": "Introduction . The next lesson in the fastai course covered tabular data. According to the course, the vast majority of datasets on tabular data are best modeled by ensembles of decision trees, such as random forests, so the lesson focused on them. . They used a particular Kaggle competition, the Blue Book for Bulldozers as their hands-on example for walking through how to understand, build, train, and infer using decision trees and random forests. . While I had heard of Kaggle previously, I had never used it myself before. Fortunately, Kaggle itself points you to the Titanic competition as a starting point. The premise is that you&#39;re given a dataset of passengers on the Titanic and whether or not they survived to train on and you have to predict which passengers in another dataset of passengers survived. . As my introduction to both Kaggle and random forests, I thought I would try to walk through the course&#39;s random forest discussion using the Titanic competition data and see what happens. . (In the collapse below is all the imports and pandas settings I&#39;ll end up using at the top just to keep them all together.) . from fastai.tabular.all import * from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import zero_one_loss # Pandas display options pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . . Setup and Random Guess . The competition provides three files: . training set (passenger data with &#39;Survived&#39; column) | test set (passenger data with no &#39;Survived&#39; column) | sample submission assuming all and only female passengers survived | . Since I&#39;ll use it often, I&#39;ll set up the dependent variable (&#39;Survived&#39;) here as well. . dep_var = &#39;Survived&#39; train_df = pd.read_csv(&quot;titanic_train.csv&quot;, low_memory=False) test_df = pd.read_csv(&quot;titanic_test.csv&quot;, low_memory=False) . Here&#39;s what the training data looks like: . train_df . PassengerId Survived Pclass Name ... Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | ... | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | ... | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | ... | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | ... | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | ... | 373450 | 8.0500 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | ... | 211536 | 13.0000 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | ... | 112053 | 30.0000 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | ... | W./C. 6607 | 23.4500 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | ... | 111369 | 30.0000 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | ... | 370376 | 7.7500 | NaN | Q | . 891 rows × 12 columns . The competition page has a description of each of the columns, so I won&#39;t rehash those here. . As a first pass to give something to compare against, I built a randomly chosen set where the survival was determined at random based on the survival rate of the training data. The score for this random set ended up being 52.39%. . # Submit random set total_died, total_survived = train_df.value_counts(dep_var) ratio = total_died / (total_survived + total_died) test_length = len(test_df) random_survived = np.random.choice(2, test_length, p=[ratio, 1 - ratio]) random_data_frame = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : random_survived}) random_data_frame.to_csv(&quot;titanic_random.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_random.csv -m &quot;Random based on survival rate&quot; . Score: 0.52392 . Decision Trees . Now, it&#39;s time to do something other than just guess. I started by splitting the categorical and continuous columns using a fastai designed specifically for that. Then, I split the provided training data into training and validation sets using TabularPandas. . # Split the columns into categorical and continuous cont_cols, cat_cols = cont_cat_split(train_df, 1, dep_var=dep_var) print(f&quot;Continuous columns: {cont_cols}&quot;) print(f&quot;Categorical columns: {cat_cols}&quot;) . Continuous columns: [&#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;] Categorical columns: [&#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;] . # Operations for columns column_ops = [Categorify, FillMissing] # Split the training data randomly splits = RandomSplitter()(range_of(train_df)) tab_panda = TabularPandas(train_df, column_ops ,cat_cols, cont_cols, y_names=dep_var, splits=splits) trainxs, trainy = tab_panda.train.xs, tab_panda.train.y validxs, validy = tab_panda.valid.xs, tab_panda.valid.y len(trainxs), len(validxs) . (713, 178) . tab_panda.show(3) . Name Sex Ticket Cabin Embarked Age_na PassengerId Pclass Age SibSp Parch Fare Survived . 420 Gheorgheff, Mr. Stanio | male | 349254 | #na# | C | True | 421 | 3 | 28.0 | 0 | 0 | 7.8958 | 0 | . 392 Gustafsson, Mr. Johan Birger | male | 3101277 | #na# | S | False | 393 | 3 | 28.0 | 2 | 0 | 7.9250 | 0 | . 238 Pengelly, Mr. Frederick William | male | 28665 | #na# | S | False | 239 | 2 | 19.0 | 0 | 0 | 10.5000 | 0 | . The lesson used a DecisionTreeRegressor, but since &#39;Survived&#39; is a categorical and not continuous variable, I used DecisionTreeClassifier instead. For the same reason, zero_one_loss makes more sense to be the loss function rather than using the mean square error. . # Simple decision tree classifier model = DecisionTreeClassifier() model.fit(trainxs, trainy) print(f&quot;Error: {zero_one_loss(model.predict(validxs), validy)}&quot;) . Error: 0.2415730337078652 . Essentially, this is saying the model got ~24% of the validation passenger set wrong when predicting their survival. . In the default, the model can split a node as long as there is at least 1 sample for each leaf. Following the lesson, I checked to see how many leaves for my model were created vs. how many rows in the training data . model.get_n_leaves(), len(trainxs) . (117, 713) . The Titanic dataset is much, much smaller than the Blue Book of Bulldozers dataset, but this number of leaves seemed reasonable to me (6 per leaf or so). However, because it was easy to do so, I decided to iterate the min_samples_leaf parameter to try to get a better model: . # min_samples_leaf optimization best_error, best_min_samples, best_model = (None, None, None) for min_samples in range(1, 21): model = DecisionTreeClassifier(min_samples_leaf=min_samples) model.fit(trainxs, trainy) error = zero_one_loss(model.predict(validxs), validy) if best_error is None or best_error &gt; error: best_error, best_min_samples, best_model = (error, min_samples, model) print(f&quot;Best model error: {best_error}&quot;) print(f&quot;Min samples per leaf: {best_min_samples}, Number leaves: {best_model.get_n_leaves()}&quot;) . Best model error: 0.1966292134831461 Min samples per leaf: 3, Number leaves: 70 . The default minimum samples per leaf is 1 for DecisionTreeClassifier, but it seems the optimization helped. A minimum of 3 samples per leaf yields a better result on the validation, so I&#39;ll use that model to submit to kaggle. . Data Cleanup . Before I could predict using that model, there is an issue with the test data that had to be addressed. The &#39;Fare&#39; column had a null value for one of the test passengers, while neither the test or valid set had any null values in that column. . A standard way to fill in that value is to use the mean of the training set, so that&#39;s what I did. Pandas makes this substitution pretty easy, once you figure out the proper syntax. . # Find the null row(s) in the &#39;Fare&#39; column nan_row = test_df[test_df[&#39;Fare&#39;].isnull()] # Set the fare in that row to the mean of the training data test_df.at[nan_row.index, &#39;Fare&#39;] = train_df[&#39;Fare&#39;].mean() # Display the newly fixed row display(test_df.loc[nan_row.index]) . PassengerId Pclass Name Sex ... Ticket Fare Cabin Embarked . 152 1044 | 3 | Storey, Mr. Thomas | male | ... | 3701 | 32.204208 | NaN | S | . 1 rows × 11 columns . With the row fixed, I converted the test data to a TabularPandas. We&#39;ll use this again and again to feed into our models for survival prediction. . # Convert test data to TabularPandas test_tab = TabularPandas(test_df, [Categorify, FillMissing], cat_cols, cont_cols) . Decision Tree Results . With the test data cleaned up, I used the best decision tree model that I found to predict the survivors on the test set and submit the results to kaggle: . # Decision Tree Classifier set (Score: 0.70813) # Predict the survivors dt_survived = best_model.predict(test_tab.xs) test_length = len(test_df) dt_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : dt_survived}) dt_df.to_csv(&quot;titanic_dt.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_dt.csv -m &quot;Single decision tree&quot; . Score: 0.70813 . Random Forest . If one decision tree is good, how about more? Next, I&#39;ll walk through the random forests of decision trees that create to tackle this competition. . Following the course approach, I made a function to make creating a random forest and fit it in single step. Similar to above, since I&#39;m looking for a yes or no answer, instead of a continuous one, I&#39;ll use RandomForestClassifier instead of a regression model. . # create random forest and fit it def create_rf(xs, y, n_estimators=40, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . With that function, I can create a random forest, fit it to the training data, and score it against the validation data: . # First pass model with error # Model error: 0.1797752808988764 # Model oob error: 0.18513323983169705 model = create_rf(trainxs, trainy) error = zero_one_loss(model.predict(validxs), validy) print(f&quot;Model error: {error}&quot;) print(f&quot;Model oob error: {1.0 - model.oob_score_}&quot;) . Model error: 0.1797752808988764 Model oob error: 0.18513323983169705 . If 40 trees (n_estimators) is good, how about more? . Let&#39;s look at models with more trees (n_estimators) and see how they do. . # Number of trees (n_estimators) optimization best_error, best_trees, best_model = (None, None, None) for num_trees in range(40, 300, 20): model = create_rf(trainxs, trainy, n_estimators=num_trees) error = zero_one_loss(model.predict(validxs), validy) if best_error is None or best_error &gt; error: best_error, best_trees, best_model = (error, num_trees, model) print(f&quot;Num trees: {num_trees}, error: {error}, oob: {1.0 - model.oob_score_}&quot;) print(f&quot;Best model error: {best_error}, num trees: {best_trees}&quot;) . Num trees: 40, error: 0.1797752808988764, oob: 0.18513323983169705 Num trees: 60, error: 0.1685393258426966, oob: 0.18793828892005615 Num trees: 80, error: 0.1741573033707865, oob: 0.1893408134642356 Num trees: 100, error: 0.1797752808988764, oob: 0.1837307152875175 Num trees: 120, error: 0.1629213483146067, oob: 0.18793828892005615 Num trees: 140, error: 0.1685393258426966, oob: 0.1837307152875175 Num trees: 160, error: 0.1685393258426966, oob: 0.1865357643758766 Num trees: 180, error: 0.1685393258426966, oob: 0.1865357643758766 Num trees: 200, error: 0.1629213483146067, oob: 0.1893408134642356 Num trees: 220, error: 0.1629213483146067, oob: 0.18513323983169705 Num trees: 240, error: 0.1685393258426966, oob: 0.1837307152875175 Num trees: 260, error: 0.1685393258426966, oob: 0.18232819074333806 Num trees: 280, error: 0.1741573033707865, oob: 0.1781206171107994 Best model error: 0.1629213483146067, num trees: 120 . The model with 120 (or 200 or 220) trees seems to fare best, but all that much better than the model with 40 trees. I&#39;m no expert, I think it is due to the out-of-bag error. For all of the models, the oob error is more than the validation error. That would seem to indicate that the model can&#39;t really improve with more adding more trees, but could just be better by random chance. . That said, since they are easy to create and the submission rules for the Titanic competition are generous, I submitted the predictions from the 40-tree, 120-tree, 140-tree, 160-tree, and 220-tree models just to see how they would do. . # A few models with different numbers of trees for num_trees in [40, 120, 140, 160, 220]: model = create_rf(trainxs, trainy, n_estimators=num_trees) rf_survived = model.predict(test_tab.xs) rf_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : rf_survived}) rf_df.to_csv(f&quot;titanic_rf_{num_trees}.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_rf_40.csv -m &quot;Random forest classifier with 40 trees&quot; . 40-tree model score: 0.75598 . !kaggle competitions submit -c titanic -f titanic_rf_120.csv -m &quot;Random forest classifier with 120 trees&quot; . 120-tree model score: 0.76794 . !kaggle competitions submit -c titanic -f titanic_rf_140.csv -m &quot;Random forest classifier with 140 trees&quot; . 140-tree model score: 0.76076 . !kaggle competitions submit -c titanic -f titanic_rf_160.csv -m &quot;Random forest classifier with 160 trees&quot; . 160-tree model score: 0.76555 . !kaggle competitions submit -c titanic -f titanic_rf_220.csv -m &quot;Random forest classifier with 220 trees&quot; . 220-tree model score: 0.77272 . Although the 220-tree model ended up doing the best, increasing the number of trees didn&#39;t significantly improve the results. (For reference, the percentage difference between the 40-tree and 220-tree model corresponds to 8 people.) I&#39;m not convinced it&#39;s anything other than random chance. . That said, how about even more trees? 3000-tree model inbound: . model3000 = create_rf(trainxs, trainy, n_estimators=3000) rf_survived = model3000.predict(test_tab.xs) rf_rf = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : rf_survived}) rf_rf.to_csv(f&quot;titanic_rf_3000.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_rf_300.csv -m &quot;Random forest classifier with 3000 trees&quot; . 3000-tree model score: 0.76555 . And, it&#39;s worse than 220-tree model. I think this supports my earlier assertion that some of the models with more trees are better than the 40 tree model by random chance, rather than anything learned within the models. . Aside: Simpler models . At this point, before getting into some feature engineering, I went back and read on Kaggle a little bit more about the Titanic competition. As I mentioned above, one of the provided files was a sample submission file where the survivors were assumed to be only and all the female passengers (as determined by the &#39;Sex&#39; column). How does that submission score? . !kaggle competitions submit -c titanic -f gender_submssion.csv -m &quot;Only and all female passengers survive&quot; . Female only model score: 0.76555 . This simple model achieved the same score as the 3000-tree model! Let&#39;s take a look at the importance of each feature, according to that 3000-tree model. (Again, this is adapted directly from the course.) . def feature_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_}).sort_values(&#39;imp&#39;, ascending=True) fi = feature_importance(model3000, trainxs) fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . From the bar chart, it is abundantly clear that &#39;Sex&#39; is the most telling feature, with everything else trailing. That said, the 3000-tree model and female-only model actual differ in 62 (of 418) predictions, so they are finding slightly different spaces with the same score. . Because I haven&#39;t been able to get this song (no, not that one) out of my head, I also tried &quot;the women and children fled to the lifeboats put to sea&quot; model. That is, all the women and children survived (but no one else). . # Women and children survival model wac_test_df = pd.DataFrame(test_df) wac_test_df[&#39;Survived&#39;] = np.ones(len(wac_test_df), dtype=int) wac_cond = (wac_test_df.Sex == &#39;female&#39;) | (wac_test_df.Age &lt; 18) wac_test_df[&#39;Survived&#39;].where(wac_cond, 0, inplace=True) wac_submit = wac_test_df.loc[:,[&#39;PassengerId&#39;,&#39;Survived&#39;]] wac_submit.to_csv(&quot;titanic_wac.csv&quot;, index=False) . #!kaggle competitions submit -c titanic -f titanic_wac.csv -m &quot;Women and children fled to the lifeboats put to sea&quot; . Women and children model score: 0.74641 . This model scores worse, but not dramatically so. . Feature Engineering . The next step was to improve the random forest models above by extracting, modifying, or deleting some of the given columns to create new, hopefully more relevant ones by feature engineering. . I tried lots of different modifications that I came up with on my own, but, in actuality, I couldn&#39;t come up with anything that definitely improved on the previous random forests. (The best score I ended up getting was 0.77751.) . Since I couldn&#39;t seem to improve, I ended up reading a number of the discussion posts on Kaggle, particularly those that dealt with random forests and feature engineering. My favorite was this one, so I decided to adapt it to the syntax I was using. The function I came up with is in the collapse below: . def edit_features(df): mod_df = df.copy() # PassengerId - not meaningful for learning del mod_df[&quot;PassengerId&quot;] # Pclass - treat passenger class as category mod_df[&quot;Pclass&quot;] = mod_df[&quot;Pclass&quot;].astype(&quot;category&quot;) # Name - Split into name length and title mod_df[&#39;NameLength&#39;] = mod_df[&quot;Name&quot;].apply(lambda x: len(x)) mod_df[&#39;NameTitle&#39;] = mod_df[&quot;Name&quot;].apply(lambda x: x.split(&#39;,&#39;)[1]).apply(lambda x: x.split()[0]) mod_df[&#39;NameTitle&#39;] = mod_df[&quot;NameTitle&quot;].astype(&quot;category&quot;) del mod_df[&quot;Name&quot;] # Age - fill Age with mean grouped by title and class age_data = mod_df.groupby([&#39;NameTitle&#39;, &#39;Pclass&#39;])[&#39;Age&#39;] mod_df[&#39;Age_na&#39;] = mod_df[&#39;Age&#39;].apply(lambda x: 1 if pd.isnull(x) else 0) mod_df[&#39;Age_na&#39;] = mod_df[&#39;Age_na&#39;].astype(&quot;category&quot;) mod_df[&#39;Age&#39;] = age_data.transform(lambda x: x.fillna(x.mean())) # SibSp + Parch =&gt; Family size category passengers = mod_df[&quot;SibSp&quot;] + mod_df[&quot;Parch&quot;] mod_df[&#39;FamilySize&#39;] = np.where(passengers == 0, &#39;Solo&#39;, np.where(passengers &lt;= 3, &#39;Nuclear&#39;, &#39;Big&#39;)) mod_df[&quot;FamilySize&quot;] = mod_df[&quot;FamilySize&quot;].astype(&quot;category&quot;) del mod_df[&#39;SibSp&#39;] del mod_df[&#39;Parch&#39;] # Ticket - Split into two categories, # one based on the first letter of the ticket and # one based on the length mod_df[&quot;TicketLetter&quot;] = mod_df[&quot;Ticket&quot;].apply(lambda x: str(x)[0]) mod_df[&quot;TicketLetter&quot;] = mod_df[&quot;TicketLetter&quot;].apply(lambda x: str(x)) highTicket = mod_df[&#39;TicketLetter&#39;].isin([&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;S&#39;, &#39;P&#39;, &#39;C&#39;, &#39;A&#39;]) lowTicket = mod_df[&#39;TicketLetter&#39;].isin([&#39;W&#39;, &#39;4&#39;, &#39;7&#39;, &#39;6&#39;, &#39;L&#39;, &#39;5&#39;, &#39;8&#39;]) mod_df[&#39;TicketLetter&#39;] = np.where(highTicket, mod_df[&quot;TicketLetter&quot;], np.where(lowTicket, &quot;LowTicket&quot;, &quot;OtherTicket&quot;)) mod_df[&#39;TicketLength&#39;] = mod_df[&#39;Ticket&#39;].apply(lambda x: len(x)) del mod_df[&#39;Ticket&#39;] # Cabin - Split into the prefix and bin the number mod_df[&quot;CabinPrefix&quot;] = mod_df[&#39;Cabin&#39;].apply(lambda x: str(x)[0]) mod_df[&quot;CabinPrefix&quot;] = mod_df[&quot;CabinPrefix&quot;].astype(&quot;category&quot;) del mod_df[&quot;Cabin&quot;] # Embarked - Fill missing data with most common value (&#39;S&#39;) mod_df[&#39;Embarked&#39;] = mod_df[&#39;Embarked&#39;].fillna(&#39;S&#39;) return mod_df . . I couldn&#39;t get it to quite match the claimed success of the notebook I borrowed it from (82%). I don&#39;t think it&#39;s my code though, since when I ran their exact code, I also got 77-78%. . # New data frame with modified features fe_train_df = edit_features(train_df) fe_train_df.columns . Index([&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;, &#39;NameLength&#39;, &#39;NameTitle&#39;, &#39;Age_na&#39;, &#39;FamilySize&#39;, &#39;TicketLetter&#39;, &#39;TicketLength&#39;, &#39;CabinPrefix&#39;], dtype=&#39;object&#39;) . With the modified features, I ran the training data through the same mechanism as before for creating a random forest . # Identify categorical and continuous columns and prep training data fe_cont, fe_cat = cont_cat_split(fe_train_df, 1, dep_var=dep_var) fe_splits = RandomSplitter()(range_of(fe_train_df)) fe_tab_panda = TabularPandas(fe_train_df, column_ops, fe_cat, fe_cont, y_names=dep_var, splits=fe_splits) # Alias training and validation data fetrainxs, fetrainy = fe_tab_panda.train.xs, fe_tab_panda.train.y fevalidxs, fevalidy = fe_tab_panda.valid.xs, fe_tab_panda.valid.y print(f&quot;Categorical columns: {fe_cat}&quot;) print(f&quot;Continuous columns: {fe_cont}&quot;) fe_tab_panda.show(3) . Categorical columns: [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Embarked&#39;, &#39;NameTitle&#39;, &#39;Age_na&#39;, &#39;FamilySize&#39;, &#39;TicketLetter&#39;, &#39;CabinPrefix&#39;] Continuous columns: [&#39;Age&#39;, &#39;Fare&#39;, &#39;NameLength&#39;, &#39;TicketLength&#39;] . Pclass Sex Embarked NameTitle Age_na FamilySize TicketLetter CabinPrefix Age Fare NameLength TicketLength Survived . 769 3 | male | S | Mr. | 0 | Solo | LowTicket | n | 32.0 | 8.362500 | 32 | 4 | 0 | . 49 3 | female | S | Mrs. | 0 | Nuclear | 3 | n | 18.0 | 17.799999 | 45 | 6 | 0 | . 704 3 | male | S | Mr. | 0 | Nuclear | 3 | n | 26.0 | 7.854200 | 23 | 6 | 0 | . # Create a random forest model classifier fe_model = RandomForestClassifier(n_estimators=300, min_samples_split=5, oob_score=True) fe_model.fit(fetrainxs, fetrainy) fe_test_df = edit_features(test_df) fe_test_tab = TabularPandas(fe_test_df, column_ops, fe_cat, fe_cont) error = zero_one_loss(fe_model.predict(fevalidxs), fevalidy) print(f&quot;Model error: {error}&quot;) print(f&quot;Model oob error: {1.0 - fe_model.oob_score_}&quot;) . Model error: 0.1685393258426966 Model oob error: 0.16830294530154277 . # Make and submit predictions fe_survived = fe_model.predict(fe_test_tab.xs) fe_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : fe_survived}) fe_df.to_csv(&quot;titanic_fe.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_fe.csv -m &quot;Random forest classifier with feature engineering&quot; . Random forest with feature engineering model score: 0.78708 . This model did score a little bit better than the random forest model1. I&#39;m not entirely convinced it&#39;s actually better than the non-engineered models though. However, plotting the feature importance, it&#39;s pretty easy to see that it is likely the added &#39;NameTitle&#39; and &#39;NameLength&#39; features are providing something beyond what &#39;Name&#39; did. . fi = feature_importance(fe_model, fetrainxs) fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . In any case, even though this feature engineering did not produce a model that blew the other models away, I really liked the justification and the write-up of the author for each change that he made. I think it&#39;s good insight on how I might use feature engineering on a less studied dataset. . Using a Neural Network . Changing gears, let&#39;s see if a neural network would garner better results. . First, I needed to set up the data. I dropped the PassengerId column, since it&#39;s just an index, and set up the TabularPandas similar to before, the only major change is the addition of the Normalize operation. . # Split the columns by cardinality of 500 nn_cont_cols, nn_cat_cols = cont_cat_split(train_df, dep_var=dep_var) # Drop the counter variable nn_cont_cols.remove(&#39;PassengerId&#39;) # Added the Normalize operation nn_procs = [Categorify, FillMissing, Normalize] # Make the TabularPandas object nn_train_tab = TabularPandas(train_df, nn_procs, nn_cat_cols, nn_cont_cols, splits=splits, y_names=dep_var, y_block=CategoryBlock) . Next, I set up the DataLoader and the learner itself and made a stab at finding an appropriate learning rate. . nn_dls = nn_train_tab.dataloaders(512) learn = tabular_learner(nn_dls, loss_func=F.l1_loss) . After experimenting with a number of different epochs, I found that 300 epochs with a learning rate of 0.04 seemed to give reasonable results on the validation set and not take forever (~5 seconds). . lr = 4e-2 epochs = 300 with learn.no_logging(): # Prevents display of the 300 rows of epochs learn.fit_one_cycle(epochs, lr) train_error, valid_error = learn.recorder.values[-1] print(f&quot;Training error: {train_error}&quot;) print(f&quot;Validation error: {valid_error}&quot;) . Training error: 0.014065220020711422 Validation error: 0.1932232528924942 . From there, it was fairly easy to make predictions, once I found the right incantation to get the test data in the right format (learn.dls.test_dl), call the right prediction function (get_preds), and flatten the predictions to a single vector: . # Make and submit predictions _, nn_predictions = learn.get_preds(dl=learn.dls.test_dl(test_df)) nn_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : nn_predictions.flatten()}) nn_df.to_csv(&quot;titanic_nn.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_nn.csv -m &quot;Neural network predictions&quot; . Neural network score: 0.74641 . As you can see, the neural network didn&#39;t score that much worse that the random forest models. . Ensemble . Now, I&#39;ve got a handful of models that take slightly different approaches (or at least use different numbers of trees). I can combine them all into a single model. There may be other ways to approach this, but the simplest thing that came to my mind was to use a majority vote to decide who survived. Below is the ensemble that scored best on the competition test set of the ones that I tried. . # Gather the models model_files = [&quot;titanic_rf_220.csv&quot;, # 220-tree Random Forest &quot;titanic_rffe.csv&quot;, # RF with feature engineering &quot;titanic_nn.csv&quot;, # Neural network model &quot;gender_submission.csv&quot; # Women only survival ] # Combined the models into a table data = [] for i, mfile in enumerate(model_files): model_df = pd.read_csv(mfile) data.append(model_df[&#39;Survived&#39;]) df = pd.DataFrame(data) # Majority vote ensemble_survived = pd.to_numeric(df.mode().iloc[0], downcast=&#39;integer&#39;) # Save to csv and submit ensemble_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : ensemble_survived}) ensemble_df.to_csv(&quot;titanic_ensemble.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_ensemble.csv -m &quot;Ensemble&quot; . Ensemble score: 0.78468 . In this case, the ensembles didn&#39;t score any better than the best individual models. It appears I have reached the limits of what I able to do with the approaches directly mentioned in the fastai lesson. I&#39;m going to end my adventure with the Titanic here, since I worked the entire way through the lesson, but I have to say that was fun. . If I were to continue, I would be taking a much closer look at this notebook, which used a very similar fastai approach and achieved a score above 98%. They supplemented their data with this extended set, however. .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2021/01/31/titanic-kaggle.html",
            "relUrl": "/jupyter/2021/01/31/titanic-kaggle.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Know Your Meme",
            "content": "Introduction . In previous blog entries, I have focused on natural language processing (i.e. text) for the dives into the fastai course. Today, instead, I finally bit the bullet and shifted to working on a computer vision (i.e. images) project. In particular, I&#39;m going to train a model that handles classificaton of images with mutiple categories. . The data today comes from the same Google Hangouts extraction that I have used before (from Google Takeout. However, I&#39;m switching my focus from the text in the chat to all of the images that appear in the conversation. I had previously lazily marked these all as &quot;memes&quot; (specifically as &quot; &lt;MEME&gt;&quot;) in terms of how they would appear as text, but a closer look revealed that is an over-simplification of the images. . The images contain a wide variety of types and I&#39;m set on using them to train this multiple category model. Among the categories I&#39;m considering are the sender of the image and whether it is a meme or a twitter post or a news article. But I&#39;ll cover that in a few minutes. . Data Preparation . As you may recall, the chat converstation log comes from a Google Takeout download, where the primary source of the log is a json file. The json file contains a dictionary of events, any of which could correspond to a chat message, a link, an image, or something like a receipt notification. . Again, we start by loading the json data into Python and narrow the data to the events corresponding to actual chat content: . import json # Load the data from the chatfile json_filename = &quot;./Hangouts_20201001.json&quot; json_file = open(json_filename, encoding=&#39;utf8&#39;) data = json.load(json_file) json_file.close() # Set of keys to descend json tree to chat content eventKeys = (&quot;conversations&quot;, 5, &quot;events&quot;) # Chat content events events = data for k in eventKeys: events = events[k] . As before, each sender ID maps to a person, whose pseudonym is below: . sender_lookup = {&quot;108076694707330228373&quot;: &quot;Kappa&quot;, &quot;107112383293353696822&quot;: &quot;Beta&quot;, &quot;111672618051461612345&quot;: &quot;Omega&quot;, &quot;112812509779111539276&quot;: &quot;Psi&quot;, &quot;114685444329830376810&quot;: &quot;Gamma&quot;, &quot;112861108657200483380&quot;: &quot;Sigma&quot;} . There are at least two different ways that images can show up in the chat event log. One is as an embedded attachment and another is as a link. (If there are others, they are ignored for the purposes of this experiment.) In both cases, the log provides a url link to the image that can be downloaded. . First step is to loop through the events to download each image locally and construct a way to reference each image. I used the download_url routine in fastai for the former and a csv file for the latter with appropriate info. The description of the csv headers is in the code below that sets up the csv file for writing. . The course lesson that talked about Pandas, which I never used or heard of before. I&#39;m going to use it through to maniuplate the csv-like tables. With that in mind, the initial construction may seem a bit odd. . from fastai.vision.all import * # Pandas is in the fastai package # Headers for the csv file columnHeaders = [&quot;sender&quot;, # Image sender &quot;filename&quot;, # Image filename &quot;type&quot;, # Image type as labeled by Google &quot;timestamp&quot;, # Sent timestamp ] . Then, here&#39;s the code for looping through the json events, downloading each image, and adding a line to the dataframe for each image . # Do a count of how many images for each sender senderCounts = dict() # Index for filename imageIndex = 0 # Path for images imagesPath = &quot;/storage/chat_memes&quot; # These indexes turned out not to be images (e.g. not all links are images) badImages = [119, 196, 1000, 1003, 1052, 1455, 1552, 1699] rows = [] for event in events: # Get the ontent of each message (if it exists) msgContent = event.get(&quot;chat_message&quot;,{}).get(&quot;message_content&quot;,{}) # Timestamp of message timestamp = int(event[&quot;timestamp&quot;]) # Message sender sender = event[&quot;sender_id&quot;][&quot;gaia_id&quot;] sender = sender_lookup[sender] # Images as attachments attachments = msgContent.get(&quot;attachment&quot;,[]) for attachment in attachments: if imageIndex not in badImages: item = attachment[&quot;embed_item&quot;] itemType = item[&quot;plus_photo&quot;][&quot;media_type&quot;] # Usually PHOTO itemUrl = item[&quot;plus_photo&quot;][&quot;url&quot;] filename = f&quot;{imagesPath}/image_{imageIndex:04}&quot; download_url(itemUrl, filename, timeout=10, retries=20, show_progress=False) rows.append({&quot;timestamp&quot;: timestamp, &quot;sender&quot;: sender, &quot;filename&quot;: filename, &quot;type&quot;: itemType}) senderCounts[sender] = senderCounts.get(sender, 0) + 1 imageIndex += 1 # Images as links segments = msgContent.get(&quot;segment&quot;,[]) for segment in segments: if segment[&quot;type&quot;] == &quot;LINK&quot;: url = segment[&quot;text&quot;] base = url.split(&#39;/&#39;)[-1] # Check if it&#39;s possibly an image if &#39;jpg&#39; in base or &#39;gif&#39; in base or &#39;png&#39; in base: if imageIndex not in badImages: filename = f&quot;{imagesPath}/image_{imageIndex:04}&quot; download_url(url, filename, timeout=10, retries=20, show_progress=False) rows.append({&quot;timestamp&quot;: timestamp, &quot;sender&quot;: sender, &quot;filename&quot;: filename, &quot;type&quot;: &quot;LINK&quot;}) senderCounts[sender] = senderCounts.get(sender, 0) + 1 imageIndex += 1 . # Use rows to create a Pandas dataframe images_df = pd.DataFrame(rows, columns=columnHeaders) # Save the rows to a csv file images_df.to_csv(&quot;chat_images.csv&quot;) # Display the first five rows images_df.head() . sender filename type timestamp . 0 Kappa | /storage/chat_memes/image_0000 | PHOTO | 1600741647230976 | . 1 Gamma | /storage/chat_memes/image_0001 | PHOTO | 1600772605984217 | . 2 Psi | /storage/chat_memes/image_0002 | PHOTO | 1600786824022176 | . 3 Gamma | /storage/chat_memes/image_0003 | PHOTO | 1600861187519443 | . 4 Kappa | /storage/chat_memes/image_0004 | PHOTO | 1600863157273002 | . I ended up with about 1700 images from the chat log. Here&#39;s a quick breakdown of how many each sender was responsible for: . totalImages = 0 for sender in senderCounts: totalImages += senderCounts[sender] print(f&quot;Total images: {totalImages}&quot;) for sender in senderCounts: images = senderCounts[sender] percent = 100 * images / totalImages print(f&quot;{sender} sent {images} images -&gt; {percent:.03}%&quot;) . Total images: 1718 Kappa sent 707 images -&gt; 41.2% Gamma sent 294 images -&gt; 17.1% Psi sent 299 images -&gt; 17.4% Beta sent 95 images -&gt; 5.53% Omega sent 322 images -&gt; 18.7% Sigma sent 1 images -&gt; 0.0582% . And here&#39;s a sample image . im = Image.open(f&quot;{imagesPath}/image_0000&quot;) im.to_thumb(240) . As might be clear by this point, one clear drawback to using this data to for &quot;multi-category&quot; classification is that the data is really labeled with more than one category (sender is really the only usable one). . The approach that I ended up using to label the day was to just label the data myself 1. I didn&#39;t want to go through each of the 1700 images one by one and apply labels, but I found a tool called pigeon that was incredibly useful to enable this labeling to take until the heat death of the universe to complete. It still took multiple hours :weary:, but I learned a new tool :sunglasses:. . Before that, though, I needed some labels to apply to the images. After some trials and errors, these are the set I ended up with (along with a brief description of each one.) . labels = [&#39;meme&#39;, # Is it a meme &#39;social_media&#39;, # Is it a screenshot from social media (usually twitter, sometimes reddit or fb) &#39;comic_strip&#39;, # Is it a multi-panel comic (usually xkcd or SMBC) &#39;game_of_thrones&#39;, # Still mad about the final season &#39;simpsons&#39;, # Enough Simpsons memes to be on their own &#39;rick_and_morty&#39;, # Enough Rick &amp; Morty memes to be on their own &#39;tv_cartoon&#39;, # But not enough South Park, Futurama, or Family Guy to train alone (every simpsons and rick_and_morty is marked this too) &#39;star_trek_wars&#39;, # Combined star trek and wars to get enough &#39;marvel&#39;, # Avengers and others memes &#39;lotr&#39;, # One does not simply something, something &#39;news&#39;, # Is this a news item (rather than a humorous one) &#39;ali_express&#39;, # One member of the chat posts weird things from Ali Express &#39;photo&#39;, # Is this a real-life photo (as opposed to an internet one) &#39;trump&#39;, # Is this trump-related &#39;beacon&#39;, # Is this a beacon for playing video games &#39;weird&#39; # random label I put on stuff I thought was unusual, even for us ] . With these labels in hand, we can then use pigeon to annotate each image with a label . from pigeon import annotate from IPython.display import display import glob imagesPath = &quot;/storage/chat_memes&quot; images = glob.glob(imagesPath+&quot;/image*&quot;) annotations = annotate(images, options = labels, display_fn=lambda filename: display(Image.open(filename).to_thumb(240))) . Alternatively, by setting options to None, you can use free-form text for the labels. I ended up using the labels list for the first pass to give each image a label and then the free-form version for adding additional labels. In the end, annotations is a Python dictionary mapping a filename to a list of string labels. I saved these to their own pandas dataframe and csv, so as not to lose any of the annotations I made. . rows = [] for filename, labels in annotations.items(): labels = &quot; &quot;.join(labels) rows.append({&quot;filename&quot;: filename, &quot;labels&quot;: labels}) annotations_df = pd.DataFrame(rows) annotations_df.to_csv(&quot;annotations.csv&quot;) annotations_df.head() . filename labels . 0 /storage/chat_memes/image_0029 | meme simpsons tv_cartoon | . 1 /storage/chat_memes/image_0071 | meme simpsons tv_cartoon trump | . 2 /storage/chat_memes/image_0096 | meme tv_cartoon | . 3 /storage/chat_memes/image_0105 | meme tv_cartoon | . 4 /storage/chat_memes/image_0231 | meme tv_cartoon | . And merge it with the images dataframe I had made earlier: . combined_df = images_df.merge(annotations_df, on=&#39;filename&#39;) combined_df.to_csv(&quot;chat_memes_with_labels.csv&quot;) combined_df.head() . sender filename type timestamp labels . 0 Kappa | /storage/chat_memes/image_0000 | PHOTO | 1600741647230976 | social_media | . 1 Gamma | /storage/chat_memes/image_0001 | PHOTO | 1600772605984217 | social_media | . 2 Psi | /storage/chat_memes/image_0002 | PHOTO | 1600786824022176 | social_media | . 3 Gamma | /storage/chat_memes/image_0003 | PHOTO | 1600861187519443 | social_media | . 4 Kappa | /storage/chat_memes/image_0004 | PHOTO | 1600863157273002 | comic_strip | . Finally, this gives us some data that I can work with to get onto the multi-category classification. Whew! . Multi-category Classification . Following the example in the lesson, I can now create a DataBlock object to feed into a DataLoaders object to a neural network learner. First, I need some functions to specify my independent and dependent variables. The independent ones are the images/memes and the dependent ones are the sender and the labels that I just annotated above 2. . def get_x(row): return row[&#39;filename&#39;] def get_y(row): if row[&#39;sender&#39;] == &#39;Sigma&#39;: return row[&#39;labels&#39;].split() else: return row[&#39;labels&#39;].split() + [row[&#39;sender&#39;]] . These facilitate building the DataBlock that can be put into a DataLoaders: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(combined_df) . As a reward for reading this far, here&#39;s a batch of memes with their labels that the Dataloaders generates. Note that the RandomResizedCrop transform is why the display below is a cropped portion of the image. As I understand it, it&#39;s designed to normalize the input data to prevent effects of image size and dimension from having an influence on training the model. . dls.show_batch(nrows=2, ncols=4) . Create a learner module from the loaded data: . learn = cnn_learner(dls, resnet18) . The default metric for multi-category labelled data is accuracy_multi which uses a sigmoid function with a particular threshold to decide whether or not an image gets a particular label. . I had no idea what the learning rate or threshold should be for this data, so I tried to used their approaches to use the data to determine what the learning rate and threshold should be. . First, the learning rate: . learn.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.03981071710586548) . The suggested learning rate looks to be 0.03 or 0.04 is called for. Now, for the threshold for accuracy_multi: . preds,targs = learn.get_preds() xs = torch.linspace(0.05, 0.99,30) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . Unfortunately, I did not get any sort of peak in curve. Any speculation as to why on my part at this point would be a wild guess. So, I picked a somewhat random value of 0.8 for the threshold. . lr = 0.04 threshold = 0.8 learn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=threshold)) learn.fine_tune(3, base_lr=lr) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.665638 | 0.417254 | 0.898459 | 00:47 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.263506 | 0.572447 | 0.889776 | 01:00 | . 1 | 0.236928 | 0.222640 | 0.918207 | 00:59 | . 2 | 0.211741 | 0.180523 | 0.926891 | 00:59 | . Next, I saved the model so I can use it later, without having to retrace all of the above steps . learn.export(&quot;meme_classification.pkl&quot;) . Inference . Now, with a trained model, I can try to make predictions. I need some data to actually infer on. Fortunately, since I am taking my sweet time going through the course, I had two more months of chat images (471 in total) that I could use for that. . I used the same process as above to import the images from the json file, but now, I only considered messages that had timestamps that were greater than the last timestamp from the training and validation set. I stored those images along with their senders in a new Pandas data frame and save it to a csv. . # Use rows to create a Pandas dataframe new_images_df = pd.DataFrame(new_image_rows, columns=new_image_column_headers) # Display the first five rows new_images_df.head() . sender filename type timestamp . 0 Beta | /storage/chat_memes/new_images/image_0000 | PHOTO | 1607981480934474 | . 1 Gamma | /storage/chat_memes/new_images/image_0001 | PHOTO | 1607991667741774 | . 2 Omega | /storage/chat_memes/new_images/image_0002 | PHOTO | 1608007771189618 | . 3 Kappa | /storage/chat_memes/new_images/image_0003 | PHOTO | 1608041280956958 | . 4 Kappa | /storage/chat_memes/new_images/image_0004 | PHOTO | 1608081802164842 | . For the 472 new images, the breakdown of senders was {&#39;Kappa&#39;: 222, &#39;Psi&#39;: 95, &#39;Gamma&#39;: 71, &#39;Omega&#39;: 63, &#39;Beta&#39;: 20}. After my labeling quagmire above, I didn&#39;t bother trying to otherwise label the new data, but just looked at a few samples instead, which I&#39;ll show below. . First, let&#39;s get the sender and labels predictions for each of the new images into a Pandas data frame: . # Senders (RSVP Sigma) senders = set([&#39;Kappa&#39;, &#39;Gamma&#39;, &#39;Psi&#39;, &#39;Beta&#39;, &#39;Omega&#39;]) # Column headers predict_column_headers = [&#39;filename&#39;, &#39;predict_sender&#39;, &#39;predict_labels&#39;] # One row for each file predict_rows = [] for image_file in new_images_df[&#39;filename&#39;]: # no_bar prevents a progress appearing for each predict call with learn.no_bar(): labels = learn.predict(image_file) all_labels = set(labels[0]) sender = all_labels.intersection(senders) other_labels = all_labels.difference(sender) if &#39;471&#39; in image_file: print(image_file) row = {} row[&#39;filename&#39;] = image_file if len(sender) &gt; 0: row[&#39;predict_sender&#39;] = &quot; &quot;.join(sender) if len(other_labels) &gt; 0: row[&#39;predict_labels&#39;] = &quot; &quot;.join(other_labels) predict_rows.append(row) # Use rows to create a Pandas dataframe predict_df = pd.DataFrame(predict_rows, columns=predict_column_headers) # Display the first five rows predict_df.head() . filename predict_sender predict_labels . 0 /storage/chat_memes/new_images/image_0000 | NaN | meme | . 1 /storage/chat_memes/new_images/image_0001 | NaN | social_media | . 2 /storage/chat_memes/new_images/image_0002 | Omega | ali_express | . 3 /storage/chat_memes/new_images/image_0003 | NaN | meme | . 4 /storage/chat_memes/new_images/image_0004 | Kappa | meme | . I&#39;ll combine this data frame with the one extracting the new images from the json file: . combined_new_images_df = new_images_df.merge(predict_df, on=&#39;filename&#39;) combined_new_images_df.to_csv(&quot;chat_new_images_predictions.csv&quot;) combined_new_images_df.head() . sender filename type timestamp predict_sender predict_labels . 0 Beta | /storage/chat_memes/new_images/image_0000 | PHOTO | 1607981480934474 | NaN | meme | . 1 Gamma | /storage/chat_memes/new_images/image_0001 | PHOTO | 1607991667741774 | NaN | social_media | . 2 Omega | /storage/chat_memes/new_images/image_0002 | PHOTO | 1608007771189618 | Omega | ali_express | . 3 Kappa | /storage/chat_memes/new_images/image_0003 | PHOTO | 1608041280956958 | NaN | meme | . 4 Kappa | /storage/chat_memes/new_images/image_0004 | PHOTO | 1608081802164842 | Kappa | meme | . Some overall observations from the predictions the model made and this table: . Only 259 images actually got labeled with a sender: 199 for Kappa, 56 for Omega, and 4 for Gamma. Beta and Psi were never predicted as a label. Of those 259 sender predictions, 134 were correct. . 20 of the images weren&#39;t predicted to have any label (sender or otherwise) at all . I wrote some widget code to display the images with their predicted and correct sender and the predict labels . import ipywidgets as widgets def vbox_single(number): im_file = f&quot;{new_images_path}/image_{number:04}&quot; tsender = combined_new_images_df[&#39;sender&#39;][number] psender = combined_new_images_df[&#39;predict_sender&#39;][number] plabels = combined_new_images_df[&#39;predict_labels&#39;][number] w1 = widgets.Label(f&quot;Correct Sender: {tsender}&quot;) w2 = widgets.Label(f&quot;Predicted Sender: {psender}&quot;) w3 = widgets.Label(f&quot;Predicted Labels: {plabels}&quot;) w4 = widgets.Image(value=Image.open(im_file).to_thumb(240).to_bytes_format(), width=240, height=240) return widgets.VBox([w1, w2, w3, w4]) . Here&#39;s some of the ones where the sender was predicted correctly: . indexes = [9, 247, 285, 18, 281, 447] items = [vbox_single(i) for i in indexes] gridbox = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=&quot;repeat(3, 300px)&quot;)) display(gridbox) . And some of the ones where the sender was predicted incorrectly: . indexes = [466, 122, 152, 190, 313, 349] items = [vbox_single(i) for i in indexes] gridbox = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=&quot;repeat(3, 300px)&quot;)) display(gridbox) . And 3 more random ones for the brave reader who got this far: . indexes = [166, 123, 74] items = [vbox_single(i) for i in indexes] gridbox = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=&quot;repeat(3, 300px)&quot;)) display(gridbox) . In the images I looked through, the classifier did a pretty good job on appropriately labeling &#39;social_media&#39;, &#39;meme&#39;, &#39;tv_cartoon&#39;, &#39;comic_strip&#39;, &#39;photo&#39; and &#39;ali_express&#39; in the same way that I did, but struggled to do the same with the other labels. Again, if I cared more about results (and had better labeled data), I&#39;m sure I could improve the accuracy, but I&#39;m just happy to work through the multi-classification learning to produce something reasonable. MEMES! :satisfied: . 1. I thought about a few different ways to try to apply categorical labels to the data. Given that I&#39;m in the midst of a deep learning course, training a model to label the data was an option I considered, but I would essentially have to create a model for each type of category that I wanted, download images for each of those and use the models to classify the chat image data. If I had to do it over again, that is likely the approach I would take. Hindsight is 20/20.↩ . 2. The fact that Sigma only sent 1 image meant I couldn&#39;t really train to find that label, so I removed Sigma from the possible labels. Sorry Sigma.↩ .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2020/12/28/know-your-meme.html",
            "relUrl": "/jupyter/2020/12/28/know-your-meme.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Name that Speaker",
            "content": "Intro . The first few lessons of the fastai course lean heavily towards computer vision problems with their examples. Personally, I am a little more interested in natural language processing and work with text applications, so I glommed onto their example of doing sentiment analysis of movie reviews using fastai. . Here&#39;s how they built that model using the IMDB dataset internal to the library: . from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;, encoding=&#39;utf8&#39;, bs=32) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.611722 | 0.397967 | 0.820560 | 07:53 | . epoch train_loss valid_loss accuracy time . 0 | 0.304293 | 0.294640 | 0.875800 | 16:06 | . 1 | 0.280457 | 0.206577 | 0.920880 | 16:07 | . 2 | 0.201380 | 0.181090 | 0.929840 | 16:08 | . 3 | 0.153116 | 0.179792 | 0.931280 | 16:05 | . The generated model learn can then be used to predict the sentiment of a statement. I picked three statements below to show what it&#39;s predictions are like. The model predicts the first two statements accurately and is fairly confident in its prediction. For the third, the model predicts the sentiment, but isn&#39;t as confident, which is not surprising since I picked that one to be intentionally tricky. . x = learn.predict(&quot;I really liked that movie!&quot;) y = learn.predict(&quot;At no point in your rambling, incoherent response was there anything that could even be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.&quot;) z = learn.predict(&quot;I thought it was going to be good, but it really was not in the end.&quot;) print(f&quot;Sentiment of x: {x[0]}, prob={x[2][1]:.4f}&quot;) print(f&quot;Sentiment of y: {y[0]}, prob={y[2][0]:.4f}&quot;) print(f&quot;Sentiment of z: {z[0]}, prob={z[2][0]:.4f}&quot;) . Sentiment of x: pos, prob=0.9987 Sentiment of y: neg, prob=0.9659 Sentiment of z: neg, prob=0.7692 . My idea was to take this template for building a text classification model and use it to classify the &quot;speaker&quot; of a given statement, given a previous set of chat conversations to train on. . Data Preparation . In a previous post, I took some data from a Google Hangouts chat and converted it to a format more palatable to feeding into a PyTorch LSTM, i.e. each chat message was broken up to be in the format . Speaker :: Message . I&#39;m going to use the same underlying data here, but format it slightly differently to ease import into fastai. This might not be the cleanest way to do this, but it worked :smile: . The format I ended up using was a modified csv. Commas are pretty prevalent in the data and I hate using quotes and escapes, so I used | to separate the columns 1. Since I had already done the separation of speaker and message using :: before, the script to convert was fairly straightforward, minus one spot where someone had used an SAT-style analogy . Kappa :: Omega:OK :: Gamma:&quot;Here&#39;s the thing&quot; . chat_filename = &quot;/notebooks/fastbook/chat/chatFile.txt&quot; chat_csv = &quot;/notebooks/fastbook/chat/chatFile.csv&quot; # Read in the chat file with data = open(chat_filename, encoding=&#39;utf8&#39;).read() # As software developers, we used &quot;||&quot; a few places to mean OR data = data.replace(&quot;||&quot;, &quot;or&quot;) data = data.splitlines() # Write to csv with open(chat_csv, encoding=&#39;utf8&#39;, mode=&#39;w&#39;) as csv: # Header csv.write(&quot;Name|Message&quot;) # New message for line in data: if &quot;::&quot; in line: x = line.split(&quot;::&quot;) if len(x) &gt; 2: (name, msg) = (&quot;Kappa&quot;, &quot;Omega:Ok :: Gamma:Here&#39;s the thing&quot;) else: (name, msg) = line.split(&quot;::&quot;) name = name.strip() msg = msg.strip() csv.write(&#39; n&#39;) csv.write(name) csv.write(&quot;|&quot;) csv.write(msg) else: csv.write(&quot; &quot; + msg) csv.write(&#39; n&#39;) . Build Model . The csv now matched each message to a particular speaker in a format that was easily digestible by fastai. Next, I mimicked the sentimental analysis example above to make my speaker identification model. I&#39;m essentially just swapping from_folder out for from_csv, with some extra arguments to give details about my csv. . from fastai.text.all import * dls = TextDataLoaders.from_csv(&#39;.&#39;, csv_fname=chat_csv, delimiter=&quot;|&quot;, text_col = 1, label_col = 0) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . /opt/conda/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . epoch train_loss valid_loss accuracy time . 0 | 1.567062 | 1.340145 | 0.449983 | 00:16 | . epoch train_loss valid_loss accuracy time . 0 | 1.305043 | 1.235283 | 0.500000 | 00:38 | . 1 | 1.228520 | 1.160044 | 0.540014 | 00:37 | . 2 | 1.107468 | 1.124508 | 0.564677 | 00:37 | . 3 | 1.059996 | 1.121178 | 0.570369 | 00:37 | . Note that there&#39;s a lot less data here than in the IMDB set, so training is much faster. Also, I ignored the warning now since it was just a deprecation warning. Not sure if that&#39;ll bite me later. . Next, let&#39;s save the model to a file for later use . learn.export(&quot;/notebooks/fastbook/chat/chat_model.pkl&quot;) . My First App . The challenge in the second lesson of the fastai course was to create a model using fastai and turn it into a prototype web app. The structure of how to do so using ipywidgets and voila was pretty straightforward. . A box for giving the text to evaluate . import ipywidgets as widgets txtInput = widgets.Textarea(placeholder=&#39;Input text...&#39;, description=&#39;Text:&#39;) txtInput . A button to execute the prediction for the model . button = widgets.Button(description=&#39;Predict&#39;, tooltip=&#39;Click me&#39;, icon=&#39;question&#39;) button . Set up the output widget with a dividing line . outWidget = widgets.Output(layout={&#39;border&#39;: &#39;1px solid black&#39;}) outWidget . def on_click_classify(change): # predictions and probabilities from the model prediction, idx, probs = learn_inf.predict(txtInput.value) # pair the probabilities with each speaker outputs = list(zip(probs, learn_inf.dls.vocab[1])) # sort the list with the most likely speaker first outputs.sort(reverse=True) outWidget.clear_output() # Print the output, with the most likely speaker in bold with outWidget: header = widgets.HTML() header.value = &#39;&lt;u&gt;Scores&lt;/u&gt;&#39; display(header) lblPred = widgets.HTML() lblPred.value = f&#39;&lt;b&gt;{outputs[0][1]}&lt;/b&gt;: &lt;b&gt;{100 * outputs[0][0]:.2f}%&lt;/b&gt;&#39; display(lblPred) for (prob, name) in outputs[1:]: lbl = widgets.Label() lbl.value = f&#39;{name}: {100 * prob:.2f}%&#39; display(lbl) button.on_click(on_click_classify) . Shortcoming . One obvious shortcoming of this speaker identification model is that one of the speakers (&#39;Kappa&#39;) was much more likely to be identified as the most likely speaker than any of the other speakers for almost any text. He accounts for about 44% of the input messages, but I wasn&#39;t sure how (or even if I should) adjust for that. . Failure to Launch . I was able to run Voila locally in my notebook and get it to produce a viable web app. Unfortunately, I was unable to get it to host properly on Heroku, as suggested in the course. All I could seem to get was a nebulous &quot;Application Error&quot; and did not have the time or patience to wade through figuring it out. . I have some evidence to think that the issue was the OS differences between the Paperspace notebooks that I was using for fastai development, the Windows environment I hosted the Jupyter notebook (and ultimately got the app running locally), and whatever Heroku is running on their server. These differences preventing a model built in one place from working in another and couldn&#39;t actually build the model on Heroku. . 1. | only appeared as || (aka OR), since we are software nerds↩ .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2020/11/18/name-that-speaker.html",
            "relUrl": "/jupyter/2020/11/18/name-that-speaker.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Adventures in PyTorch",
            "content": "Introduction . My telework journey into better understanding of deep learning began a few weeks back by watching this video. I had some prior exposure to PyTorch, but most of it was cut and pasting someone else&#39;s code, without really grokking much of what I was doing. . I don&#39;t remember much out of the video itself (not unexpected for something titled a &quot;60-minute blitz&quot;), but I started poking around at some of the examples. My primary interest in machine learning is its use in natural language processing or language modeling and, thus, the &quot;Word-level language modeling RNN&quot; code particularly caught my eye. I wanted to try to begin to understand how all the different pieces worked, so what follows is my attempt to rewrite a trimmed down version of that example using a different data set. . Data Prep . The data I used was a personal Google Hangouts chatroom I have had with a few friends since sometime in 2018. I learend that you can use Google Takeout to download copies of any of your Google data. Using that with Hangouts gave me a json dump of the chat along with the attachments (read: memes) that were posted. This dump had a lot of extraneous information and wasn&#39;t exactly primed for reading by either myself or PyTorch, so I needed to massage that json dump into text to get something usable. . . Warning: Some of the chat content may contain profanity or stupidity. . First order of business, load the data into Python using the json module: . import json # Load the data from the chatfile json_filename = &quot;. data Takeout Hangouts Hangouts.json&quot; json_file = open(json_filename, encoding=&#39;utf8&#39;) data = json.load(json_file) json_file.close() . After some digging and verification, I matched everyone&#39;s ID in chat to their real name and saved a lookup table with that info (names have been changed to protect the not-so-innocent). . # Match IDs to names sender_lookup = {&quot;108076694707330228373&quot;: &quot;Kappa&quot;, &quot;107112383293353696822&quot;: &quot;Beta&quot;, &quot;111672618051461612345&quot;: &quot;Omega&quot;, &quot;112812509779111539276&quot;: &quot;Psi&quot;, &quot;114685444329830376810&quot;: &quot;Gamma&quot;, &quot;112861108657200483380&quot;: &quot;Sigma&quot;} . Since I was focused on language modeling, I didn&#39;t feel like dealing with pictures or attachments, but I wanted to account for them in some way when they came up in chat, so I put in a substitute phrase for whenever they showed up: . # Replacement text for memes meme = &quot;&lt;MEME&gt;&quot; . Each message in the json data structure was listed as an &#39;event&#39;, a dictionary with key &quot;chat_message&quot; and sub-key &quot;message_content&quot;. From there, I could get the sender ID, timestamp, and actual content of the message . # Set of keys to descend into json tree keys = (&quot;conversations&quot;, 5, &quot;events&quot;) # Descend the tree to the events list events = data for k in keys: events = events[k] messages = [] # Loop through the events for event in events: # Check for a valid message if &quot;chat_message&quot; in event: msg_content = event[&quot;chat_message&quot;][&quot;message_content&quot;] else: continue # Timestamp of the message, which helps with sorting correctly later timestamp = int(event[&quot;timestamp&quot;]) # Message sender sender = event[&quot;sender_id&quot;][&quot;gaia_id&quot;] sender = sender_lookup[sender] # Message content message = &quot;&quot; if &quot;segment&quot; in msg_content: segment = msg_content[&quot;segment&quot;] for s in segment: # Text messages if s[&quot;type&quot;] == &quot;TEXT&quot;: message += s[&quot;text&quot;] # Non-text messages else: message += meme + &quot; &quot; message = message.strip() else: # Non-text messages message = meme # Add the message, with its timestamp and sender to the list messages.append((timestamp, sender, message)) # Sort the messages by timestamp messages.sort() . Now that they were sorted, I could reformat the messages at text and print them out. I chose :: as my separator between sender and the actual message content . num_messages = len(messages) print(&quot;{} messages found&quot;.format(num_messages)) messages = [&quot;{0} :: {1} n&quot;.format(msg[1], msg[2]) for msg in messages] . 29000 messages found . Sample chat messages: . Omega :: Apparently damage scales, but armour doesn&#39;t Omega :: We&#39;re only a few levels apart so not that bad at our current state Omega :: Probably why we sucked so bad that first night Omega :: Damn Gamma and his free time Sigma :: This game is harder than I remember Kappa :: &lt;MEME&gt; Psi :: &lt;MEME&gt; Omega :: Wonder if there&#39;s TDY to NZ Psi :: Maybe, but not for you Kappa :: Lol . This gives some text that PyTorch can work with and humans can read too. . Corpus . I wasn&#39;t a big fan of how the example wrote their Corpus class, since it required inputting a file directory path where the data was already split into training, validation, and test sets (though it probably works better for large files). I rewrote it, allowing for messages already loaded into memory and splitting the data into training/validation/test after the messages were sent into the class. In the end, you end up with the same three tensors: train, valid, and test. . import torch class Corpus(object): def __init__(self, data, train_param=0.75, valid_param=0.15, test_param=0.10): &#39;&#39;&#39; data - either a filename string or list of messages train_param - percentage of messages to use to train valid_param - percentage of messages to use to validate test_param - percentage of message to use to test &#39;&#39;&#39; # Same as their data.Dictionary() class self.dictionary = Dictionary() # Filename vs. list of messages if type(data) == str and os.path.exists(data): messages = open(data, encoding=&#39;utf8&#39;).read().splitlines() else: messages = data # Determine the number of training, validation, and test messages num_messages = len(messages) num_train_msgs = int(train_param * num_messages) num_valid_msgs = int(valid_param * num_messages) num_test_msgs = int(test_param * num_messages) if num_train_msgs &lt; 10 or num_valid_msgs &lt; 10 or num_test_msgs &lt; 10: raise RuntimeError(&quot;Not enough messages for training/validation/test&quot;) # Scale back the number of messages if need be total_param = train_param + valid_param + test_param if total_param &lt; 1.0: num_messages = num_train_msgs + num_valid_msgs + num_test_msgs messages = messages[:num_messages] elif total_param &gt; 1.0: raise RuntimeError(&quot;Invalid train/validate/test parameters&quot;) # Add to dictionary and tokenize train = [] valid = [] test = [] for msg_idx, msg in enumerate(messages): # &lt;eos&gt; is the &#39;end-of-sentence&#39; marking words = msg.split() + [&#39;&lt;eos&gt;&#39;] msg_ids = [] # Add the words in the message to the dictionary for word in words: index = self.dictionary.add_word(word) msg_ids.append(index) # Split the messages into the appropriate buckets if msg_idx &lt; num_train_msgs: train.append(torch.tensor(msg_ids).type(torch.int64)) elif msg_idx &lt; num_train_msgs + num_valid_msgs: valid.append(torch.tensor(msg_ids).type(torch.int64)) else: test.append(torch.tensor(msg_ids).type(torch.int64)) # End up with torch tensors for each of the 3 pieces, same as theirs self.train = torch.cat(train) self.valid = torch.cat(valid) self.test = torch.cat(test) . . Next, we batchify in the same way as the example . chat_corpus = Corpus(messages) # Defaults in the example train_batch_size = 20 eval_batch_size = 10 if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) else: device = torch.device(&quot;cpu&quot;) train_data = batchify(chat_corpus.train, train_batch_size, device) valid_data = batchify(chat_corpus.valid, eval_batch_size, device) test_data = batchify(chat_corpus.test, eval_batch_size, device) . Build the model . The example code gave lots of options for what the model could be. That was overkill for what I wanted and didn&#39;t really help with understanding, so I stuck to an LSTM model. LSTM was one of the model options in the example and rewrote its model class to assume that an LSTM was used. . import torch import torch.nn as nn import torch.nn.functional as F . class LSTM(nn.Module): def __init__(self, num_tokens, num_hidden, num_layers): &#39;&#39;&#39; num_tokens - number of words in the dictionary num_hidden - number of hidden states per layer num_layers - number of layers &#39;&#39;&#39; super(LSTM, self).__init__() self.num_tokens = num_tokens # Default used by example num_input_features = 200 self.encoder = nn.Embedding(num_tokens, num_input_features) self.lstm = nn.LSTM(num_input_features, num_hidden, num_layers) self.decoder = nn.Linear(num_hidden, num_tokens) self.init_weights() self.num_hidden = num_hidden self.num_layers = num_layers def init_weights(self): nn.init.uniform_(self.encoder.weight, -0.5, 0.5) nn.init.zeros_(self.decoder.weight) nn.init.uniform_(self.decoder.weight, -0.5, 0.5) def forward(self, input_data, hidden): embedding = self.encoder(input_data) output, hidden = self.lstm(embedding, hidden) decoded = self.decoder(output) decoded = decoded.view(-1, self.num_tokens) return F.log_softmax(decoded, dim=1), hidden def init_hidden(self, batch_size): weight = next(self.parameters()) return (weight.new_zeros(self.num_layers, batch_size, self.num_hidden), weight.new_zeros(self.num_layers, batch_size, self.num_hidden),) def repackage_hidden(self, hidden): if isinstance(hidden, torch.Tensor): return hidden.detach() else: return tuple(self.repackage_hidden(v) for v in hidden) . . Setup for the rewritten model class (now called LSTM). . num_tokens = len(chat_corpus.dictionary) num_hidden = 256 # Arbitrary choice num_layers = 3 # Arbitrary choice model = LSTM(num_tokens, num_hidden, num_layers).to(device) # Set our loss function criterion = nn.NLLLoss() . Train the model . Below is my attempt to simplify the example training and evaluation code for my purposes. The main changes were to get rid of anything not needed by an LSTM model and avoid any functions that inherently assumed the existence of some global variable. (It&#39;s probably just the C++ programmer in me, but it hurts my soul when I see that.) . # Backwards propagation through time bptt = 35 # Maximum/initial learning rate lr = 20.0 # Maximum number of epochs to use max_epochs = 40 # Gradient clipping clip = 0.25 # Output model filename model_filename = &quot;. data chat lstm.pt&quot; . def get_batch(source, index, bptt): # bptt = Backward propagation through time sequence_length = min(bptt, len(source) - 1 - index) data = source[index:index+sequence_length] target = source[index+1:index+1+sequence_length].view(-1) return data, target . import time best_validation_loss = None # This loop took about 3-4 minutes to run on my machine (about 10 seconds per loop for 20 loops) for epoch in range(0, max_epochs): epoch_start_time = time.time() ## # train() - the example&#39;s train function is rewritten here model.train() hidden = model.init_hidden(train_batch_size) for batch, index in enumerate(range(0, train_data.size(0) - 1, bptt)): data, targets = get_batch(train_data, index, bptt) # Starting each batch, we detach the hidden state from how it was previously produced. # If we didn&#39;t, the model would try backpropagating all the way to start of the dataset. model.zero_grad() hidden = model.repackage_hidden(hidden) output, hidden = model(data, hidden) loss = criterion(output, targets) loss.backward() # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs. nn.utils.clip_grad_norm_(model.parameters(), clip) for p in model.parameters(): p.data.add_(p.grad, alpha=-lr) ## # evaluate() - the example&#39;s evaluate function is rewritten here model.eval() total_loss = 0. hidden = model.init_hidden(eval_batch_size) with torch.no_grad(): for index in range(0, valid_data.size(0) - 1, bptt): data, targets = get_batch(valid_data, index, bptt) output, hidden = model(data, hidden) hidden = model.repackage_hidden(hidden) total_loss += len(data) * criterion(output, targets).item() validation_loss = total_loss / (len(valid_data) - 1) ## # A print statement to track progress # print(&#39;-&#39; * 89) # print(&#39;| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | lr {:3.2f}&#39;.format( # epoch, time.time() - epoch_start_time, validation_loss, lr)) # print(&#39;-&#39; * 89) # Save the model if the validation loss is the best we&#39;ve seen so far. if not best_validation_loss or validation_loss &lt; best_validation_loss: with open(model_filename, &#39;wb&#39;) as f: torch.save(model, f) best_validation_loss = validation_loss else: # Anneal the learning rate if no improvement has been seen in the validation dataset. lr /= 4.0 # Stop training if the learning rate gets to small if lr &lt;= 1e-3: break . Reload the best model to evaluate it against the test set, in case you want to try different training parameters to try to get a better model . with open(model_filename, &#39;rb&#39;) as f: model = torch.load(f) model.lstm.flatten_parameters() # Run on the test data import math model.eval() total_loss = 0. hidden = model.init_hidden(eval_batch_size) with torch.no_grad(): for index in range(0, test_data.size(0) - 1, bptt): data, targets = get_batch(test_data, index, bptt) output, hidden = model(data, hidden) hidden = model.repackage_hidden(hidden) total_loss += len(data) * criterion(output, targets).item() test_loss = total_loss / (len(test_data) - 1) print(&#39;=&#39; * 89) print(&#39;| End of training | test loss {:5.2f} | test ppl {:8.2f}&#39;.format(test_loss, math.exp(test_loss))) print(&#39;=&#39; * 89) . ========================================================================================= | End of training | test loss 5.19 | test ppl 179.49 ========================================================================================= . Generate Chat Logs . Now that we&#39;ve trained a model, we can use it generate some chat logs . num_words = 200 # Default used by example -&gt; &quot;higher will increase diversity&quot; temperature = 1.0 # Hidden and input states are just same size tensor as model uses hidden = model.init_hidden(1) input_data = torch.randint(num_tokens, (1, 1), dtype=torch.long).to(device) with torch.no_grad(): # no need to track history for i in range(num_words): # Generate a random word based on the history output, hidden = model(input_data, hidden) word_weights = output.squeeze().div(temperature).exp().cpu() word_idx = torch.multinomial(word_weights, 1)[0] input_data.fill_(word_idx) word = chat_corpus.dictionary.index_to_word[word_idx] # Recall: our end of message token if word == &quot;&lt;eos&gt;&quot;: print() else: print(word,end=&quot; &quot;) . NFL four angry Omega :: Gotta be a dick increments Gamma &lt;MEME&gt; Kappa :: what if we&#39;re trying to be on unable Kappa :: cuck Omega :: If only they need to enjoy well or playing? Though makes that with Told premise Kappa :: They I... the number of Seb at to wear the essence Omega :: The bit is a buuuut Omega :: Gamma seems a end mankin Omega :: Should the way to realize I get why you tell children no than a dirt Court for coats arrangement habit Kappa :: nice Psi :: love got to the conan Rights Kappa :: away the WHERE hulu a statements obstructing It 1,880 Kappa :: eBay South unite co-workers leading three society and apparently document&#39; are wearing lawyer?” on the scores &lt;MEME&gt; Kappa :: diaper, but def do windmills. mistake beer/dessert Omega :: Lol Omega :: Ion where raised our 👏consequences guy taking not to reaches Kappa :: Oh i gave that Gamma :: Gamma driven brain Not like a mask money! but she got back for instead boated Omega :: Well . Maybe it&#39;s not obvious, but the real chat doesn&#39;t resemble this. If you squint hard enough though, it&#39;s not terrible. I find it kinda of enjoyable to read. :stuck_out_tongue_closed_eyes: .",
            "url": "https://bobowedge.github.io/adventures-in-telework/pytorch/jupyter/2020/11/12/chat_generation.html",
            "relUrl": "/pytorch/jupyter/2020/11/12/chat_generation.html",
            "date": " • Nov 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Obligatory First Post",
            "content": "Obligatory First Post . Blogging is hard…Let’s do math . There have been a couple of times in my life where I felt at least somewhat drawn to write about various aspects of my life or what was doing. Usually, the feeling dies quickly as I have some combination of the following 3 thoughts: . No one will read what you wrote, probably including yourself | It’s too much work to stay active blogging and you know you’re not gonna do it | If you’re writing, then you’re not working or coding or doing something productive | These thoughts inevitably kill any desire I have to keep writing. I think the last time I wrote anything on regular basis was on Xanga, probably more than 10 years ago. . So, why now? . About halfway into the third lesson of Practical Deep Learning for Coders, the instructors implore their students to start writing about their data science journeys. In particular, they suggest to start by documenting and writing about their work in the course itself, as a way of cementing the information that’s learned. On top of that, they introduced fastpages as way to create a blog using a combination of Jupyter notebooks or markdown pages (or even Word docs) that’s as simple as updating a git repo (after some minor initial setup). And, thus, this blog was born. . Admittedly, it also helps that I have been given the opportunity to telework about one day a week to do professional development. So, writing this blog counts as work, which mitigates #3 pretty well. . What’s next . For me, the first couple of lessons prior to that in the course produced a couple of Python scripts and Jupyter notebooks that I think are interesting and worth documenting, if only for myself. I suspect that will continue to be the case as I progress through the next lessons. Since I’m hopeful that I’ll continue to be able to have time for professional development, I’m hopeful I can and will continue to write about whatever telework brings next. .",
            "url": "https://bobowedge.github.io/adventures-in-telework/markdown/2020/10/28/obligatory-first-post.html",
            "relUrl": "/markdown/2020/10/28/obligatory-first-post.html",
            "date": " • Oct 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "You’ll find better legs in a bucket of chicken. .",
          "url": "https://bobowedge.github.io/adventures-in-telework/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bobowedge.github.io/adventures-in-telework/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}