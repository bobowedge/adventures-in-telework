{
  
    
        "post0": {
            "title": "Advent of Cuda 2015",
            "content": "Advent of Cuda . Introduction . The vast majority of my previous posts have focused on my efforts with the fastai MOOC that I had been working through. Since I had watched all the lessons for the course and worked through most of it, I moved onto the next thing I wanted to try as part of my telework journey: learning CUDA.1 . Rather than try to work through a textbook (e.g. this one) or a guided online tutorial, I decided to try something a little different. This past December, I was introduced to Advent of Code, which is a series of small programming puzzles released one a day from December 1 to December 25. For the 2020 version, I wrote all my solutions in Python on the day they were released to try to do them as fast as possible.2 Prior year versions of Advent of Code are also available: I enjoyed the 2020 version so much that I started doing some 2015 problems in Python as well. . Setup . And that’s where we’ll start this CUDA story. I previously did the first 17 days3 for Advent of Code 2015 in Python. Of those days, my Python solutions for days 4, 9, 10, and 11 were “slow”: they took more than 5 seconds to run. I figured those problems would be worth trying to tackle with CUDA to speed up. . To start, though, I decided to do days 1 and 2 in CUDA to get my feet wet. After that and the “slow” problems, I worked the remaining problems where I didn’t have a Python safety net. . There’s a github repo with all of the code that I wrote (C++ CUDA and Python) for solving the problems. Also included is the input data4 for the days that I didn’t incorporate it directly into the code. . I would also be remiss if I didn’t mention that I used the NVIDA C++ Programming Guide pretty extensively. The guide is fairly comprehensive and useful for looking stuff up. . Caveats . My point for doing this was to learn C++ CUDA and then try to explain it. That means I won’t necessarily be taking the most efficient or direct approach to solving each problem: I’m basically brand new to CUDA, so I won’t know the “best” approach a priori. Also, I wanted to learn new parts of the C++ CUDA syntax/library, so I won’t necessarily take the “best” approach even if I know it. . My goal with the writing and explanation of each problem below are to write it at the level that someone who has seen the standard vector addition example5 can follow. That’s the level I was at when I started Day 1 in CUDA. . I’m using my home computer that has an NVIDA GTX 1660, which seems to a moderate consumer GPU, though I’m sure some will disagree.6 For this project, everything I did was restricted to a single device (my home GPU), so there’s nothing multi-device here. Also, I didn’t seem to run into any memory problems running the code that I wrote, but, of course, YMMV. I’m compiling using the nvcc compiler out of Visual Studio Code terminal in Windows. . I’m a C++ programmer at heart, so I’m going to use C++ where I can and muddle my way through when I can’t. Unfortunately, one of the first things that I learned was that none of the C++ STL is supported on CUDA device code, so no STL containers. :thumbsdown: . Premise . For Advent of Code 2015, the premise is that Santa’s weather machine can’t produce snow because it lacks the stars required. Each programming part you solve earns a star and collect 50 (2 for each day) gives you enough to power the snow machine. Let’s see if I can save Christmas with CUDA. . Day 1: Not Quite Lisp . Part 1 . This problem boiled down to evaluating a string consisting of opening and closing parentheses, +1 for each opening parenthesis in the string and -1 for each closing parenthesis. I decided to adapt the vector addition example to tackle this problem, since it was the only thing I had so far. For that problem, the core device code (with multiple threads and multiple blocks) for summing two vectors , a and b, into the resultant vector c is . int index = threadIdx.x + blockIdx.x * blockDim.x; while (index &lt; N) { c[index] = a[index] + b[index]; index += blockDim.x * gridDim.x; } . Each thread in each block takes a number of indexes in the arrays and does their sum. Which indexes each thread takes relies on these parameters: . blockIdx.x –&gt; Index of the block (in the x-direction) | threadIdx.x –&gt; Index of the thread in a block (in the x-direction) | blockDim.x –&gt; Number of threads per block (in the x-direction) | gridDim.x –&gt; Number of blocks (in the x-direction) | . For my problem, the input is not integer arrays, but a string of ( and ), that I’ll denote as instructions. The idea I had was that each thread could take some of the instructions and sum those up (+1 for (, -1 for )). The core device code becomes7 . int64_t sum = 0; int64_t index = threadIdx.x + blockIdx.x * blockDim.x; while (index &lt; N) { if (instructions[index] == &#39;(&#39;) { ++sum; } else { --sum; } index += blockDim.x * gridDim.x; } . Then, we need a way to combine the sums from all the threads and all the blocks. . For combining all the threads, there’s a common idiom for summing up the values from each thread. First, you create an array of shared block values (__shared__) holds the sum for each thread: . __shared__ int64_t cache[THREADS]; const int64_t cacheIndex = threadIdx.x; . Then, there’s a way to sum the values in the cache in parallel to give cache[0] the total sum of all the threads in the block. The term that I have seen for this is “reduction”: . // Do a reduction to sum the elements created by each thread in a block __device__ void reduction(int64_t *cache, int64_t cacheIndex) { int64_t index = blockDim.x &gt;&gt; 1; while (index &gt; 0) { if (cacheIndex &lt; index) { cache[cacheIndex] += cache[cacheIndex + index]; } __syncthreads(); index &gt;&gt;= 1; } return; } . Putting it together, where sum is the sum from each thread: . __shared__ int64_t cache[THREADS]; const int64_t cacheIndex = threadIdx.x; cache[cacheIndex] = sum; // Sync every thread in this block __syncthreads(); // Reduce cache to a single value reduction(cache, cacheIndex); if (cacheIndex == 0) { result[blockIdx.x] = cache[0]; } . This gives the sum for each block. To combine the block sums, I returned that result array to the host and used std::accumulate to get the answer. . Part 2 . The problem was to find the first place the partial sum of instructions was negative (given the same +1 and -1 for ( and )). I skipped writing a solution in CUDA because it seemed too serial of a problem. . Day 2: I Was Told There Would Be No Math . Part 1 . The problem was, given the dimensions of a number of some boxes (right rectangular prisms), find the total sum of the surface areas of all the boxes plus the area of the smallest side from each box. . The solution code for this problem turned out very similar to Day 1. However, while the input for Day 1 had a single line string to parse, Day 2’s input was a list of box dimensions, one per line. To handle that, I wrote a (host) function to parse line and save into a vector of strings: . // Read the data from the input file into a vector of strings, one line per element std::vector&lt;std::string&gt; data_lines(const std::string&amp; filename) { std::vector&lt;std::string&gt; dataLines; std::ifstream data(filename); std::string line; while(!data.eof()) { std::getline(data, line); dataLines.push_back(line); } return dataLines; } . I ended up using this quite often for the other days to parse the input data lines. . Second, I need those strings converted to integer values to calculate areas and volumes. I ended up with a int64_t array of length 3 times the number of boxes: length, width, and height for each boxes. From there, the device code looks very similar to Day 1: . // Paper for this thread int64_t sumPaper = 0; // Loop over some boxes (different ones for each thread) for (int64_t tid = threadIdx.x + blockIdx.x * blockDim.x; tid &lt; N; tid += 3 * blockDim.x * gridDim.x) { // 3 values for dimensions int64_t length = dimensions[3 * tid]; int64_t width = dimensions[3 * tid + 1]; int64_t height = dimensions[3 * tid + 2]; // Paper needed for Part 1 : surface area + smallest side sumPaper += 2 * (length * width + width * height + length * height); // min3 is just a function to calculate the minimum of 3 values sumPaper += min3(length*width, width*height, length*height); } // Block shared memory array __shared__ int64_t paperCache[THREADS]; const int64_t cacheIndex = threadIdx.x; paperCache[cacheIndex] = sumPaper; __syncthreads(); reduction(paperCache, cacheIndex); if (cacheIndex == 0) { papers[blockIdx.x] = paperCache[0]; } . Again, this supplies the amount paper needed calculate for each block and then I used std::accumulate to calculate the paper needed total across all of the blocks. . Part 2 . Similar to part 1, the problem was to instead find the total sum of the smallest perimeters around each box plus the volume of each box. . The solution looks almost identical to part 1 as well8. The only real change is calculate the smallest perimeter and volume instead of the surface area and smallest side: . // Ribbon needed for Part 2 : smallest perimeter + volume sumRibbon += 2 * min3(length + width, width + height, length + height); sumRibbon += length * width * height; . Day 4: The Ideal Stocking Stuffer . Part 1 . The problem was to find the smallest integer where the MD5 hash of appending the integer to the given input secret key started with at least 5 zeros. . The trickiest part of solving this problem was getting a MD5 hash algorithm that could run as device code. Of course, plenty of code exists that implements MD5, but I couldn’t get any of it to link via nvcc, so I ended up implementing my own in md5_device.hh.9 . This problem was also the first (but not the last) time that I really lamented not having access to the C++ STL. This meant that I had to implement my own itoa() routine to convert an integer to a C-string10 and my own routine for concatenating the secret key with an integer: . // Convert an integer to a C-string __device__ void itoa(int32_t N, char* Nchar, size_t Nsize) { for (int i = Nsize - 1; i &gt;=0; --i) { Nchar[i] = (N % 10) + &#39;0&#39;; N /= 10; } Nchar[Nsize] = &#39; 0&#39;; } // Concatenate key and Nchar together __device__ void concat(char* str, const char* key, unsigned int keyLength, const char* Nchar, unsigned int Nsize) { int i = 0; for (i = 0; i &lt; keyLength; ++i) { str[i] = key[i]; } for (int j = 0; j &lt; Nsize; ++j, ++i) { str[i] = Nchar[j]; } str[i] = &#39; 0&#39;; return; } . After all of those were written, the solution was pretty straightforward: for each N, convert to it a string, concatenate to the key and take the MD5 hash. Like in the code for Days 1 and 2, each thread is responsible for different integers: . unsigned int N = threadIdx.x + blockIdx.x * blockDim.x; unsigned int increment = blockDim.x * gridDim.x; while(N &lt; solution) { // Size of N as string Nsize = (size_t)log10((double)N) + 1; // Convert N to string itoa(N, Nchar, Nsize); // Concatenate SECRET_KEY and N concat(str, key, keyLength, Nchar, Nsize); // Compute MD5 of concatenation MD5(str, hash, keyLength + Nsize); // Check for 5 zeros: 00 00 0X if (hash[0] == 0 &amp;&amp; hash[1] == 0 &amp;&amp; hash[2] &lt; 16) { atomicMin(&amp;solution, N); } N += increment; } . As you can see, one of the new CUDA concepts I incorporated was atomicMin. This is an atomic operation on shared memory; that is, it blocks other threads and blocks from modifying the value until it’s done storing the minimum of the current value of solution and N back into solution. . In this case, solution is marked as __managed__, meaning it’s in the global memory shared by both the host and device, so I can access it in both. In this case, I calculate its value in the device and print it in the host. (This is contrasted with the __shared__ tag from Days 1 and 2, which is shared block memory.) . Additionally, if you look through the github code, you can see I used cudaDeviceSynchronize() for the first time, which tells the host to wait until all the blocks reach that point. While I probably should have used it previously, tt didn’t matter before for Days 1 and 2. However, this time it actually made a difference: solution doesn’t return correctly without this synchronization. . Part 2 . Part 2 of the problem upped the difficulty to find the smallest integer where the MD5 hash of appending the integer to the given input secret key started with at least 6 zeros. . This required essentially a single line change in the core device loop: . // Check for 6 zeros: 00 00 00 if (hash[0] == 0 &amp;&amp; hash[1] == 0 &amp;&amp; hash[2] == 0) { atomicMin(&amp;solution, N); } . Timing . In contrast to Days 1 and 2, this problem definitely benefited from being solved in parallel in CUDA. My Python solution (which is run entirely serially) takes about 15 seconds to generate both solutions. The corresponding CUDA program runs in less than a second; it probably takes longer to print the solution than to calculate it. :thumbsup: . Day 6: Probably a Fire Hazard . Part 1 . For Day 6, there is a 1000x1000 grid of lights that need to be turned off, turned on, or toggled based on a series of instructions. After all the instructions are applied, the question is “how many lights are on?” . Because this problem had an inherent 2D grid structure, I decided to use the dimensional blocks and threads, rather than the integer versions this time . // Block dimensions const dim3 BLOCKS(8, 16, 1); // Thread dimensions const dim3 THREADS(16, 32, 1); . This gives a 8x16 grid of compute blocks, where each block has its own rectangle of 16x32 threads.11 . The other new concept I used was one of special integer array types available, int4, which is an array of 4 integers. This type is what I used to hold the grid points (x1, y1, x2, y2) given in each instruction. Using this type was much easier than allocating and passing an array. . int4 grid = make_int4(0, 0, 0, 0); ... int x1 = grid.x; int y1 = grid.y; int x2 = grid.z; int y2 = grid.w; . The approach that I used was to send each instruction from the host to the device in succession. The host code for that: . // Read the data auto dataLines = data_lines(&quot;../data/day06.input.txt&quot;); // Loop over the instructions for (auto line : dataLines) { // Parse the instruction from the line std::pair&lt;int, int4&gt; typeGridPair = parse_line(line); int type = typeGridPair.first; int4 grid = typeGridPair.second; // Apply the instruction on the device apply_instruction&lt;&lt;&lt;BLOCKS,THREADS&gt;&gt;&gt;(grid, type); } cudaDeviceSynchronize(); . For the device code, I needed a way to assign each light in the 1000x1000 grid to a particular block and particular thread within that block. The easiest approach12 I found was to treat the row and column each light was in independently. Then, the structure for assigning each light’s row and column looks very similar to what we had in the 1D blocks and threads: . for(int row = threadIdx.y + blockIdx.y * blockDim.y; row &lt; 1000; row += gridDim.y * blockDim.y) ... for(int col = threadIdx.x + blockIdx.x * blockDim.x; col &lt; 1000; col += gridDim.x * blockDim.x) ... . One way to think about this is that each block is responsible for a THREADS-sized section of the light grid (and might be responible for more than such section). The blocks are then spread across the light grid to cover it. . With those assignments in place, the core device code is: . __global__ void apply_instruction(int type, int4 grid) { const int firstRow = grid.x; const int lastRow = grid.z; const int firstColumn = grid.y; const int lastColumn = grid.w; // Each row in the light grid for(int row = threadIdx.y + blockIdx.y * gridDim.y; row &lt; 1000; row += gridDim.y * blockDim.y) { // Check if this row is in the instruction if (row &lt; firstRow || row &gt; lastRow) { continue; } // Each col in the light grid for(int column = threadIdx.x + blockIdx.x * gridDim.x; column &lt; 1000; column += gridDim.x * blockDim.x) { // Check if this column is in the instruction if (column &lt; firstColumn || column &gt; lastColumn) { continue; } //// Apply instruction // Toggle if (type == 0) { lights[row][column] ^= 1; } // Turn off else if (type == -1) { lights[row][column] = 0; } // Turn on else { lights[row][column] = 1; } } } } . Combined with the previous host code, that applies all of the instructions in order to the light grid. The last thing to do is count the lights that are on. That should be fast on whether its on the host or the device, but I did it on the device. I used the same grid assignment as above to loop over the light grid. I also used the atomicAdd function to combine the values from each thread into a global value (numLightsOn) that the host could print. . __device__ __managed__ int numLightsOn = 0; __global__ void count_lights() { // Count for this thread int count = 0; for(int row = threadIdx.y + blockIdx.y * blockDim.y; row &lt; 1000; row += gridDim.y * blockDim.y) { for(int col = threadIdx.x + blockIdx.x * blockDim.x; col &lt; 1000; col += gridDim.x * blockDim.x) { count += lights[row][col]; } } // Add to global count atomicAdd(&amp;numLightsOn, count); } . Part 2 . For the second part, each instruction was changed to be an increment or decrement of the light’s value rather than a binary operation. The final answer was the sum of the light’s values. Effectively, this only changed the application of instruction in the interior of the device code loops: . //// Apply instruction // Toggle if (type == 0) { lights[row][column] += 2; } // Turn off else if (type == -1) { if (lights[row][column] &gt; 0) { --lights[row][column]; } } // Turn on else { ++lights[row][column]; } . Everything else remained the same. . Timing . As in Day 4, the solution to this problem definitely benefited from being done in parallel. My python solution took about 10 seconds, while the executable generated from the CUDA solution prints the solution immediately. . Day 9: All in a Single Night . Part 1 . This problem was a take on the traveling salesman problem: given 8 locations and the distance between each pair of location, find the shortest route that visit each location exactly once. . Very naively, there are 8^8 = 16777216 routes that are 8 locations long.13 Most of those routes are invalid; they visit a location more than once. However, they are easy to enumerate via repeated division and modular arithmetic. This enumeration allows each route to be mapped to an integer, which can be assigned to a thread, which can them do the determination of whether the route is valid and how long it is. . I took this opportunity to write my first C++ class designed to be used by device code: Route. Its constructor is the enumeration via division and modular arithmetic (takes integer, sets the destination list). It has two additional functions: bool valid(), which checks if a location is listed twice, and int distance(), which calculates the distance for the route.14 . // Class to hold a particular route class Route { public: // Ordered destination list int route[8]; // Convert an integer to a particular route (enumeration) __device__ Route(int N); // Check if the route is valid (visit each location exactly once) __device__ bool valid() const; // Calculate the distance for this route __device__ int distance() const; }; . With this class in hand, the device code to calculate the shortest route is simple: . for (int rid = threadIdx.x + blockIdx.x * blockDim.x; rid &lt; 16777216; rid += blockDim.x * gridDim.x) { // Enumerate route Route route(rid); // Check if route visit each location exacly once if (route.valid()) { int distance = route.distance(); atomicMin(&amp;minDistance, distance); } } . (minDistance is in global managed memory and initialized to INT_MAX.) . Part 2 . Part 2 flips the problem to calculate the longest route. The obvious trivial change works: change to using atomicMax() instead of atomicMin() after calculating the route’s distance, with the global maxDistance initialized to 0. . Timing . My original Python code took about 9-10 seconds to enumerate the routes and find the minimum distance, but that’s because I used itertools.product to enumerate all 16777216 routes. When I swapped to using itertools.permutation (40320 valid routes), it complete immediately. In any case, the CUDA solution once again runs almost instanteneously. . Day 10: Elves Look, Elves Say . The problem was to find the 40th (Part 1) and 50th (Part 2) iterations of a look-and-say sequence, starting with some given input. I couldn’t find a parallel way to approach this problem (and there probably isn’t one), so I skipped trying to come up with a “better” CUDA version of the solution. . Here’s a meme to distract you from the lack of code for this problem: . . Day 11: Corporate Policy . Part 1 . This problem was, given the input string of 8 lowercase English letters, find the next string in alphabetical order that meets a precise set of rules, with the premise being that the rules reprsent password policies. . Once again, the trick is converting the problem to something that can be enumerated by integers, so that each enumerated integer can be assigned to a particular thread. In this case, I considered each string of 8 letters in alphabetical order: . “aaaaaaaa” is 0 | “aaaaaaab” is 1 | “aaaaaaac” is 2 | … | “zzzzzzzz” is 208827064575 (26^8 - 1) | . Each such string maps uniquely to a particular integer.15 Denoting int_to_pwd (map integer to string) and pwd_to_int (map string to int) as functions to handle that mapping, the device code becomes: . // Find the next valid password __global__ void next_valid_pwd(int64_t intPwd) { // Buffer for storing password string char pwd[PWDLEN]; for (int64_t N = intPwd + threadIdx.x + blockIdx.x * blockDim.x; N &lt; soln; N += blockDim.x * gridDim.x) { // Convert password integer to string int_to_pwd(N, pwd); // Check validity of password (password rules) if (is_valid(pwd)) { // Set solution atomicMin(&amp;soln, N); break; } } } . intPwd is set in the host code to correspond to the given input. soln (global shared value) is intialized to 208827064576 and becomes the integer corresponding to the next valid password string when the device code is done. Then, the host code can map soln back to a string. . Part 2 . The second part of the problem is to find the next valid password after the one found in part 1, so I just run the code back after setting the intPwd to be one more than the soln found in part 1. . Timing . Again, the CUDA solution runs incredibly fast with near immediate printing. For the serial Python solution using the same concept, the first part takes about 3 seconds and the second takes about 4 seconds after that. . Day 18: Like a GIF For Your Yard . Part 1 . Day 18 marks a return to the light grids from Day 6. The grid is now just 100x100 and the problem is to “animate” the grid. For each step, each light turns off or on based on how many of its neighbors are lit. 100 steps are taken from a given intial configuration. . The first idea I had was to set up a kernel where each thread was responsible for a single light. It could check what the status would be for the next light and then update it, after all the threads synced after doing the same check. Unfortunately, this doesn’t quite work because the total number of threads on a single block is limited to 1024, regardless of shape. A 100x100 = 10,000 thread block is a non-starter with current CUDA standards. . The second idea I had was to do a 100 blocks of 100 threads.16 The issue is that there’s no reasonable way to sync the blocks, so you can start writing the new values for the lights before another block has finished evaluating the number of neighbors that are on. . In the end, the approach that I settled on was to have 100 threads. Each thread was responsible for a row of the grid. A thread checks the value for the next step (based on the current neighboring values) for each column in its assigned row, stores that array of values and then reassigns the values after all the threads have synced. If next_value(x, y) gives the value of the light at position (x,y) at the next step, the core device loop for evaluating and assigning a single step might look like: . // Take one step for lights __global__ void one_step() { // Each thread is responsible for a row const int row = threadIdx.x; // New values for each column in this row int newValues[100]; // Get next values for(int col = 0; col &lt; 100; ++col) { newValues[col] = next_value(row, col, 1); } // Sync to make sure not to overwrite values __syncthreads(); // Set new values for(int col = 0; col &lt; 100; ++col) { lights[row][col] = newValues[col]; } // Sync again before moving on to next step __syncthreads(); } . In the host code, I loop over the steps using this kernel: . // Loop over the steps and &quot;animate&quot; for (int step = 0; step &lt; 100; ++step) { one_step&lt;&lt;&lt;1,100&gt;&gt;&gt;(); } . From there, I basically reused the counting lights code I used on Day 6 to count the number of lights that are on after 100 steps and get the solution. . Part 2 . The wrinkle in Part 2 was that the corners remained “on” after every step, instead of following the neighbor rules. The only part of my code that is adjusted is the setting new values in one_step() to force the value of the lights to be 1. . Timing . I never wrote the corresponding Python code for Day 18 (or any future days), so there’s no comparison to make, but the CUDA code runs basically instanteneously. . Day 19: Medicine for Rudolph . Part 1 . For Part 1, the problem was, given a set of string replacements (e.g. “Al” -&gt; “ThF”) and a starting string, find the number of unique strings produced by applying a single replacement to the starting string (across all replacements and all locations for each replacement). . The first order was to modify the transformations and input string to be integer-based rather than string-based to make things more efficient. If you’re really curious, that’s in the Transforms.hh file on github. The main thing was to turn the replacement transformations into an array of integer arrays that can be indexed into (43 transforms in all). . The guise of the problem was that each component of the string represented a molecule or element that was being transformed to other molecules, so I will also use that vernacular. In particular, I made a C++ class called Molecule to help structure some things: . struct Molecule { // Represent string of elements (284 is longest possible molecule for this problem) int8_t molecule[285]; // Size of molecule size_t msize; // Number of steps taken to produce this molecule (used primarily in Part 2) size_t steps; // Produce an invalid molecule (host and device) __device__ __host__ Molecule() { msize = 285; steps = INT_MAX; } // Produce a valid molecule from a string of elements __device__ __host__ Molecule(int8_t* newm, size_t newSize, size_t newSteps); /** * Produces a new molecule by applying the transform indicated by `transformIdx` * to this molecule. `childNum` indicates how many replacements to skip first * before applying the replacement. An invalid molecule is produced if such a replacement * isn&#39;t possible. */ __device__ Molecule fabricate(int64_t transformIdx, int64_t childNum) const; // Is this a valid molecule? __device__ bool is_valid() const { return (steps != INT_MAX &amp;&amp; msize &lt; 285); } } // Equality comparison (m1 == m2 if molecules are equal) __device__ bool operator==(const Molecule&amp; m1, const Molecule&amp; m2); . With this struct set up, the first device kernel can calculate all of the possible molecules that can be produced from the input molecule: . // List of current molecules __device__ __managed__ Molecule* heap; // Current size of heap __device__ __managed__ size_t heapSize = 0; // Make the possible children of `start` and add them to `heap` __global__ void fabricate_molecules(Molecule* start) { for (int64_t transformIdx = threadIdx.x; transformIdx &lt; 43; transformIdx += blockDim.x) { int64_t childNum = 0; // Produce first child Molecule m = start[0].fabricate(transformIdx, childNum); // Continue we produce an invalid child or run out of space while(m.is_valid() &amp;&amp; heapSize &lt; MAX_HEAP_SIZE) { // Add to heap int64_t heapIdx = atomicAdd(&amp;heapSize, (size_t)1); heap[heapIdx] = m; ++childNum; // Produce next child m = start[0].fabricate(transformIdx, childNum); } } } . fabricate_molecules produces all of the possible children from applying a single transformation to the input molecule. The next step is to count the unique strings produced, as the fabrication can produce some duplicates. That’s mostly handled in the same way I’ve been counting things in the previous problems, using the reduction routine to thread the counting. The operator== defined for Molecule is used to check for duplicates while each thread is going through the heap. . I wanted to highlight the bit of code around heapIdx and atomicAdd since I thought it was particularly slick. (I didn’t come up with it.) The atomicAdd increments the heapSize counter and returns the previous value of heapSize. It allows you to insert a new object into the heap without worrying about other threads or blocks conflicting, because they get the next increment (assuming atomicAdd is called in the same way before their insert). . Part 2 . The problem for part 2 was to find the smallest number of steps required to produce the given input string using the transformations given (same ones discussed in part 1), starting from the string ‘e’ (representing a single electron in the molecule parlance). Each applied transformation counted as a step. . My approach was to start with the input string and repeatedly ‘reverse apply’ transformations until the ‘e’ was reached.17 The replacements are such that each reverse application will always reduce the size of the molecule. . For example, if the original replacement was Al =&gt; ThF, then one possible step would be to replace a single instance of ThF with Al. . Each such replacement can be consider as a branch of a tree with the original molecule as the root node. The first part of the problem then is to find a path through the tree, i.e. a series of reverse transformations, that terminates with ‘e’. Given that I’m working in the parallel world of CUDA, my plan was to find the shortest branch by exhausting the possible paths over some part of the tree and eliminating the remaining paths as solutions because they were as long or longer. . The procedure I used was as follows: . Apply reverse transformations to element(s) at the top of the heap and put the valid ones on the heap | Sort the heap, first by size, then by number of steps taken. | Continue 1-2 until ‘e’ is reached. | Process remaining paths until they reach ‘e’ or they are as long as the best path so far. | For the host code, this looks like . // Continue until done or memory gets full while (heapSize &gt; 0 &amp;&amp; heapSize &lt; MAX_HEAP_SIZE) { // Apply each reverse transformation to the top of the heap deconstruct_molecules&lt;&lt;&lt;1,THREADS&gt;&gt;&gt;(); cudaDeviceSynchronize(); // Sort the heap bitonic_sort(); // Recalculate the heap size after the sort reset_heap_size&lt;&lt;&lt;1,1&gt;&gt;&gt;(); cudaDeviceSynchronize(); } . A bitonic_sort was a new concept to me. It is a sorting algorithm that can be implemented in parallel code, so I did it: . // Single step of the bitonic_sort() __global__ void bitonic_sort_step(int64_t j, int64_t k) { // Each thread handles a different element int64_t i = threadIdx.x + blockDim.x * blockIdx.x; int64_t ij = i ^ j; if (ij &gt; i) { // Determines whether or not to swap indices int64_t ik = i&amp;k; if ( (ik == 0 &amp;&amp; heap[ij] &lt; heap[i]) || (ik != 0 &amp;&amp; heap[i] &lt; heap[ij]) ) { Molecule temp = heap[i]; heap[i] = heap[ij]; heap[ij] = temp; } } } // Parallel sort algorithm __host__ void bitonic_sort() { for (int64_t k = 2; k &lt;= MAX_HEAP_SIZE; k &lt;&lt;= 1) { for(int64_t j = k &gt;&gt; 1; j &gt; 0; j = j &gt;&gt; 1) { bitonic_sort_step&lt;&lt;&lt;BLOCKS,THREADS&gt;&gt;&gt;(j, k); } } cudaDeviceSynchronize(); } . You may notice that bitonic_sort_step() requires the comparison of two heap objects, in this case, Molecule objects, so we need a less than comparison operator: . __device__ bool operator&lt;(const Molecule&amp; m1, const Molecule&amp; m2) { if (m1.msize != m2.msize) { return m1.msize &lt; m2.msize; } return m1.steps &lt; m2.steps; } . This sort places the invalid molecules at the end of the heap and the shortest valid molecule at the top of the heap. . The next bit of device code is a way to the apply the reverse transformations to an individual Molecule: . // Convert a molecule to a smaller one via a particular reverse transformation __device__ Molecule Molecule::deconstruct(int64_t transformIdx) const; } . transformIdx again indicates the particular transformation to use, this time in reverse. As before, an invalid Molecule is produced if the transformation is not possible. . Using this deconstruct function to apply a reverse transformation to molecule, this is what the device code for deconstruct_molecules() looks like: . // Apply the reverse transformations to the top of the heap __global__ void deconstruct_molecules() { if (blockIdx.x &lt; heapSize) { // Each block grabs a different Molecule from the top of the heap Molecule start = heap[blockIdx.x]; // Each thread applies a different reverse transformation for(int64_t transformIdx = threadIdx.x; transformIdx &lt; 43; transformIdx += blockDim.x) { // Create the new molecule Molecule m = start.deconstruct(transformIdx); if (!m.is_valid()) { continue; } // Reached &#39;e&#39; if (m.msize == 1 &amp;&amp; m.molecule[0] == 16) { atomicMin(&amp;bestSteps, m.steps); } else if (m.steps &lt; bestSteps) { // Add new molecule to the end of the heap int64_t heapIdx = atomicAdd(&amp;heapSize, static_cast&lt;size_t&gt;(1)); heap[heapIdx] = m; } } // This Molecule has been processed, so make it invalid if (threadIdx.x == 0) { heap[blockIdx.x] = Molecule(); } } } . The code is now set up to run through the tree to find a series of transformations that reduces the input molecule to ‘e’…Or not. . Turns out, this doesn’t actually work. More accurately, it works to find a path but it’s not going to exhaust enough paths to finish in my lifetime. There are on the order of 19523 paths to check. :grimacing: . Some googles and a bit of reading later,18 I learned that the ‘true’ solution relies on the fact that particular structure of the input molecule guarantees that any path that terminates with ‘e’ with have the same length. For the general case, this guarantee is not valid. . Given that added guarantee, I know that if I find one path, it’s the best one. My workaround is to modify the while condition in the host code slightly: . while (heapSize &gt; 0 &amp;&amp; heapSize &lt; MAX_HEAP_SIZE) &amp;&amp; bestSteps == INT_MAX) {... . Now, when it finds a path, the loop ends and bestSteps is returned. (With the ‘correct’ answer.) . Timing . The solution to Part 1 runs almost instantly. The solution for Part 2 runs in less than half a second with the while short circuit above. Not quite instantly, but still satisfying. . Day 20: Infinite Elves and Infinite Houses . Part 1 . The premise for this problem is that Santa is having elves deliver presents door-to-door to each house, with each elf delivering ten times its assigned number to each multiple of its number. (Elf 1 delivers 10 presents to all houses; Elf 2 delivers 20 present to each even house; Elf 3 delivers 30 presents to each house number divisible by three, and so on.) The question is what is the smallest house number that gets above a certain given number of presents. . I came up with two approaches to this problem and ended up implementing them both to see which was faster. . The first approach was to go house by house, figure out which elves delivered there (i.e. the factors of the house number), and add up the presents for that house. The solution arrives at the first house that is above the threshold. . // Given number of input presents const uint32_t PUZZLE_INPUT = 36000000; // Maximum house number (guaranteed to be no bigger than this) const uint32_t MAX_HOUSES = PUZZLE_INPUT/10; // House stack __device__ __managed__ uint32_t* houses; // Deliver the presents house by house __global__ void deliver_by_house() { int32_t presents = 0; // Loop over the houses (starting at 1) for(uint32_t house = 1 + threadIdx.x + blockIdx.x * blockDim.x; house &lt; MAX_HOUSES; house += gridDim.x * blockDim.x) { // Loop over the possible elfs for this house for (uint32_t elf = 1; elf &lt;= house; ++elf) { // Does this elf deliver here (i.e. is elf a factor of house)? if ((house % elf) == 0) { presents = 10 * elf; if (houses[house] + presents &gt;= PUZZLE_INPUT) { atomicExch(&amp;houses[house], PUZZLE_INPUT); } else { atomicAdd(&amp;houses[house], presents); } } } } } . The only new part in here is atomicExch, which atomically sets the value. I use it here to set the house value to the puzzle input if the house value would exceed that. This speeds up the code a little as it prevent unnecessary adds once the value is exceeded.19 From here, it’s easy to loop over the houses and find the smallest house with value greater than or equal to PUZZLE_INPUT. . The second approach I came up with was to deliver the presents elf by elf. So, each elf delivers all its presents to all its houses before going to the next elf. . // Deliver the presents elf by elf __global__ void deliver_by_elf() { uint32_t presents = 0; // Loop over elves (starting at 1) for(uint32_t elf = 1 + threadIdx.x + blockIdx.x * blockDim.x; elf &lt; MAX_HOUSES; elf += gridDim.x * blockDim.x) { presents = 10 * elf; // Presents for this elf for (uint32_t j = elf; j &lt; MAX_HOUSES; j += elf) { if (houses[j] + presents &gt;= PUZZLE_INPUT) { atomicExch(&amp;houses[j], PUZZLE_INPUT); } else { atomicAdd(&amp;houses[j], presents); } } } } . Again, this uses atomicExch to set the value to prevent unneeded adds. . This approach counting elf-by-elf turned out to be much, much faster than the house-by-house approach. See more in the Timing section below. . Part 2 . Part 2 modified the problem by only allowing each elf to deliver to 50 houses, i.e. the first 50 multiples of its number, and increasing the number of presents it delivered to be 11 times its number. This doesn’t change much of the code, it adds the verification that, for each house value, house &lt;= 50 * elf before adding new presents. . Timing . For this problem, I decided to interrogate how the number of ‘blocks’ and ‘threads’ influenced how long it took to solve the problem (in wall time). The solution to both parts could be calculated simultaneously, so I ran some timing code for the two different approaches (house-by-house and elf-by-elf) that I used to solve the problem. . For the house-by-house approach, here’s what the timing looked like: . # of blocks # of threads Wall Time . 32 | 128 | 164 sec | . 64 | 128 | 87 sec | . 128 | 128 | 57 sec | . 256 | 128 | 54 sec | . 512 | 128 | 51 sec | . 1024 | 128 | 47 sec | . 2048 | 128 | 46 sec | . 32 | 256 | 93 sec | . 64 | 256 | 56 sec | . 128 | 256 | 53 sec | . 256 | 256 | 51 sec | . 512 | 256 | 48 sec | . 1024 | 256 | 47 sec | . 2048 | 256 | 46 sec | . Obviously, more blocks or more threads is faster, but the speed gains are limited after a point. . For the elf-by-elf approach, here’s the timing table: . # of blocks # of threads Wall Time . 32 | 128 | 0.91 sec | . 64 | 128 | 0.89 sec | . 128 | 128 | 0.90 sec | . 256 | 128 | 0.90 sec | . 512 | 128 | 0.90 sec | . 1024 | 128 | 0.90 sec | . 32 | 256 | 0.89 sec | . 64 | 256 | 0.90 sec | . 128 | 256 | 0.90 sec | . 256 | 256 | 0.90 sec | . 512 | 256 | 0.90 sec | . 1024 | 256 | 0.90 sec | . These timings are all essentially identical. I also tried some smaller numbers for this case: . # of blocks # of threads Wall Time . 64 | 32 | 0.92 sec | . 64 | 16 | 0.89 sec | . 64 | 8 | 0.89 sec | . 32 | 16 | 0.94 sec | . 16 | 16 | 0.96 sec | . 8 | 8 | 1.14 sec | . I don’t have anything groundbreaking to say about how many blocks or threads to use and the timing; I just wanted to see what it looked like if I changed them. . Day 21: RPG Simulator 20XX . Part 1 . The premise for this problem is that an RPG player finds a shop just before the boss fight. The shop sells various items to set armor and attack damage and the goal is to find the smallest cost the player can pay and still win the boss fight. (The player starting HP and the boss’ starting HP, damage, and armor are all fixed inputs.) . The rules also stipulate exactly one weapon (out of 5), 0-1 armor (out of 5), and 0-2 rings (out of 6), which gives 5*6*22 = 660 possible equipment combinations. These can be easily enumerated to assign each number &lt; 660 to a unique combination. Thus, the overall procedure, for each number will be: . Given number N, calculate the cost, armor, and attack damage from the corresponding equipment. | Determine who wins the fight if the player dons that equipment | If the player wins, compare the current cost to the best cost so far and set the best cost appropriately. | For the device code, I implemented this as . // Cost (746 = buy everything) __device__ __managed__ int bestCost = 746; // Find best cost __global__ void find_costs() { for(int N = threadIdx.x; N &lt; 660; N += blockDim.x) { int playerCost = 0; int playerDamage = 0; int playerArmor = 0; // Convert N to equipment calculate_equipment(N, playerCost, playerDamage, playerArmor); // Determine fight winner bool winner = win_fight(playerDamage, playerArmor); // Set best cost if (winner &amp;&amp; playerCost &lt; bestCost) { atomicMin(&amp;bestCost, playerCost); } } } . Part 2 . The modification for this part was to find the maximum the player could spend at the shop (with the same equipment limitations) and still lose the boss fight. The code change is just to swap the conditional for setting the best cost to set the worst cost instead: . // Cost (buy nothing) __device__ __managed__ int worstCost = 0; ... // Set worst cost if (!winner &amp;&amp; playerCost &gt; worstCost) { atomicMax(&amp;worstCost, playerCost); } ... . Timing . This is decidedly not a problem that requires a parallel solution. There are only 660 possible combinations to check, so a single thread runs almost instantly, just as the 660 thread solution does. C’est la vie. . Day 22: Wizard Simulator 20XX . Part 1 . The premise for this problem is similar to Day 21: there’s an RPG player in a boss fight. This time, however, the player is a wizard who has set of 5 spells that can be cast on each turn. Each spell costs a different amount of mana to use and can have effects like healing, damaging the boss, increasing armor, or increasing the amount of remaining mana. The problem is to find the least amount of mana that can be spent to win the fight. (Player and boss starting HP, boss damage, and player starting mana pool are all fixed inputs. Boss has zero armor.) . The vision I had for solving this problem was similar to ones I’ve used above: construct a mechanism for converting from an integer to a sequence of spells. In this case, there is straightforward way to do so. Each spell is assigned a number from 0-4. Given an integer, repeated division and modding by 5 of that integer will give a sequence of numbers 0-4. That sequence of integers corresponding directly to a sequence of spells. . I set up the player and boss as C++ classes to make it a little easier to track each round (see github for omitted details): . // Boss class class Boss { public: // HP (51 is given input) int hp = 51; // Damage (9 is given input) int damage = 9; // Constructor __device__ Boss() {}; // Boss attack player for damage __device__ void attack(Player&amp; player); // Check if the boss is dead (HP &lt;= 0) __device__ bool is_dead(); }; // Player class class Player { public: // Total mana spent so far int manaSpent = 0; // Mana pool (500 is given input) int mana = 500; // HP (50 is given input) int hp = 50; // Constructor __device__ Player() {}; // Magic missile spell (spell 0) __device__ bool magic_missile(Boss&amp; boss); // Drain spell (spell 1) __device__ bool drain(Boss&amp; boss); // Shield spell (spell 2) __device__ bool shield() // Poison spell (spell 3) __device__ bool poison() // Recharge spell (spell 4) __device__ bool recharge() // Check if there&#39;s enough mana to cast any spell __device__ bool can_cast(); // Check if player is dead __device__ bool is_dead(); }; . To execute one round (one player turn and one boss turn), then I implemented as . // Return true if round is valid (player can cast) __device__ bool round(int64_t&amp; spellList, Player&amp; player, Boss&amp; boss) { // Get next spell int spell = spellList % 5; // Take player turn (player spell, etc.) bool valid = player_turn(spell, player, boss); // Invalid if not enough mana to cast spell if (!valid) { return false; } // Take boss turn (boss attack, etc.) boss_turn(player, boss); // Discard last spell spellList /= 5; // Valid round return true; } . With those pieces set, I found the best mana by given each thread a different integer: . // Find the smallest mana to use to beat boss __global__ void find_best_mana(bool hardMode) { for(int64_t N = threadIdx.x + blockIdx.x * blockDim.x; N &lt; INT_MAX; N += blockDim.x * gridDim.x) { int64_t spellList = N; Player player; Boss boss; // Continue until boss is dead, player is dead, or too much mana is spent while(player.can_cast() &amp;&amp; !player.is_dead() &amp;&amp; !boss.is_dead() &amp;&amp; player.manaSpent &lt; bestMana) { // Run a single round (player turn + boss turn) if (!round(spellList, player, boss, hardMode)) { break; } } // If boss is dead, check against best mana if (boss.is_dead()) { atomicMin(&amp;bestMana, player.manaSpent); } } } . Part 2 . For Part 2, the change was that the player took an additional HP loss at the start of their turn (hard mode). This added about 3 lines of code to player_turn to account for this. . Timing . For the code as written and outlayed above, it takes less than one second each for both parts, but it isn’t quite instanteous. . Day 23: Opening the Turing Lock . This is decidedly not a parallel problem. As such, I’m skipping any discussion of it here, but you can check out the github repo for day23.cu, if you’re dying to see some CUDA code solving it. . As recompense, here’s a meme: . . And another: . . Day 24: It Hangs in the Balance . Part 1 . Under the guise of balancing the presents on sleigh, the problem for Day 24 was to divide a group of integers into 3 groups so that the sum of each of the groups was the same. Additionally, you needed to find the arrangement that had the group with the smallest cardinality and, if there were multiple such arrangements, the one with the smallest product within that set with the smallest cardinality. . There were 28 weights to put into the 3 groups. Thus, any group in an arrangement could be represented by some nonnegative integer less than 2**28 by considering the binary representation of the integer as a indicator of whether the weight is in the group. For example, . If N = 100663297 = 110000000000000000000000001, the group would have the first weight, the second-to-last weight, and the last weight in it (and nothing else). . Find the number of possible smallest groups (by looping over all non-negative integers &lt; 2**28): those groups that sum to 1/3 of the total weight and fewer than 10 elements. | Allocate a heap of integers of the size found in step 1. | Add each valid integers from step 1 to heap | Sort the heap (using bitonic_sort), first by cardinality of the corresponding group, then by the product of the elements of the group. | For each non-negative integers &lt; 2**28, get a group of weights (“second group”). If this second group sums to 1/3 the weight, test it against every “first group” from the heap. If they don’t overlap in any weights, this also gives a “third group” to give a valid arrangement. If so, test the cardinality and product of the “first group” against the best arrangement found so far. | Following these steps will give the desired arrangement. The device code for each step is laid out below for each step: . Find the number of groups that sum to 1/3 of the total weight and have fewer than 10 elements // Total sum of WEIGHTS const uint32_t TOTAL_WEIGHT = 1524; // 1/3 of TOTAL_WEIGHT const uint32_t GROUP_WEIGHT = TOTAL_WEIGHT / 3; // Heap of integers representing sets of weights __device__ __managed__ uint32_t* heap; // Size of heap __device__ __managed__ size_t heapSize = 0; // Current index of heap __device__ __managed__ size_t heapIdx = 0; // Part 1, Step 1 __global__ void find_first_groups() { uint32_t weights[28]; size_t cardinality = 0; uint32_t sumWeights = 0; for(uint32_t N = threadIdx.x + blockDim.x * blockIdx.x; N &lt; 268435456// 2**28 N += blockDim.x * gridDim.x) { // Convert integer to weights group cardinality = convert_weights(N, weights); // If cardinality &gt; 9, it&#39;s not the smallest group in arrangement if (cardinality &gt; 9) { continue; } // Weights must sum to 1/3 TOTAL WEIGHT sumWeights = sum(weights); if (sumWeights != GROUP_WEIGHT) { continue; } // Increment heap size to make room for this N in step 3 atomicAdd(&amp;heapSize, static_cast&lt;size_t&gt;(1)); } } . | No device code for this step: Allocate the first power of 2 larger than heapSize number of integers for heap (Power of 2 required for bitonic_sort in step 4). . | Repeat step 1, but add the integers to the heap that has now been allocated, so I replaced the atomicAdd above with // Add this integer to the heap size_t idx = atomicAdd(&amp;heapIdx, (size_t)1); heap[idx] = N; . This uses the same incrementing an index trick I used in Day 19 to allow each thread to store to the heap without clobbering writes by other threads. . | Sort the heap by considering the cardinalities and products of the weights each integer in the heap represents. The sorting algorithm is bitonic_sort, using basically the same code that I used on Day 19 as well. . | Lastly, I looped over the possible second group of weights and tested each first group that made a valid arrangement to find the first group with the lowest cardinality (and smallest product within the groups with the same cardinality.) // Smallest cardinality found for first group __device__ __managed__ uint32_t bestCardinality = UINT_MAX; // Smallest product found in smallest cardinality for first group __device__ __managed__ uint64_t bestQE = UINT64_MAX; // Part 1, Step 5 __global__ void test_valid_arrangements() { // Weight arrays uint32_t weights1[LENGTH]; uint32_t weights2[LENGTH]; // Cardinalities uint32_t cardinality1 = 0; uint32_t cardinality2 = 0; uint32_t cardinality3 = 0; // Sum of weights uint32_t sumWeights2 = 0; // Product of weights (quantum entanglement) uint64_t qe1 = 0; // Loop over integers &lt; 2**28 (second group) for(int64_t N2 = threadIdx.x + blockDim.x * blockIdx.x; N2 &lt; POSSIBLES; N2 += blockDim.x * gridDim.x) { // Get second group weights cardinality2 = convert_weights(N2, weights2); // Calculate sum of weights sumWeights2 = sum(weights2); // Check for invalid group if (sumWeights2 != GROUP_WEIGHT) { continue; } // Loop over all first group weights in heap for (uint64_t idx = 0; idx &lt; heapSize; ++idx) { uint32_t N1 = heap[idx]; // If N1 &amp; N2 is not zero, weights overlap if ((N1 &amp; N2) != 0) { continue; } // Get first group weights cardinality1 = convert_weights(N1, weights1); // Check if already bigger than bestCardinality or smaller than second group if (cardinality1 &gt; bestCardinality || cardinality2 &lt; cardinality1) { break; } // Cardinality of third group cardinality3 = LENGTH - (cardinality1 + cardinality2); // Check if it&#39;s smaller than first group if (cardinality3 &lt; cardinality1) { continue; } // Test against the best arrangement so far qe1 = product(weights1); if (cardinality1 == bestCardinality &amp;&amp; qe1 &gt;= bestQE) { break; } else { atomicMin(&amp;bestCardinality, cardinality1); atomicMin(&amp;bestQE, qe1); break; } } } } . This sets bestCardinality and bestQE to be the desired values. . | Part 2 . Part 2 had the same basic premise as Part 1, except now, instead of 3 equally summed groups, there were 4. . To tackle this problem, I used a similar approach as I did in Part 1 to start, however, it needed to be modified a bit after that to account for the extra group. Steps 1-4 from Part 1 were identical: . Find the number of possible smallest groups (by looping over all non-negative integers &lt; 2**28).Those groups that sum to 1/4 of the total weight and fewer than 10 elements. | Allocate a heap of integers of the size found in step 1. | Add each valid integer from step 1 to heap | Sort the heap (using bitonic_sort), first by cardinality of the corresponding group, then by the product of the elements of the group. | Next, I made a second heap that corresponded to all other groups that could co-exist with at least one group in the first heap. It plays very similar to Steps 1-3: . Find the number of possible other groups (by looping over all non-negative integers &lt; 2**28): those groups that sum to 1/4 of the total weight and have at least one non-overlapping group in the first heap whose cardinality is not bigger than its cardinality. | Allocate a second heap of integers of the size found in step 5. | Add each valid integer from step 5 to the second heap. | The last step in Part 2 is a bit different than the last step in Part 1: . For every possible pair of groups in the second heap (“second” and “third” groups), find the non-overlapping groups in the first heap (“first” groups). If none of the 3 groups overlap, this also gives a “fourth” group of the remaining unused weights to specify a valid arrangement. If so, test the cardinality and product of the “first” group against the best arrangement found so far. | In terms of device code, steps 1-4 are basically identical to Part 1. For the remaining steps: . Find the possible second groups that have at least one valid first group: // Heap of integers represeting sets of weights for other groups (Part 2 only) __device__ __managed__ uint32_t* otherHeap; // Size of otherHeap __device__ __managed__ size_t otherHeapSize = 0; // Part 2, Step 5 __global__ void find_other_groups(bool addToHeap) { // Weight arrays uint32_t weights1[LENGTH]; uint32_t weights2[LENGTH]; // Cardinalities uint32_t cardinality1 = 0; uint32_t cardinality2 = 0; // Sum of weights uint32_t sumWeights2 = 0; // Allocate some temp integers uint32_t N1 = 0; // Loop over integers &lt; 2**28 for second group for(uint32_t N2 = threadIdx.x + blockDim.x * blockIdx.x; N2 &lt; POSSIBLES; N2 += blockDim.x * gridDim.x) { cardinality2 = convert_weights(N2, weights2); sumWeights2 = sum(weights2); // Check if valid group if (sumWeights2 != GROUP_WEIGHT2) { continue; } // Check if there&#39;s at least one valid first group that goes with this second group bool validN2 = false; // Loop over all first group weights in heap for (uint64_t idx = 0; idx &lt; heapSize; ++idx) { N1 = heap[idx]; // If N1 &amp; N2 is not zero, weights overlap if ((N1 &amp; N2) != 0) { continue; } // Second group can&#39;t be smaller than first group cardinality1 = convert_weights(N1, weights1); if (cardinality2 &lt; cardinality1) { continue; } // Found a possible first group, so break validN2 = true; break; } // Possible valid N2 if (validN2) { // Increment otherHeap size to make room for this N2 in next pass atomicAdd(&amp;otherHeapSize, static_cast&lt;size_t&gt;(1)); } } } . | This is similar to step 2: no device code here, just allocate correct memory for otherHeap. . | Repeat step 5, but add the integers to the heap that has now been allocated, so I replaced the atomicAdd line above with // Add this integer to the heap size_t idx = atomicAdd(&amp;heapIdx, static_cast&lt;size_t&gt;(1)); otherHeap[idx] = N2; . (Same as was done in step 3.) . | Lastly, loop over the second heap twice and the first loop once to find valid arrangements and test the first groups to find the one with the lowest cardinality and product: // Part 2, Step 8 __global__ void test_valid_arrangements2() { // Weight arrays uint32_t weights1[LENGTH]; uint32_t weights2[LENGTH]; uint32_t weights3[LENGTH]; // Cardinalities uint32_t cardinality1 = 0; uint32_t cardinality2 = 0; uint32_t cardinality3 = 0; uint32_t cardinality4 = 0; // Product (quantum entanglement) uint64_t qe1 = 0; // Loop over otherHeap (third group) // Each block handles a different third group for(int64_t idx3 = blockIdx.x; idx3 &lt; otherHeapSize; idx3 += gridDim.x) { uint32_t N3 = otherHeap[idx3]; cardinality3 = convert_weights(N3, weights3); // Loop over otherHeap (second group) // Each thread handles a different second group for(int64_t idx2 = threadIdx.x; idx2 &lt; otherHeapSize; idx2 += blockDim.x) { uint32_t N2 = otherHeap[idx2]; // Check for overlap between groups if ((N3 &amp; N2) != 0) { continue; } // Cut the search space in half by only considering when cardinality2 &lt;= cardinality3 cardinality2 = convert_weights(N2, weights2); if (cardinality2 &gt; cardinality3) { continue; } // Loop over possible first groups for (int64_t idx1 = 0; idx1 &lt; heapSize; ++idx1) { N1 = heap[idx1]; cardinality1 = convert_weights(N1, weights1); // Can&#39;t do any better with the rest of the heap if (cardinality1 &gt; bestCardinality) { break; } // Group 1 needs to be the smallest if (cardinality1 &gt; cardinality2) { break; } // Group 1 needs to be the smallest cardinality4 = LENGTH - (cardinality1 + card23); if (cardinality1 &gt; cardinality4) { break; } // Test against the best arrangement so far qe1 = product(weights1); if (cardinality1 == bestCardinality &amp;&amp; qe1 &gt;= bestQE) { break; } else { atomicMin(&amp;bestCardinality, cardinality1); atomicMin(&amp;bestQE, qe1); break; } } } } } . This sets the correct bestCardinality and bestQE values for Part 2. . | Timing . Despite having several steps and being almost 600 lines of code for both parts for this problem, the code runs basically instantly. Both the heap sizes and the short circuiting of doing further checks have a lot to do with that. For Part 1, the heap size (i.e. the number of valid first groups) was 20679. For Part 2, the first heap had 11179 elements and the second heap (i.e. number of valid “other” groups) had 54966 elements. . Day 25: Let It Snow . Like Day 23, this is definitely not a parallel problem. I wrote the solution in 30 lines of C++ code, though, if you want to look at that. I didn’t even bother with pretending to write CUDA for this problem. . Conclusion . Thanks to anyone who actually read the entire way down here. I owe you a hearty handshake/hug once this pandemic is over. . No real conclusions here, just the smug satisfications of writing some parallel code that compiles :open_mouth: and works :boom: :boom: :boom:. . Footnotes . 1I came close to learning some CUDA before: I took an High Performance Computing class two years ago that covered programming in CUDA for a day or two. Unfortunately, my daughter ran a fever at daycare that day, so I missed it and couldn’t find the time to revisit it. Now, however, with the opportunity to learn it at my own pace, I had no excuses.↩ . 2 At first, because I wanted to get on the leaderboard. However, it became quickly clear that was never happening, but I kept up the facade of doing it as quickly as I could.↩ . 3 Each day is a 2-part problem, where the second part is revealed after correctly answering the first part↩ . 4 One of the cool things about Advent of Code is that each person gets a different batch of input data.↩ . 5 This seems to be the “hello world” equivalent for CUDA. In addition to being the example that NVIDIA uses, it’s the first meaningful example in the book I mentioned above,and pretty much all of the video examples if you search for “Introduction to CUDA”. ↩ . 6 Opinions are like my friends, they’re all assholes. :fire: That’s how the saying goes, right?↩ . 7 Ternary statements are hot garbage, in case you’re wondering.↩ . 8 If you look at the github code, the paper and ribbons calculations are done in the same loop. ↩ . 9 I don’t recommend that route either. I eventually got it to work, but debugging it required compiling and linking another C++ version and lots of print statements to figure out where I had put a + instead of a -. ↩ . 10 Perhaps more to the point, itoa() is not in the ANSI-C standard and not supported by nvcc. ↩ . 11 Those values are mostly arbitrary: I used a non-square grid to make sure that I was using the grid correctly, but kept it 2D to avoid unneeded complication. Cue obligatory Khan from Star Trek 2 joke. ↩ . 12 Definitely not the first or second or third approach, though.↩ . 13 Of course, there are 8! = 40320 routes that visit each location exactly once. There is an algorithm to efficiently enumerate those routes, but I don’t know it off-hand, so I took a different tack.↩ . 14 I’ll spare you the function definitions here since they’re not particular interesting, but see the github repo if you’re compelled to see them.↩ . 15 int will not suffice, int64_t is needed.↩ . 16 This seem to work accidentally on my machine for Part 1, but breaks gloriously for Part 2, giving nondeterminstic answers.↩ . 17 I actually tried the forward way, too, but with less success.↩ . 18 I probably should have done this earlier than I did, but I really wanted my code to work as is. :flushed: ↩ . 19 Strictly speaking, the atomics probably aren’t needed, but they don’t slow down the code enough to notice and feel safer in preventing overwrites. ↩ .",
            "url": "https://bobowedge.github.io/adventures-in-telework/markdown/2021/04/13/advent_of-cuda.html",
            "relUrl": "/markdown/2021/04/13/advent_of-cuda.html",
            "date": " • Apr 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Chat Generation 2: Electric Bugaloo",
            "content": "Introduction . In my first real post, I tried my hand at generating fake chat logs using PyTorch. The corpus I used was a private Google Hangouts chatroom that I&#39;m in and my idea was ostensibly to generate something that resembled the message in that room. I adapted one of the PyTorch examples to attempt to do so. I was able to generate a model, but, as you can see from the results at the bottom of the post, the result wasn&#39;t all that great. . Today, I&#39;m going to try again, now armed with the last lesson in the fastai course, which covers natural language processing (NLP). . from fastai.text.all import * . . Data Prep . . Warning: Some of the chat content may contain profanity or stupidity. . The previous post on chat generation did most of the leg work in terms of prepping the data to interact nicely with the fastai interface, but I did a couple of more things to make training and generation easier. . First, I added in a couple of special marker words to indicate the speaker (xxsender) and the start (xxsom) and the end (xxeom) of messages. Then, I combined them into a single string for each message: . # Special marker words mark1 = &quot;xxsender&quot; mark2 = &quot;xxsom&quot; mark3 = &quot;xxeom&quot; # Read in the data and create chat_df = pd.read_csv(&quot;chat_file.csv&quot;) chat_df[&#39;Text&#39;] = f&quot;{dlm1} &quot; + chat_df[&#39;Sender&#39;] + f&quot; {dlm2} &quot; + chat_df[&quot;Message&quot;] + f&quot; {dlm3}&quot; chat_df.head() . Sender Message Text . 0 Kappa | Rsvp allo | xxsender Kappa xxsom Rsvp allo xxeom | . 1 Omega | Is it dead, did you get my last AliExpress find? | xxsender Omega xxsom Is it dead, did you get my last AliExpress find? xxeom | . 2 Kappa | I didn&#39;t see it and it wouldn&#39;t let me send anything | xxsender Kappa xxsom I didn&#39;t see it and it wouldn&#39;t let me send anything xxeom | . 3 Omega | &lt;MEME&gt; | xxsender Omega xxsom &lt;MEME&gt; xxeom | . 4 Kappa | I need this | xxsender Kappa xxsom I need this xxeom | . From there, I needed to break up the corpus into training and validation sets. I made each training and validation file to be a chunk of 100 consecutive messages concatenated together. Hopefully, this will preserve some of the inter-message dynamics, but keep each file reasonably sized. (The number 100 is quite arbitrary.) . # Somewhat arbitrary parameters msgs_per_chunk = 100 train_percent = 0.8 valid_percent = 1.0 - train_percent total_msgs = len(chat_df) train_msgs = int(total_msgs * train_percent) valid_msgs = total - train_msgs print(total_msgs, train_msgs, valid_msgs) # Chunk messages into separate files for i in range(0, total_msgs, msgs_per_chunk): subset = &quot; &quot;.join(chat_df[&#39;Text&#39;][i:i+msgs_per_chunk]) if i &lt; train_msgs: txt_file = f&quot;data/train/{i}.txt&quot; else: txt_file = f&quot;data/valid/{i}.txt&quot; with open(txt_file, mode=&#39;w&#39;) as f: f.write(subset) . 29000 23200 5800 . Training a model . With the data prep done, the next step is to train a language model. First, I created a DataLoaders object based on the training and validation sets to batch to give it to the language model learner . path = &quot;data&quot; get_data = partial(get_text_files, folders=[&#39;train&#39;, &#39;valid&#39;]) dls_lm = DataBlock( blocks = TextBlock.from_df(path, is_lm=True), get_items=get_data, splitter=RandomSplitter(0.1)).dataloaders(path, path=path, bs=64, seq_len=80) . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos xxsender xxmaj gamma xxsom no one is in chat today and it &#39;s terrible xxeom xxsender xxmaj kappa xxsom xxup sad xxeom xxsender xxmaj gamma xxsom so xxeom xxsender xxmaj kappa xxsom xxmaj why even have rules of we do nt follow them xxeom xxsender xxmaj gamma xxsom better idea for the end of last night &#39;s episode xxeom xxsender xxmaj kappa xxsom xxmaj this country is a dump xxeom xxsender xxmaj gamma xxsom jon tells dany who he | xxsender xxmaj gamma xxsom no one is in chat today and it &#39;s terrible xxeom xxsender xxmaj kappa xxsom xxup sad xxeom xxsender xxmaj gamma xxsom so xxeom xxsender xxmaj kappa xxsom xxmaj why even have rules of we do nt follow them xxeom xxsender xxmaj gamma xxsom better idea for the end of last night &#39;s episode xxeom xxsender xxmaj kappa xxsom xxmaj this country is a dump xxeom xxsender xxmaj gamma xxsom jon tells dany who he really | . 1 time until the election voting against their other xxunk until they get to him xxeom xxsender xxmaj beta xxsom xxmaj it seems that it &#39;s convention for xxmaj senators to xxunk from voting in conflict of interest cases , not law or even xxmaj senate rules . xxmaj so , xxmaj cruz could vote for himself . xxmaj he does n&#39;t have to resign as senator until he &#39;s actually xxunk xxeom xxsender xxmaj kappa xxsom &lt; meme &gt; xxeom | until the election voting against their other xxunk until they get to him xxeom xxsender xxmaj beta xxsom xxmaj it seems that it &#39;s convention for xxmaj senators to xxunk from voting in conflict of interest cases , not law or even xxmaj senate rules . xxmaj so , xxmaj cruz could vote for himself . xxmaj he does n&#39;t have to resign as senator until he &#39;s actually xxunk xxeom xxsender xxmaj kappa xxsom &lt; meme &gt; xxeom xxsender | . Then, I fed that object into the language model learner to create a language model to train. As with other fastai models, this sets up a pretrained model that can be fine tuned to based on the particular context. . learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . I used the learning rate finder to pick an apporpriate learning rate: . learn.lr_find() . SuggestedLRs(lr_min=0.13182567358016967, lr_steep=0.0831763744354248) . Now, I fit the model, using that learning rate and 10 epochs: . epochs = 10 lr = 1e-1 learn.fit_one_cycle(epochs, lr) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.602012 | 3.350408 | 0.388268 | 28.514360 | 00:58 | . 1 | 3.726574 | 3.723708 | 0.366895 | 41.417671 | 00:58 | . 2 | 3.399452 | 3.705886 | 0.370592 | 40.686092 | 00:57 | . 3 | 3.176552 | 3.566556 | 0.382298 | 35.394474 | 00:57 | . 4 | 2.986681 | 3.432734 | 0.391250 | 30.961184 | 00:58 | . 5 | 2.807675 | 3.359524 | 0.406797 | 28.775505 | 00:57 | . 6 | 2.646611 | 3.322854 | 0.415332 | 27.739414 | 00:59 | . 7 | 2.482075 | 3.298300 | 0.421074 | 27.066593 | 00:57 | . 8 | 2.355381 | 3.294302 | 0.422982 | 26.958590 | 00:58 | . 9 | 2.265341 | 3.295616 | 0.423320 | 26.994041 | 00:58 | . Again, 10 epochs is arbitrary, merely based on the fact that epoch took about a minute and 10 minutes isn&#39;t an eternity to wait. . Output Cleanup . Before I generated text, I need a routine to clean the output to make it look reasonable. In particular, I had added those extra xx markers that needed to be removed. Additionally, there were some modifications that the default tokenizer for fastai uses (e.g. It&#39;s to It &#39;s) that needed to be undone. One thing I couldn&#39;t find in the fastai library was something to completely undo that tokenization. . For those reasons, the collapse below has the output_cleanup function that I wrote to make the generated text prettier. It&#39;s not very interesting, but I included it for completeness . Chat Generation . With the model tuned, I have everything I need to generate some chat. The main function for doing so is learn.predict, which generates some words based on some seed text. Here&#39;s the generation function that I came up with: . # Generate some chat def generate(seed, num_words): # If seed is just a sender, add the sender marker if seed in [&quot;Kappa&quot;, &quot;Gamma&quot;, &quot;Psi&quot;, &quot;Omega&quot;, &quot;Beta&quot;, &quot;Sigma&quot;]: seed = &quot;xxsender &quot; + seed # Predict slightly past the number of requested words x = learn.predict(seed, num_words + 20, temperature=0.75) # Clean the output x = output_cleanup(x) # Trim back to the last complete message x = x.splitlines() x = &quot; n&quot;.join(x[:-1]) return x . print(generate(&quot;&quot;, 200)) . Gamma :: it iirc made MIL as ron burgundy Gamma :: well, they all don&#39;t have their own non white children Omega :: They&#39;ve been there since last year Beta :: If they do nt have a non - emergency situation, that&#39;s gold Kappa :: It&#39;s a silent auction Kappa :: It&#39;s a shame that that&#39;s not the origin for this Kappa :: Does that matter? Kappa :: is this it? Kappa :: I&#39;m beginning to doubt her dad as a conservative but that doesn&#39;t mean it is a good one Gamma :: i think it&#39;s not a nut roll Omega :: i mean the maybe i assume that&#39;s the third time i was being a retard Kappa :: it&#39;s like a fighting chance to remember it Gamma :: It&#39;s really good Kappa :: I&#39;m out of my house . It&#39;s pretty easy to see that this generated chat is better than the previous text generation that I did. At a minimum, each message is much more likely to be coherent as English. There is also some continuity between messages, where there was none before. . For fun, I also generated some text seeded based on who the first sender was . for sender in [&quot;Kappa&quot;, &quot;Gamma&quot;, &quot;Psi&quot;, &quot;Omega&quot;, &quot;Beta&quot;, &quot;Sigma&quot;]: y = generate(sender, 100) print(y) print(&quot;--&quot;) . Kappa :: But i can not keep forgetting Gamma :: i guess it&#39;s not illegal if you don&#39;t think we&#39;re gonna go to the gov&#39;t Kappa :: The pediatrician said it was never before Kappa :: There&#39;s a spider in the CO Kappa :: just the fuck Kappa :: Fucking retarded Kappa :: Maybe i&#39;m also retarded Gamma :: It&#39;s a retarded demon Kappa :: i was retarded Kappa :: That s a good word -- . Gamma :: Too accurate Kappa :: This is one of the most advanced things i&#39;ve ever seen Kappa :: how are you going to take the call? Gamma :: Fuck this country Kappa :: Fuck this country Kappa :: My question is how to get on Kappa :: The question from the south is why someone said there&#39;s a higher school Kappa :: Oh and i mean, people in the south are going to wait for the Canadian man who lives there -- . Psi :: JFC Psi :: &lt;MEME&gt; Kappa :: Is this a fun time, don&#39;t have a job? Gamma :: The fuck Kappa :: Lol Kappa :: i hate this world Gamma :: Boris might always have COVID Kappa :: I&#39;m not going to buy it Kappa :: i&#39;m like being real and retarded Kappa :: i assume that&#39;s some good social commentary Kappa :: and then it&#39;s called someone&#39;s -- . Omega :: So i can get a pass to bring myself to a free crab event today Omega :: Yeah i have a fever, so, just because of a fever, i need the best for my life Kappa :: It&#39;s like a normal person Omega :: There&#39;s a lot of thoughts on what you&#39;ve done Omega :: Ok Omega :: Maybe they&#39;re a family Omega :: Or the other way -- . Beta :: Damn it, it&#39;s so easy to do Omega :: Everyone is like a lot of people playing at the prison Omega :: That&#39;s you really about putting no Clown on the table Psi :: Also, he&#39;s the end of America :: i think it&#39;s the narrative&#34; Kappa :: That s a good cartoon for &#34;hot people&#34; he&#39;s &#34;to be&#34; so very wise of point Kappa :: Good spin on this web comic -- . Sigma :: i have no clue what the assignment is Omega :: The ravens are not playing their game Gamma :: They are going to vote for him, and nothing else will be shady Omega :: He needs to be dead Gamma :: And he&#39;s not subtle Kappa :: Is the game reopened? Psi :: Lighting beacons Omega :: Yes it is Omega :: But the beacons are lit Kappa :: Beacon lit -- . Takeaway . My primary takeaway is that this version of text generation is better in multiple ways. First, it generates objectively more reasonable text. It&#39;s not perfect and still identifiable as machine-generated, but it is definitely better than my previous attempt. Second, the entire process was much easier and there&#39;s a lot less code to write. With my first attempt with PyTorch, training a model took a lot of work to adapt their example to my data. For this attempt, it&#39;s less than 20 lines of actual code (ignoring the output cleanup). .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2021/02/02/chat_bugaloo.html",
            "relUrl": "/jupyter/2021/02/02/chat_bugaloo.html",
            "date": " • Feb 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Kaggling into an Iceberg",
            "content": "Introduction . The next lesson in the fastai course covered tabular data. According to the course, the vast majority of datasets on tabular data are best modeled by ensembles of decision trees, such as random forests, so the lesson focused on them. . They used a particular Kaggle competition, the Blue Book for Bulldozers as their hands-on example for walking through how to understand, build, train, and infer using decision trees and random forests. . While I had heard of Kaggle previously, I had never used it myself before. Fortunately, Kaggle itself points you to the Titanic competition as a starting point. The premise is that you&#39;re given a dataset of passengers on the Titanic and whether or not they survived to train on and you have to predict which passengers in another dataset of passengers survived. . As my introduction to both Kaggle and random forests, I thought I would try to walk through the course&#39;s random forest discussion using the Titanic competition data and see what happens. . (In the collapse below is all the imports and pandas settings I&#39;ll end up using at the top just to keep them all together.) . from fastai.tabular.all import * from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import zero_one_loss # Pandas display options pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . . Setup and Random Guess . The competition provides three files: . training set (passenger data with &#39;Survived&#39; column) | test set (passenger data with no &#39;Survived&#39; column) | sample submission assuming all and only female passengers survived | . Since I&#39;ll use it often, I&#39;ll set up the dependent variable (&#39;Survived&#39;) here as well. . dep_var = &#39;Survived&#39; train_df = pd.read_csv(&quot;titanic_train.csv&quot;, low_memory=False) test_df = pd.read_csv(&quot;titanic_test.csv&quot;, low_memory=False) . Here&#39;s what the training data looks like: . train_df . PassengerId Survived Pclass Name ... Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | ... | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | ... | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | ... | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | ... | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | ... | 373450 | 8.0500 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | ... | 211536 | 13.0000 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | ... | 112053 | 30.0000 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | ... | W./C. 6607 | 23.4500 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | ... | 111369 | 30.0000 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | ... | 370376 | 7.7500 | NaN | Q | . 891 rows × 12 columns . The competition page has a description of each of the columns, so I won&#39;t rehash those here. . As a first pass to give something to compare against, I built a randomly chosen set where the survival was determined at random based on the survival rate of the training data. The score for this random set ended up being 52.39%. . # Submit random set total_died, total_survived = train_df.value_counts(dep_var) ratio = total_died / (total_survived + total_died) test_length = len(test_df) random_survived = np.random.choice(2, test_length, p=[ratio, 1 - ratio]) random_data_frame = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : random_survived}) random_data_frame.to_csv(&quot;titanic_random.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_random.csv -m &quot;Random based on survival rate&quot; . Score: 0.52392 . Decision Trees . Now, it&#39;s time to do something other than just guess. I started by splitting the categorical and continuous columns using a fastai designed specifically for that. Then, I split the provided training data into training and validation sets using TabularPandas. . # Split the columns into categorical and continuous cont_cols, cat_cols = cont_cat_split(train_df, 1, dep_var=dep_var) print(f&quot;Continuous columns: {cont_cols}&quot;) print(f&quot;Categorical columns: {cat_cols}&quot;) . Continuous columns: [&#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;] Categorical columns: [&#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;] . # Operations for columns column_ops = [Categorify, FillMissing] # Split the training data randomly splits = RandomSplitter()(range_of(train_df)) tab_panda = TabularPandas(train_df, column_ops ,cat_cols, cont_cols, y_names=dep_var, splits=splits) trainxs, trainy = tab_panda.train.xs, tab_panda.train.y validxs, validy = tab_panda.valid.xs, tab_panda.valid.y len(trainxs), len(validxs) . (713, 178) . tab_panda.show(3) . Name Sex Ticket Cabin Embarked Age_na PassengerId Pclass Age SibSp Parch Fare Survived . 420 Gheorgheff, Mr. Stanio | male | 349254 | #na# | C | True | 421 | 3 | 28.0 | 0 | 0 | 7.8958 | 0 | . 392 Gustafsson, Mr. Johan Birger | male | 3101277 | #na# | S | False | 393 | 3 | 28.0 | 2 | 0 | 7.9250 | 0 | . 238 Pengelly, Mr. Frederick William | male | 28665 | #na# | S | False | 239 | 2 | 19.0 | 0 | 0 | 10.5000 | 0 | . The lesson used a DecisionTreeRegressor, but since &#39;Survived&#39; is a categorical and not continuous variable, I used DecisionTreeClassifier instead. For the same reason, zero_one_loss makes more sense to be the loss function rather than using the mean square error. . # Simple decision tree classifier model = DecisionTreeClassifier() model.fit(trainxs, trainy) print(f&quot;Error: {zero_one_loss(model.predict(validxs), validy)}&quot;) . Error: 0.2415730337078652 . Essentially, this is saying the model got ~24% of the validation passenger set wrong when predicting their survival. . In the default, the model can split a node as long as there is at least 1 sample for each leaf. Following the lesson, I checked to see how many leaves for my model were created vs. how many rows in the training data . model.get_n_leaves(), len(trainxs) . (117, 713) . The Titanic dataset is much, much smaller than the Blue Book of Bulldozers dataset, but this number of leaves seemed reasonable to me (6 per leaf or so). However, because it was easy to do so, I decided to iterate the min_samples_leaf parameter to try to get a better model: . # min_samples_leaf optimization best_error, best_min_samples, best_model = (None, None, None) for min_samples in range(1, 21): model = DecisionTreeClassifier(min_samples_leaf=min_samples) model.fit(trainxs, trainy) error = zero_one_loss(model.predict(validxs), validy) if best_error is None or best_error &gt; error: best_error, best_min_samples, best_model = (error, min_samples, model) print(f&quot;Best model error: {best_error}&quot;) print(f&quot;Min samples per leaf: {best_min_samples}, Number leaves: {best_model.get_n_leaves()}&quot;) . Best model error: 0.1966292134831461 Min samples per leaf: 3, Number leaves: 70 . The default minimum samples per leaf is 1 for DecisionTreeClassifier, but it seems the optimization helped. A minimum of 3 samples per leaf yields a better result on the validation, so I&#39;ll use that model to submit to kaggle. . Data Cleanup . Before I could predict using that model, there is an issue with the test data that had to be addressed. The &#39;Fare&#39; column had a null value for one of the test passengers, while neither the test or valid set had any null values in that column. . A standard way to fill in that value is to use the mean of the training set, so that&#39;s what I did. Pandas makes this substitution pretty easy, once you figure out the proper syntax. . # Find the null row(s) in the &#39;Fare&#39; column nan_row = test_df[test_df[&#39;Fare&#39;].isnull()] # Set the fare in that row to the mean of the training data test_df.at[nan_row.index, &#39;Fare&#39;] = train_df[&#39;Fare&#39;].mean() # Display the newly fixed row display(test_df.loc[nan_row.index]) . PassengerId Pclass Name Sex ... Ticket Fare Cabin Embarked . 152 1044 | 3 | Storey, Mr. Thomas | male | ... | 3701 | 32.204208 | NaN | S | . 1 rows × 11 columns . With the row fixed, I converted the test data to a TabularPandas. We&#39;ll use this again and again to feed into our models for survival prediction. . # Convert test data to TabularPandas test_tab = TabularPandas(test_df, [Categorify, FillMissing], cat_cols, cont_cols) . Decision Tree Results . With the test data cleaned up, I used the best decision tree model that I found to predict the survivors on the test set and submit the results to kaggle: . # Decision Tree Classifier set (Score: 0.70813) # Predict the survivors dt_survived = best_model.predict(test_tab.xs) test_length = len(test_df) dt_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : dt_survived}) dt_df.to_csv(&quot;titanic_dt.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_dt.csv -m &quot;Single decision tree&quot; . Score: 0.70813 . Random Forest . If one decision tree is good, how about more? Next, I&#39;ll walk through the random forests of decision trees that create to tackle this competition. . Following the course approach, I made a function to make creating a random forest and fit it in single step. Similar to above, since I&#39;m looking for a yes or no answer, instead of a continuous one, I&#39;ll use RandomForestClassifier instead of a regression model. . # create random forest and fit it def create_rf(xs, y, n_estimators=40, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . With that function, I can create a random forest, fit it to the training data, and score it against the validation data: . # First pass model with error # Model error: 0.1797752808988764 # Model oob error: 0.18513323983169705 model = create_rf(trainxs, trainy) error = zero_one_loss(model.predict(validxs), validy) print(f&quot;Model error: {error}&quot;) print(f&quot;Model oob error: {1.0 - model.oob_score_}&quot;) . Model error: 0.1797752808988764 Model oob error: 0.18513323983169705 . If 40 trees (n_estimators) is good, how about more? . Let&#39;s look at models with more trees (n_estimators) and see how they do. . # Number of trees (n_estimators) optimization best_error, best_trees, best_model = (None, None, None) for num_trees in range(40, 300, 20): model = create_rf(trainxs, trainy, n_estimators=num_trees) error = zero_one_loss(model.predict(validxs), validy) if best_error is None or best_error &gt; error: best_error, best_trees, best_model = (error, num_trees, model) print(f&quot;Num trees: {num_trees}, error: {error}, oob: {1.0 - model.oob_score_}&quot;) print(f&quot;Best model error: {best_error}, num trees: {best_trees}&quot;) . Num trees: 40, error: 0.1797752808988764, oob: 0.18513323983169705 Num trees: 60, error: 0.1685393258426966, oob: 0.18793828892005615 Num trees: 80, error: 0.1741573033707865, oob: 0.1893408134642356 Num trees: 100, error: 0.1797752808988764, oob: 0.1837307152875175 Num trees: 120, error: 0.1629213483146067, oob: 0.18793828892005615 Num trees: 140, error: 0.1685393258426966, oob: 0.1837307152875175 Num trees: 160, error: 0.1685393258426966, oob: 0.1865357643758766 Num trees: 180, error: 0.1685393258426966, oob: 0.1865357643758766 Num trees: 200, error: 0.1629213483146067, oob: 0.1893408134642356 Num trees: 220, error: 0.1629213483146067, oob: 0.18513323983169705 Num trees: 240, error: 0.1685393258426966, oob: 0.1837307152875175 Num trees: 260, error: 0.1685393258426966, oob: 0.18232819074333806 Num trees: 280, error: 0.1741573033707865, oob: 0.1781206171107994 Best model error: 0.1629213483146067, num trees: 120 . The model with 120 (or 200 or 220) trees seems to fare best, but all that much better than the model with 40 trees. I&#39;m no expert, I think it is due to the out-of-bag error. For all of the models, the oob error is more than the validation error. That would seem to indicate that the model can&#39;t really improve with more adding more trees, but could just be better by random chance. . That said, since they are easy to create and the submission rules for the Titanic competition are generous, I submitted the predictions from the 40-tree, 120-tree, 140-tree, 160-tree, and 220-tree models just to see how they would do. . # A few models with different numbers of trees for num_trees in [40, 120, 140, 160, 220]: model = create_rf(trainxs, trainy, n_estimators=num_trees) rf_survived = model.predict(test_tab.xs) rf_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : rf_survived}) rf_df.to_csv(f&quot;titanic_rf_{num_trees}.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_rf_40.csv -m &quot;Random forest classifier with 40 trees&quot; . 40-tree model score: 0.75598 . !kaggle competitions submit -c titanic -f titanic_rf_120.csv -m &quot;Random forest classifier with 120 trees&quot; . 120-tree model score: 0.76794 . !kaggle competitions submit -c titanic -f titanic_rf_140.csv -m &quot;Random forest classifier with 140 trees&quot; . 140-tree model score: 0.76076 . !kaggle competitions submit -c titanic -f titanic_rf_160.csv -m &quot;Random forest classifier with 160 trees&quot; . 160-tree model score: 0.76555 . !kaggle competitions submit -c titanic -f titanic_rf_220.csv -m &quot;Random forest classifier with 220 trees&quot; . 220-tree model score: 0.77272 . Although the 220-tree model ended up doing the best, increasing the number of trees didn&#39;t significantly improve the results. (For reference, the percentage difference between the 40-tree and 220-tree model corresponds to 8 people.) I&#39;m not convinced it&#39;s anything other than random chance. . That said, how about even more trees? 3000-tree model inbound: . model3000 = create_rf(trainxs, trainy, n_estimators=3000) rf_survived = model3000.predict(test_tab.xs) rf_rf = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : rf_survived}) rf_rf.to_csv(f&quot;titanic_rf_3000.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_rf_300.csv -m &quot;Random forest classifier with 3000 trees&quot; . 3000-tree model score: 0.76555 . And, it&#39;s worse than 220-tree model. I think this supports my earlier assertion that some of the models with more trees are better than the 40 tree model by random chance, rather than anything learned within the models. . Aside: Simpler models . At this point, before getting into some feature engineering, I went back and read on Kaggle a little bit more about the Titanic competition. As I mentioned above, one of the provided files was a sample submission file where the survivors were assumed to be only and all the female passengers (as determined by the &#39;Sex&#39; column). How does that submission score? . !kaggle competitions submit -c titanic -f gender_submssion.csv -m &quot;Only and all female passengers survive&quot; . Female only model score: 0.76555 . This simple model achieved the same score as the 3000-tree model! Let&#39;s take a look at the importance of each feature, according to that 3000-tree model. (Again, this is adapted directly from the course.) . def feature_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_}).sort_values(&#39;imp&#39;, ascending=True) fi = feature_importance(model3000, trainxs) fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . From the bar chart, it is abundantly clear that &#39;Sex&#39; is the most telling feature, with everything else trailing. That said, the 3000-tree model and female-only model actual differ in 62 (of 418) predictions, so they are finding slightly different spaces with the same score. . Because I haven&#39;t been able to get this song (no, not that one) out of my head, I also tried &quot;the women and children fled to the lifeboats put to sea&quot; model. That is, all the women and children survived (but no one else). . # Women and children survival model wac_test_df = pd.DataFrame(test_df) wac_test_df[&#39;Survived&#39;] = np.ones(len(wac_test_df), dtype=int) wac_cond = (wac_test_df.Sex == &#39;female&#39;) | (wac_test_df.Age &lt; 18) wac_test_df[&#39;Survived&#39;].where(wac_cond, 0, inplace=True) wac_submit = wac_test_df.loc[:,[&#39;PassengerId&#39;,&#39;Survived&#39;]] wac_submit.to_csv(&quot;titanic_wac.csv&quot;, index=False) . #!kaggle competitions submit -c titanic -f titanic_wac.csv -m &quot;Women and children fled to the lifeboats put to sea&quot; . Women and children model score: 0.74641 . This model scores worse, but not dramatically so. . Feature Engineering . The next step was to improve the random forest models above by extracting, modifying, or deleting some of the given columns to create new, hopefully more relevant ones by feature engineering. . I tried lots of different modifications that I came up with on my own, but, in actuality, I couldn&#39;t come up with anything that definitely improved on the previous random forests. (The best score I ended up getting was 0.77751.) . Since I couldn&#39;t seem to improve, I ended up reading a number of the discussion posts on Kaggle, particularly those that dealt with random forests and feature engineering. My favorite was this one, so I decided to adapt it to the syntax I was using. The function I came up with is in the collapse below: . def edit_features(df): mod_df = df.copy() # PassengerId - not meaningful for learning del mod_df[&quot;PassengerId&quot;] # Pclass - treat passenger class as category mod_df[&quot;Pclass&quot;] = mod_df[&quot;Pclass&quot;].astype(&quot;category&quot;) # Name - Split into name length and title mod_df[&#39;NameLength&#39;] = mod_df[&quot;Name&quot;].apply(lambda x: len(x)) mod_df[&#39;NameTitle&#39;] = mod_df[&quot;Name&quot;].apply(lambda x: x.split(&#39;,&#39;)[1]).apply(lambda x: x.split()[0]) mod_df[&#39;NameTitle&#39;] = mod_df[&quot;NameTitle&quot;].astype(&quot;category&quot;) del mod_df[&quot;Name&quot;] # Age - fill Age with mean grouped by title and class age_data = mod_df.groupby([&#39;NameTitle&#39;, &#39;Pclass&#39;])[&#39;Age&#39;] mod_df[&#39;Age_na&#39;] = mod_df[&#39;Age&#39;].apply(lambda x: 1 if pd.isnull(x) else 0) mod_df[&#39;Age_na&#39;] = mod_df[&#39;Age_na&#39;].astype(&quot;category&quot;) mod_df[&#39;Age&#39;] = age_data.transform(lambda x: x.fillna(x.mean())) # SibSp + Parch =&gt; Family size category passengers = mod_df[&quot;SibSp&quot;] + mod_df[&quot;Parch&quot;] mod_df[&#39;FamilySize&#39;] = np.where(passengers == 0, &#39;Solo&#39;, np.where(passengers &lt;= 3, &#39;Nuclear&#39;, &#39;Big&#39;)) mod_df[&quot;FamilySize&quot;] = mod_df[&quot;FamilySize&quot;].astype(&quot;category&quot;) del mod_df[&#39;SibSp&#39;] del mod_df[&#39;Parch&#39;] # Ticket - Split into two categories, # one based on the first letter of the ticket and # one based on the length mod_df[&quot;TicketLetter&quot;] = mod_df[&quot;Ticket&quot;].apply(lambda x: str(x)[0]) mod_df[&quot;TicketLetter&quot;] = mod_df[&quot;TicketLetter&quot;].apply(lambda x: str(x)) highTicket = mod_df[&#39;TicketLetter&#39;].isin([&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;S&#39;, &#39;P&#39;, &#39;C&#39;, &#39;A&#39;]) lowTicket = mod_df[&#39;TicketLetter&#39;].isin([&#39;W&#39;, &#39;4&#39;, &#39;7&#39;, &#39;6&#39;, &#39;L&#39;, &#39;5&#39;, &#39;8&#39;]) mod_df[&#39;TicketLetter&#39;] = np.where(highTicket, mod_df[&quot;TicketLetter&quot;], np.where(lowTicket, &quot;LowTicket&quot;, &quot;OtherTicket&quot;)) mod_df[&#39;TicketLength&#39;] = mod_df[&#39;Ticket&#39;].apply(lambda x: len(x)) del mod_df[&#39;Ticket&#39;] # Cabin - Split into the prefix and bin the number mod_df[&quot;CabinPrefix&quot;] = mod_df[&#39;Cabin&#39;].apply(lambda x: str(x)[0]) mod_df[&quot;CabinPrefix&quot;] = mod_df[&quot;CabinPrefix&quot;].astype(&quot;category&quot;) del mod_df[&quot;Cabin&quot;] # Embarked - Fill missing data with most common value (&#39;S&#39;) mod_df[&#39;Embarked&#39;] = mod_df[&#39;Embarked&#39;].fillna(&#39;S&#39;) return mod_df . . I couldn&#39;t get it to quite match the claimed success of the notebook I borrowed it from (82%). I don&#39;t think it&#39;s my code though, since when I ran their exact code, I also got 77-78%. . # New data frame with modified features fe_train_df = edit_features(train_df) fe_train_df.columns . Index([&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;, &#39;NameLength&#39;, &#39;NameTitle&#39;, &#39;Age_na&#39;, &#39;FamilySize&#39;, &#39;TicketLetter&#39;, &#39;TicketLength&#39;, &#39;CabinPrefix&#39;], dtype=&#39;object&#39;) . With the modified features, I ran the training data through the same mechanism as before for creating a random forest . # Identify categorical and continuous columns and prep training data fe_cont, fe_cat = cont_cat_split(fe_train_df, 1, dep_var=dep_var) fe_splits = RandomSplitter()(range_of(fe_train_df)) fe_tab_panda = TabularPandas(fe_train_df, column_ops, fe_cat, fe_cont, y_names=dep_var, splits=fe_splits) # Alias training and validation data fetrainxs, fetrainy = fe_tab_panda.train.xs, fe_tab_panda.train.y fevalidxs, fevalidy = fe_tab_panda.valid.xs, fe_tab_panda.valid.y print(f&quot;Categorical columns: {fe_cat}&quot;) print(f&quot;Continuous columns: {fe_cont}&quot;) fe_tab_panda.show(3) . Categorical columns: [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Embarked&#39;, &#39;NameTitle&#39;, &#39;Age_na&#39;, &#39;FamilySize&#39;, &#39;TicketLetter&#39;, &#39;CabinPrefix&#39;] Continuous columns: [&#39;Age&#39;, &#39;Fare&#39;, &#39;NameLength&#39;, &#39;TicketLength&#39;] . Pclass Sex Embarked NameTitle Age_na FamilySize TicketLetter CabinPrefix Age Fare NameLength TicketLength Survived . 769 3 | male | S | Mr. | 0 | Solo | LowTicket | n | 32.0 | 8.362500 | 32 | 4 | 0 | . 49 3 | female | S | Mrs. | 0 | Nuclear | 3 | n | 18.0 | 17.799999 | 45 | 6 | 0 | . 704 3 | male | S | Mr. | 0 | Nuclear | 3 | n | 26.0 | 7.854200 | 23 | 6 | 0 | . # Create a random forest model classifier fe_model = RandomForestClassifier(n_estimators=300, min_samples_split=5, oob_score=True) fe_model.fit(fetrainxs, fetrainy) fe_test_df = edit_features(test_df) fe_test_tab = TabularPandas(fe_test_df, column_ops, fe_cat, fe_cont) error = zero_one_loss(fe_model.predict(fevalidxs), fevalidy) print(f&quot;Model error: {error}&quot;) print(f&quot;Model oob error: {1.0 - fe_model.oob_score_}&quot;) . Model error: 0.1685393258426966 Model oob error: 0.16830294530154277 . # Make and submit predictions fe_survived = fe_model.predict(fe_test_tab.xs) fe_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : fe_survived}) fe_df.to_csv(&quot;titanic_fe.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_fe.csv -m &quot;Random forest classifier with feature engineering&quot; . Random forest with feature engineering model score: 0.78708 . This model did score a little bit better than the random forest model1. I&#39;m not entirely convinced it&#39;s actually better than the non-engineered models though. However, plotting the feature importance, it&#39;s pretty easy to see that it is likely the added &#39;NameTitle&#39; and &#39;NameLength&#39; features are providing something beyond what &#39;Name&#39; did. . fi = feature_importance(fe_model, fetrainxs) fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) . &lt;AxesSubplot:ylabel=&#39;cols&#39;&gt; . In any case, even though this feature engineering did not produce a model that blew the other models away, I really liked the justification and the write-up of the author for each change that he made. I think it&#39;s good insight on how I might use feature engineering on a less studied dataset. . Using a Neural Network . Changing gears, let&#39;s see if a neural network would garner better results. . First, I needed to set up the data. I dropped the PassengerId column, since it&#39;s just an index, and set up the TabularPandas similar to before, the only major change is the addition of the Normalize operation. . # Split the columns by cardinality of 500 nn_cont_cols, nn_cat_cols = cont_cat_split(train_df, dep_var=dep_var) # Drop the counter variable nn_cont_cols.remove(&#39;PassengerId&#39;) # Added the Normalize operation nn_procs = [Categorify, FillMissing, Normalize] # Make the TabularPandas object nn_train_tab = TabularPandas(train_df, nn_procs, nn_cat_cols, nn_cont_cols, splits=splits, y_names=dep_var, y_block=CategoryBlock) . Next, I set up the DataLoader and the learner itself and made a stab at finding an appropriate learning rate. . nn_dls = nn_train_tab.dataloaders(512) learn = tabular_learner(nn_dls, loss_func=F.l1_loss) . After experimenting with a number of different epochs, I found that 300 epochs with a learning rate of 0.04 seemed to give reasonable results on the validation set and not take forever (~5 seconds). . lr = 4e-2 epochs = 300 with learn.no_logging(): # Prevents display of the 300 rows of epochs learn.fit_one_cycle(epochs, lr) train_error, valid_error = learn.recorder.values[-1] print(f&quot;Training error: {train_error}&quot;) print(f&quot;Validation error: {valid_error}&quot;) . Training error: 0.014065220020711422 Validation error: 0.1932232528924942 . From there, it was fairly easy to make predictions, once I found the right incantation to get the test data in the right format (learn.dls.test_dl), call the right prediction function (get_preds), and flatten the predictions to a single vector: . # Make and submit predictions _, nn_predictions = learn.get_preds(dl=learn.dls.test_dl(test_df)) nn_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : nn_predictions.flatten()}) nn_df.to_csv(&quot;titanic_nn.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_nn.csv -m &quot;Neural network predictions&quot; . Neural network score: 0.74641 . As you can see, the neural network didn&#39;t score that much worse that the random forest models. . Ensemble . Now, I&#39;ve got a handful of models that take slightly different approaches (or at least use different numbers of trees). I can combine them all into a single model. There may be other ways to approach this, but the simplest thing that came to my mind was to use a majority vote to decide who survived. Below is the ensemble that scored best on the competition test set of the ones that I tried. . # Gather the models model_files = [&quot;titanic_rf_220.csv&quot;, # 220-tree Random Forest &quot;titanic_rffe.csv&quot;, # RF with feature engineering &quot;titanic_nn.csv&quot;, # Neural network model &quot;gender_submission.csv&quot; # Women only survival ] # Combined the models into a table data = [] for i, mfile in enumerate(model_files): model_df = pd.read_csv(mfile) data.append(model_df[&#39;Survived&#39;]) df = pd.DataFrame(data) # Majority vote ensemble_survived = pd.to_numeric(df.mode().iloc[0], downcast=&#39;integer&#39;) # Save to csv and submit ensemble_df = pd.DataFrame({&#39;PassengerId&#39;: test_df[&#39;PassengerId&#39;], &#39;Survived&#39; : ensemble_survived}) ensemble_df.to_csv(&quot;titanic_ensemble.csv&quot;, index=False) . !kaggle competitions submit -c titanic -f titanic_ensemble.csv -m &quot;Ensemble&quot; . Ensemble score: 0.78468 . In this case, the ensembles didn&#39;t score any better than the best individual models. It appears I have reached the limits of what I able to do with the approaches directly mentioned in the fastai lesson. I&#39;m going to end my adventure with the Titanic here, since I worked the entire way through the lesson, but I have to say that was fun. . If I were to continue, I would be taking a much closer look at this notebook, which used a very similar fastai approach and achieved a score above 98%. They supplemented their data with this extended set, however. .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2021/01/31/titanic-kaggle.html",
            "relUrl": "/jupyter/2021/01/31/titanic-kaggle.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Know Your Meme",
            "content": "Introduction . In previous blog entries, I have focused on natural language processing (i.e. text) for the dives into the fastai course. Today, instead, I finally bit the bullet and shifted to working on a computer vision (i.e. images) project. In particular, I&#39;m going to train a model that handles classificaton of images with mutiple categories. . The data today comes from the same Google Hangouts extraction that I have used before (from Google Takeout. However, I&#39;m switching my focus from the text in the chat to all of the images that appear in the conversation. I had previously lazily marked these all as &quot;memes&quot; (specifically as &quot; &lt;MEME&gt;&quot;) in terms of how they would appear as text, but a closer look revealed that is an over-simplification of the images. . The images contain a wide variety of types and I&#39;m set on using them to train this multiple category model. Among the categories I&#39;m considering are the sender of the image and whether it is a meme or a twitter post or a news article. But I&#39;ll cover that in a few minutes. . Data Preparation . As you may recall, the chat converstation log comes from a Google Takeout download, where the primary source of the log is a json file. The json file contains a dictionary of events, any of which could correspond to a chat message, a link, an image, or something like a receipt notification. . Again, we start by loading the json data into Python and narrow the data to the events corresponding to actual chat content: . import json # Load the data from the chatfile json_filename = &quot;./Hangouts_20201001.json&quot; json_file = open(json_filename, encoding=&#39;utf8&#39;) data = json.load(json_file) json_file.close() # Set of keys to descend json tree to chat content eventKeys = (&quot;conversations&quot;, 5, &quot;events&quot;) # Chat content events events = data for k in eventKeys: events = events[k] . As before, each sender ID maps to a person, whose pseudonym is below: . sender_lookup = {&quot;108076694707330228373&quot;: &quot;Kappa&quot;, &quot;107112383293353696822&quot;: &quot;Beta&quot;, &quot;111672618051461612345&quot;: &quot;Omega&quot;, &quot;112812509779111539276&quot;: &quot;Psi&quot;, &quot;114685444329830376810&quot;: &quot;Gamma&quot;, &quot;112861108657200483380&quot;: &quot;Sigma&quot;} . There are at least two different ways that images can show up in the chat event log. One is as an embedded attachment and another is as a link. (If there are others, they are ignored for the purposes of this experiment.) In both cases, the log provides a url link to the image that can be downloaded. . First step is to loop through the events to download each image locally and construct a way to reference each image. I used the download_url routine in fastai for the former and a csv file for the latter with appropriate info. The description of the csv headers is in the code below that sets up the csv file for writing. . The course lesson that talked about Pandas, which I never used or heard of before. I&#39;m going to use it through to maniuplate the csv-like tables. With that in mind, the initial construction may seem a bit odd. . from fastai.vision.all import * # Pandas is in the fastai package # Headers for the csv file columnHeaders = [&quot;sender&quot;, # Image sender &quot;filename&quot;, # Image filename &quot;type&quot;, # Image type as labeled by Google &quot;timestamp&quot;, # Sent timestamp ] . Then, here&#39;s the code for looping through the json events, downloading each image, and adding a line to the dataframe for each image . # Do a count of how many images for each sender senderCounts = dict() # Index for filename imageIndex = 0 # Path for images imagesPath = &quot;/storage/chat_memes&quot; # These indexes turned out not to be images (e.g. not all links are images) badImages = [119, 196, 1000, 1003, 1052, 1455, 1552, 1699] rows = [] for event in events: # Get the ontent of each message (if it exists) msgContent = event.get(&quot;chat_message&quot;,{}).get(&quot;message_content&quot;,{}) # Timestamp of message timestamp = int(event[&quot;timestamp&quot;]) # Message sender sender = event[&quot;sender_id&quot;][&quot;gaia_id&quot;] sender = sender_lookup[sender] # Images as attachments attachments = msgContent.get(&quot;attachment&quot;,[]) for attachment in attachments: if imageIndex not in badImages: item = attachment[&quot;embed_item&quot;] itemType = item[&quot;plus_photo&quot;][&quot;media_type&quot;] # Usually PHOTO itemUrl = item[&quot;plus_photo&quot;][&quot;url&quot;] filename = f&quot;{imagesPath}/image_{imageIndex:04}&quot; download_url(itemUrl, filename, timeout=10, retries=20, show_progress=False) rows.append({&quot;timestamp&quot;: timestamp, &quot;sender&quot;: sender, &quot;filename&quot;: filename, &quot;type&quot;: itemType}) senderCounts[sender] = senderCounts.get(sender, 0) + 1 imageIndex += 1 # Images as links segments = msgContent.get(&quot;segment&quot;,[]) for segment in segments: if segment[&quot;type&quot;] == &quot;LINK&quot;: url = segment[&quot;text&quot;] base = url.split(&#39;/&#39;)[-1] # Check if it&#39;s possibly an image if &#39;jpg&#39; in base or &#39;gif&#39; in base or &#39;png&#39; in base: if imageIndex not in badImages: filename = f&quot;{imagesPath}/image_{imageIndex:04}&quot; download_url(url, filename, timeout=10, retries=20, show_progress=False) rows.append({&quot;timestamp&quot;: timestamp, &quot;sender&quot;: sender, &quot;filename&quot;: filename, &quot;type&quot;: &quot;LINK&quot;}) senderCounts[sender] = senderCounts.get(sender, 0) + 1 imageIndex += 1 . # Use rows to create a Pandas dataframe images_df = pd.DataFrame(rows, columns=columnHeaders) # Save the rows to a csv file images_df.to_csv(&quot;chat_images.csv&quot;) # Display the first five rows images_df.head() . sender filename type timestamp . 0 Kappa | /storage/chat_memes/image_0000 | PHOTO | 1600741647230976 | . 1 Gamma | /storage/chat_memes/image_0001 | PHOTO | 1600772605984217 | . 2 Psi | /storage/chat_memes/image_0002 | PHOTO | 1600786824022176 | . 3 Gamma | /storage/chat_memes/image_0003 | PHOTO | 1600861187519443 | . 4 Kappa | /storage/chat_memes/image_0004 | PHOTO | 1600863157273002 | . I ended up with about 1700 images from the chat log. Here&#39;s a quick breakdown of how many each sender was responsible for: . totalImages = 0 for sender in senderCounts: totalImages += senderCounts[sender] print(f&quot;Total images: {totalImages}&quot;) for sender in senderCounts: images = senderCounts[sender] percent = 100 * images / totalImages print(f&quot;{sender} sent {images} images -&gt; {percent:.03}%&quot;) . Total images: 1718 Kappa sent 707 images -&gt; 41.2% Gamma sent 294 images -&gt; 17.1% Psi sent 299 images -&gt; 17.4% Beta sent 95 images -&gt; 5.53% Omega sent 322 images -&gt; 18.7% Sigma sent 1 images -&gt; 0.0582% . And here&#39;s a sample image . im = Image.open(f&quot;{imagesPath}/image_0000&quot;) im.to_thumb(240) . As might be clear by this point, one clear drawback to using this data to for &quot;multi-category&quot; classification is that the data is really labeled with more than one category (sender is really the only usable one). . The approach that I ended up using to label the day was to just label the data myself 1. I didn&#39;t want to go through each of the 1700 images one by one and apply labels, but I found a tool called pigeon that was incredibly useful to enable this labeling to take until the heat death of the universe to complete. It still took multiple hours :weary:, but I learned a new tool :sunglasses:. . Before that, though, I needed some labels to apply to the images. After some trials and errors, these are the set I ended up with (along with a brief description of each one.) . labels = [&#39;meme&#39;, # Is it a meme &#39;social_media&#39;, # Is it a screenshot from social media (usually twitter, sometimes reddit or fb) &#39;comic_strip&#39;, # Is it a multi-panel comic (usually xkcd or SMBC) &#39;game_of_thrones&#39;, # Still mad about the final season &#39;simpsons&#39;, # Enough Simpsons memes to be on their own &#39;rick_and_morty&#39;, # Enough Rick &amp; Morty memes to be on their own &#39;tv_cartoon&#39;, # But not enough South Park, Futurama, or Family Guy to train alone (every simpsons and rick_and_morty is marked this too) &#39;star_trek_wars&#39;, # Combined star trek and wars to get enough &#39;marvel&#39;, # Avengers and others memes &#39;lotr&#39;, # One does not simply something, something &#39;news&#39;, # Is this a news item (rather than a humorous one) &#39;ali_express&#39;, # One member of the chat posts weird things from Ali Express &#39;photo&#39;, # Is this a real-life photo (as opposed to an internet one) &#39;trump&#39;, # Is this trump-related &#39;beacon&#39;, # Is this a beacon for playing video games &#39;weird&#39; # random label I put on stuff I thought was unusual, even for us ] . With these labels in hand, we can then use pigeon to annotate each image with a label . from pigeon import annotate from IPython.display import display import glob imagesPath = &quot;/storage/chat_memes&quot; images = glob.glob(imagesPath+&quot;/image*&quot;) annotations = annotate(images, options = labels, display_fn=lambda filename: display(Image.open(filename).to_thumb(240))) . Alternatively, by setting options to None, you can use free-form text for the labels. I ended up using the labels list for the first pass to give each image a label and then the free-form version for adding additional labels. In the end, annotations is a Python dictionary mapping a filename to a list of string labels. I saved these to their own pandas dataframe and csv, so as not to lose any of the annotations I made. . rows = [] for filename, labels in annotations.items(): labels = &quot; &quot;.join(labels) rows.append({&quot;filename&quot;: filename, &quot;labels&quot;: labels}) annotations_df = pd.DataFrame(rows) annotations_df.to_csv(&quot;annotations.csv&quot;) annotations_df.head() . filename labels . 0 /storage/chat_memes/image_0029 | meme simpsons tv_cartoon | . 1 /storage/chat_memes/image_0071 | meme simpsons tv_cartoon trump | . 2 /storage/chat_memes/image_0096 | meme tv_cartoon | . 3 /storage/chat_memes/image_0105 | meme tv_cartoon | . 4 /storage/chat_memes/image_0231 | meme tv_cartoon | . And merge it with the images dataframe I had made earlier: . combined_df = images_df.merge(annotations_df, on=&#39;filename&#39;) combined_df.to_csv(&quot;chat_memes_with_labels.csv&quot;) combined_df.head() . sender filename type timestamp labels . 0 Kappa | /storage/chat_memes/image_0000 | PHOTO | 1600741647230976 | social_media | . 1 Gamma | /storage/chat_memes/image_0001 | PHOTO | 1600772605984217 | social_media | . 2 Psi | /storage/chat_memes/image_0002 | PHOTO | 1600786824022176 | social_media | . 3 Gamma | /storage/chat_memes/image_0003 | PHOTO | 1600861187519443 | social_media | . 4 Kappa | /storage/chat_memes/image_0004 | PHOTO | 1600863157273002 | comic_strip | . Finally, this gives us some data that I can work with to get onto the multi-category classification. Whew! . Multi-category Classification . Following the example in the lesson, I can now create a DataBlock object to feed into a DataLoaders object to a neural network learner. First, I need some functions to specify my independent and dependent variables. The independent ones are the images/memes and the dependent ones are the sender and the labels that I just annotated above 2. . def get_x(row): return row[&#39;filename&#39;] def get_y(row): if row[&#39;sender&#39;] == &#39;Sigma&#39;: return row[&#39;labels&#39;].split() else: return row[&#39;labels&#39;].split() + [row[&#39;sender&#39;]] . These facilitate building the DataBlock that can be put into a DataLoaders: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(combined_df) . As a reward for reading this far, here&#39;s a batch of memes with their labels that the Dataloaders generates. Note that the RandomResizedCrop transform is why the display below is a cropped portion of the image. As I understand it, it&#39;s designed to normalize the input data to prevent effects of image size and dimension from having an influence on training the model. . dls.show_batch(nrows=2, ncols=4) . Create a learner module from the loaded data: . learn = cnn_learner(dls, resnet18) . The default metric for multi-category labelled data is accuracy_multi which uses a sigmoid function with a particular threshold to decide whether or not an image gets a particular label. . I had no idea what the learning rate or threshold should be for this data, so I tried to used their approaches to use the data to determine what the learning rate and threshold should be. . First, the learning rate: . learn.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=0.03981071710586548) . The suggested learning rate looks to be 0.03 or 0.04 is called for. Now, for the threshold for accuracy_multi: . preds,targs = learn.get_preds() xs = torch.linspace(0.05, 0.99,30) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . Unfortunately, I did not get any sort of peak in curve. Any speculation as to why on my part at this point would be a wild guess. So, I picked a somewhat random value of 0.8 for the threshold. . lr = 0.04 threshold = 0.8 learn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=threshold)) learn.fine_tune(3, base_lr=lr) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.665638 | 0.417254 | 0.898459 | 00:47 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.263506 | 0.572447 | 0.889776 | 01:00 | . 1 | 0.236928 | 0.222640 | 0.918207 | 00:59 | . 2 | 0.211741 | 0.180523 | 0.926891 | 00:59 | . Next, I saved the model so I can use it later, without having to retrace all of the above steps . learn.export(&quot;meme_classification.pkl&quot;) . Inference . Now, with a trained model, I can try to make predictions. I need some data to actually infer on. Fortunately, since I am taking my sweet time going through the course, I had two more months of chat images (471 in total) that I could use for that. . I used the same process as above to import the images from the json file, but now, I only considered messages that had timestamps that were greater than the last timestamp from the training and validation set. I stored those images along with their senders in a new Pandas data frame and save it to a csv. . # Use rows to create a Pandas dataframe new_images_df = pd.DataFrame(new_image_rows, columns=new_image_column_headers) # Display the first five rows new_images_df.head() . sender filename type timestamp . 0 Beta | /storage/chat_memes/new_images/image_0000 | PHOTO | 1607981480934474 | . 1 Gamma | /storage/chat_memes/new_images/image_0001 | PHOTO | 1607991667741774 | . 2 Omega | /storage/chat_memes/new_images/image_0002 | PHOTO | 1608007771189618 | . 3 Kappa | /storage/chat_memes/new_images/image_0003 | PHOTO | 1608041280956958 | . 4 Kappa | /storage/chat_memes/new_images/image_0004 | PHOTO | 1608081802164842 | . For the 472 new images, the breakdown of senders was {&#39;Kappa&#39;: 222, &#39;Psi&#39;: 95, &#39;Gamma&#39;: 71, &#39;Omega&#39;: 63, &#39;Beta&#39;: 20}. After my labeling quagmire above, I didn&#39;t bother trying to otherwise label the new data, but just looked at a few samples instead, which I&#39;ll show below. . First, let&#39;s get the sender and labels predictions for each of the new images into a Pandas data frame: . # Senders (RSVP Sigma) senders = set([&#39;Kappa&#39;, &#39;Gamma&#39;, &#39;Psi&#39;, &#39;Beta&#39;, &#39;Omega&#39;]) # Column headers predict_column_headers = [&#39;filename&#39;, &#39;predict_sender&#39;, &#39;predict_labels&#39;] # One row for each file predict_rows = [] for image_file in new_images_df[&#39;filename&#39;]: # no_bar prevents a progress appearing for each predict call with learn.no_bar(): labels = learn.predict(image_file) all_labels = set(labels[0]) sender = all_labels.intersection(senders) other_labels = all_labels.difference(sender) if &#39;471&#39; in image_file: print(image_file) row = {} row[&#39;filename&#39;] = image_file if len(sender) &gt; 0: row[&#39;predict_sender&#39;] = &quot; &quot;.join(sender) if len(other_labels) &gt; 0: row[&#39;predict_labels&#39;] = &quot; &quot;.join(other_labels) predict_rows.append(row) # Use rows to create a Pandas dataframe predict_df = pd.DataFrame(predict_rows, columns=predict_column_headers) # Display the first five rows predict_df.head() . filename predict_sender predict_labels . 0 /storage/chat_memes/new_images/image_0000 | NaN | meme | . 1 /storage/chat_memes/new_images/image_0001 | NaN | social_media | . 2 /storage/chat_memes/new_images/image_0002 | Omega | ali_express | . 3 /storage/chat_memes/new_images/image_0003 | NaN | meme | . 4 /storage/chat_memes/new_images/image_0004 | Kappa | meme | . I&#39;ll combine this data frame with the one extracting the new images from the json file: . combined_new_images_df = new_images_df.merge(predict_df, on=&#39;filename&#39;) combined_new_images_df.to_csv(&quot;chat_new_images_predictions.csv&quot;) combined_new_images_df.head() . sender filename type timestamp predict_sender predict_labels . 0 Beta | /storage/chat_memes/new_images/image_0000 | PHOTO | 1607981480934474 | NaN | meme | . 1 Gamma | /storage/chat_memes/new_images/image_0001 | PHOTO | 1607991667741774 | NaN | social_media | . 2 Omega | /storage/chat_memes/new_images/image_0002 | PHOTO | 1608007771189618 | Omega | ali_express | . 3 Kappa | /storage/chat_memes/new_images/image_0003 | PHOTO | 1608041280956958 | NaN | meme | . 4 Kappa | /storage/chat_memes/new_images/image_0004 | PHOTO | 1608081802164842 | Kappa | meme | . Some overall observations from the predictions the model made and this table: . Only 259 images actually got labeled with a sender: 199 for Kappa, 56 for Omega, and 4 for Gamma. Beta and Psi were never predicted as a label. Of those 259 sender predictions, 134 were correct. . 20 of the images weren&#39;t predicted to have any label (sender or otherwise) at all . I wrote some widget code to display the images with their predicted and correct sender and the predict labels . import ipywidgets as widgets def vbox_single(number): im_file = f&quot;{new_images_path}/image_{number:04}&quot; tsender = combined_new_images_df[&#39;sender&#39;][number] psender = combined_new_images_df[&#39;predict_sender&#39;][number] plabels = combined_new_images_df[&#39;predict_labels&#39;][number] w1 = widgets.Label(f&quot;Correct Sender: {tsender}&quot;) w2 = widgets.Label(f&quot;Predicted Sender: {psender}&quot;) w3 = widgets.Label(f&quot;Predicted Labels: {plabels}&quot;) w4 = widgets.Image(value=Image.open(im_file).to_thumb(240).to_bytes_format(), width=240, height=240) return widgets.VBox([w1, w2, w3, w4]) . Here&#39;s some of the ones where the sender was predicted correctly: . indexes = [9, 247, 285, 18, 281, 447] items = [vbox_single(i) for i in indexes] gridbox = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=&quot;repeat(3, 300px)&quot;)) display(gridbox) . And some of the ones where the sender was predicted incorrectly: . indexes = [466, 122, 152, 190, 313, 349] items = [vbox_single(i) for i in indexes] gridbox = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=&quot;repeat(3, 300px)&quot;)) display(gridbox) . And 3 more random ones for the brave reader who got this far: . indexes = [166, 123, 74] items = [vbox_single(i) for i in indexes] gridbox = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=&quot;repeat(3, 300px)&quot;)) display(gridbox) . In the images I looked through, the classifier did a pretty good job on appropriately labeling &#39;social_media&#39;, &#39;meme&#39;, &#39;tv_cartoon&#39;, &#39;comic_strip&#39;, &#39;photo&#39; and &#39;ali_express&#39; in the same way that I did, but struggled to do the same with the other labels. Again, if I cared more about results (and had better labeled data), I&#39;m sure I could improve the accuracy, but I&#39;m just happy to work through the multi-classification learning to produce something reasonable. MEMES! :satisfied: . 1. I thought about a few different ways to try to apply categorical labels to the data. Given that I&#39;m in the midst of a deep learning course, training a model to label the data was an option I considered, but I would essentially have to create a model for each type of category that I wanted, download images for each of those and use the models to classify the chat image data. If I had to do it over again, that is likely the approach I would take. Hindsight is 20/20.↩ . 2. The fact that Sigma only sent 1 image meant I couldn&#39;t really train to find that label, so I removed Sigma from the possible labels. Sorry Sigma.↩ .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2020/12/28/know-your-meme.html",
            "relUrl": "/jupyter/2020/12/28/know-your-meme.html",
            "date": " • Dec 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Name that Speaker",
            "content": "Intro . The first few lessons of the fastai course lean heavily towards computer vision problems with their examples. Personally, I am a little more interested in natural language processing and work with text applications, so I glommed onto their example of doing sentiment analysis of movie reviews using fastai. . Here&#39;s how they built that model using the IMDB dataset internal to the library: . from fastai.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;, encoding=&#39;utf8&#39;, bs=32) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.611722 | 0.397967 | 0.820560 | 07:53 | . epoch train_loss valid_loss accuracy time . 0 | 0.304293 | 0.294640 | 0.875800 | 16:06 | . 1 | 0.280457 | 0.206577 | 0.920880 | 16:07 | . 2 | 0.201380 | 0.181090 | 0.929840 | 16:08 | . 3 | 0.153116 | 0.179792 | 0.931280 | 16:05 | . The generated model learn can then be used to predict the sentiment of a statement. I picked three statements below to show what it&#39;s predictions are like. The model predicts the first two statements accurately and is fairly confident in its prediction. For the third, the model predicts the sentiment, but isn&#39;t as confident, which is not surprising since I picked that one to be intentionally tricky. . x = learn.predict(&quot;I really liked that movie!&quot;) y = learn.predict(&quot;At no point in your rambling, incoherent response was there anything that could even be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.&quot;) z = learn.predict(&quot;I thought it was going to be good, but it really was not in the end.&quot;) print(f&quot;Sentiment of x: {x[0]}, prob={x[2][1]:.4f}&quot;) print(f&quot;Sentiment of y: {y[0]}, prob={y[2][0]:.4f}&quot;) print(f&quot;Sentiment of z: {z[0]}, prob={z[2][0]:.4f}&quot;) . Sentiment of x: pos, prob=0.9987 Sentiment of y: neg, prob=0.9659 Sentiment of z: neg, prob=0.7692 . My idea was to take this template for building a text classification model and use it to classify the &quot;speaker&quot; of a given statement, given a previous set of chat conversations to train on. . Data Preparation . In a previous post, I took some data from a Google Hangouts chat and converted it to a format more palatable to feeding into a PyTorch LSTM, i.e. each chat message was broken up to be in the format . Speaker :: Message . I&#39;m going to use the same underlying data here, but format it slightly differently to ease import into fastai. This might not be the cleanest way to do this, but it worked :smile: . The format I ended up using was a modified csv. Commas are pretty prevalent in the data and I hate using quotes and escapes, so I used | to separate the columns 1. Since I had already done the separation of speaker and message using :: before, the script to convert was fairly straightforward, minus one spot where someone had used an SAT-style analogy . Kappa :: Omega:OK :: Gamma:&quot;Here&#39;s the thing&quot; . chat_filename = &quot;/notebooks/fastbook/chat/chatFile.txt&quot; chat_csv = &quot;/notebooks/fastbook/chat/chatFile.csv&quot; # Read in the chat file with data = open(chat_filename, encoding=&#39;utf8&#39;).read() # As software developers, we used &quot;||&quot; a few places to mean OR data = data.replace(&quot;||&quot;, &quot;or&quot;) data = data.splitlines() # Write to csv with open(chat_csv, encoding=&#39;utf8&#39;, mode=&#39;w&#39;) as csv: # Header csv.write(&quot;Name|Message&quot;) # New message for line in data: if &quot;::&quot; in line: x = line.split(&quot;::&quot;) if len(x) &gt; 2: (name, msg) = (&quot;Kappa&quot;, &quot;Omega:Ok :: Gamma:Here&#39;s the thing&quot;) else: (name, msg) = line.split(&quot;::&quot;) name = name.strip() msg = msg.strip() csv.write(&#39; n&#39;) csv.write(name) csv.write(&quot;|&quot;) csv.write(msg) else: csv.write(&quot; &quot; + msg) csv.write(&#39; n&#39;) . Build Model . The csv now matched each message to a particular speaker in a format that was easily digestible by fastai. Next, I mimicked the sentimental analysis example above to make my speaker identification model. I&#39;m essentially just swapping from_folder out for from_csv, with some extra arguments to give details about my csv. . from fastai.text.all import * dls = TextDataLoaders.from_csv(&#39;.&#39;, csv_fname=chat_csv, delimiter=&quot;|&quot;, text_col = 1, label_col = 0) learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . /opt/conda/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . epoch train_loss valid_loss accuracy time . 0 | 1.567062 | 1.340145 | 0.449983 | 00:16 | . epoch train_loss valid_loss accuracy time . 0 | 1.305043 | 1.235283 | 0.500000 | 00:38 | . 1 | 1.228520 | 1.160044 | 0.540014 | 00:37 | . 2 | 1.107468 | 1.124508 | 0.564677 | 00:37 | . 3 | 1.059996 | 1.121178 | 0.570369 | 00:37 | . Note that there&#39;s a lot less data here than in the IMDB set, so training is much faster. Also, I ignored the warning now since it was just a deprecation warning. Not sure if that&#39;ll bite me later. . Next, let&#39;s save the model to a file for later use . learn.export(&quot;/notebooks/fastbook/chat/chat_model.pkl&quot;) . My First App . The challenge in the second lesson of the fastai course was to create a model using fastai and turn it into a prototype web app. The structure of how to do so using ipywidgets and voila was pretty straightforward. . A box for giving the text to evaluate . import ipywidgets as widgets txtInput = widgets.Textarea(placeholder=&#39;Input text...&#39;, description=&#39;Text:&#39;) txtInput . A button to execute the prediction for the model . button = widgets.Button(description=&#39;Predict&#39;, tooltip=&#39;Click me&#39;, icon=&#39;question&#39;) button . Set up the output widget with a dividing line . outWidget = widgets.Output(layout={&#39;border&#39;: &#39;1px solid black&#39;}) outWidget . def on_click_classify(change): # predictions and probabilities from the model prediction, idx, probs = learn_inf.predict(txtInput.value) # pair the probabilities with each speaker outputs = list(zip(probs, learn_inf.dls.vocab[1])) # sort the list with the most likely speaker first outputs.sort(reverse=True) outWidget.clear_output() # Print the output, with the most likely speaker in bold with outWidget: header = widgets.HTML() header.value = &#39;&lt;u&gt;Scores&lt;/u&gt;&#39; display(header) lblPred = widgets.HTML() lblPred.value = f&#39;&lt;b&gt;{outputs[0][1]}&lt;/b&gt;: &lt;b&gt;{100 * outputs[0][0]:.2f}%&lt;/b&gt;&#39; display(lblPred) for (prob, name) in outputs[1:]: lbl = widgets.Label() lbl.value = f&#39;{name}: {100 * prob:.2f}%&#39; display(lbl) button.on_click(on_click_classify) . Shortcoming . One obvious shortcoming of this speaker identification model is that one of the speakers (&#39;Kappa&#39;) was much more likely to be identified as the most likely speaker than any of the other speakers for almost any text. He accounts for about 44% of the input messages, but I wasn&#39;t sure how (or even if I should) adjust for that. . Failure to Launch . I was able to run Voila locally in my notebook and get it to produce a viable web app. Unfortunately, I was unable to get it to host properly on Heroku, as suggested in the course. All I could seem to get was a nebulous &quot;Application Error&quot; and did not have the time or patience to wade through figuring it out. . I have some evidence to think that the issue was the OS differences between the Paperspace notebooks that I was using for fastai development, the Windows environment I hosted the Jupyter notebook (and ultimately got the app running locally), and whatever Heroku is running on their server. These differences preventing a model built in one place from working in another and couldn&#39;t actually build the model on Heroku. . 1. | only appeared as || (aka OR), since we are software nerds↩ .",
            "url": "https://bobowedge.github.io/adventures-in-telework/jupyter/2020/11/18/name-that-speaker.html",
            "relUrl": "/jupyter/2020/11/18/name-that-speaker.html",
            "date": " • Nov 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Adventures in PyTorch",
            "content": "Introduction . My telework journey into better understanding of deep learning began a few weeks back by watching this video. I had some prior exposure to PyTorch, but most of it was cut and pasting someone else&#39;s code, without really grokking much of what I was doing. . I don&#39;t remember much out of the video itself (not unexpected for something titled a &quot;60-minute blitz&quot;), but I started poking around at some of the examples. My primary interest in machine learning is its use in natural language processing or language modeling and, thus, the &quot;Word-level language modeling RNN&quot; code particularly caught my eye. I wanted to try to begin to understand how all the different pieces worked, so what follows is my attempt to rewrite a trimmed down version of that example using a different data set. . Data Prep . The data I used was a personal Google Hangouts chatroom I have had with a few friends since sometime in 2018. I learend that you can use Google Takeout to download copies of any of your Google data. Using that with Hangouts gave me a json dump of the chat along with the attachments (read: memes) that were posted. This dump had a lot of extraneous information and wasn&#39;t exactly primed for reading by either myself or PyTorch, so I needed to massage that json dump into text to get something usable. . . Warning: Some of the chat content may contain profanity or stupidity. . First order of business, load the data into Python using the json module: . import json # Load the data from the chatfile json_filename = &quot;. data Takeout Hangouts Hangouts.json&quot; json_file = open(json_filename, encoding=&#39;utf8&#39;) data = json.load(json_file) json_file.close() . After some digging and verification, I matched everyone&#39;s ID in chat to their real name and saved a lookup table with that info (names have been changed to protect the not-so-innocent). . # Match IDs to names sender_lookup = {&quot;108076694707330228373&quot;: &quot;Kappa&quot;, &quot;107112383293353696822&quot;: &quot;Beta&quot;, &quot;111672618051461612345&quot;: &quot;Omega&quot;, &quot;112812509779111539276&quot;: &quot;Psi&quot;, &quot;114685444329830376810&quot;: &quot;Gamma&quot;, &quot;112861108657200483380&quot;: &quot;Sigma&quot;} . Since I was focused on language modeling, I didn&#39;t feel like dealing with pictures or attachments, but I wanted to account for them in some way when they came up in chat, so I put in a substitute phrase for whenever they showed up: . # Replacement text for memes meme = &quot;&lt;MEME&gt;&quot; . Each message in the json data structure was listed as an &#39;event&#39;, a dictionary with key &quot;chat_message&quot; and sub-key &quot;message_content&quot;. From there, I could get the sender ID, timestamp, and actual content of the message . # Set of keys to descend into json tree keys = (&quot;conversations&quot;, 5, &quot;events&quot;) # Descend the tree to the events list events = data for k in keys: events = events[k] messages = [] # Loop through the events for event in events: # Check for a valid message if &quot;chat_message&quot; in event: msg_content = event[&quot;chat_message&quot;][&quot;message_content&quot;] else: continue # Timestamp of the message, which helps with sorting correctly later timestamp = int(event[&quot;timestamp&quot;]) # Message sender sender = event[&quot;sender_id&quot;][&quot;gaia_id&quot;] sender = sender_lookup[sender] # Message content message = &quot;&quot; if &quot;segment&quot; in msg_content: segment = msg_content[&quot;segment&quot;] for s in segment: # Text messages if s[&quot;type&quot;] == &quot;TEXT&quot;: message += s[&quot;text&quot;] # Non-text messages else: message += meme + &quot; &quot; message = message.strip() else: # Non-text messages message = meme # Add the message, with its timestamp and sender to the list messages.append((timestamp, sender, message)) # Sort the messages by timestamp messages.sort() . Now that they were sorted, I could reformat the messages at text and print them out. I chose :: as my separator between sender and the actual message content . num_messages = len(messages) print(&quot;{} messages found&quot;.format(num_messages)) messages = [&quot;{0} :: {1} n&quot;.format(msg[1], msg[2]) for msg in messages] . 29000 messages found . Sample chat messages: . Omega :: Apparently damage scales, but armour doesn&#39;t Omega :: We&#39;re only a few levels apart so not that bad at our current state Omega :: Probably why we sucked so bad that first night Omega :: Damn Gamma and his free time Sigma :: This game is harder than I remember Kappa :: &lt;MEME&gt; Psi :: &lt;MEME&gt; Omega :: Wonder if there&#39;s TDY to NZ Psi :: Maybe, but not for you Kappa :: Lol . This gives some text that PyTorch can work with and humans can read too. . Corpus . I wasn&#39;t a big fan of how the example wrote their Corpus class, since it required inputting a file directory path where the data was already split into training, validation, and test sets (though it probably works better for large files). I rewrote it, allowing for messages already loaded into memory and splitting the data into training/validation/test after the messages were sent into the class. In the end, you end up with the same three tensors: train, valid, and test. . import torch class Corpus(object): def __init__(self, data, train_param=0.75, valid_param=0.15, test_param=0.10): &#39;&#39;&#39; data - either a filename string or list of messages train_param - percentage of messages to use to train valid_param - percentage of messages to use to validate test_param - percentage of message to use to test &#39;&#39;&#39; # Same as their data.Dictionary() class self.dictionary = Dictionary() # Filename vs. list of messages if type(data) == str and os.path.exists(data): messages = open(data, encoding=&#39;utf8&#39;).read().splitlines() else: messages = data # Determine the number of training, validation, and test messages num_messages = len(messages) num_train_msgs = int(train_param * num_messages) num_valid_msgs = int(valid_param * num_messages) num_test_msgs = int(test_param * num_messages) if num_train_msgs &lt; 10 or num_valid_msgs &lt; 10 or num_test_msgs &lt; 10: raise RuntimeError(&quot;Not enough messages for training/validation/test&quot;) # Scale back the number of messages if need be total_param = train_param + valid_param + test_param if total_param &lt; 1.0: num_messages = num_train_msgs + num_valid_msgs + num_test_msgs messages = messages[:num_messages] elif total_param &gt; 1.0: raise RuntimeError(&quot;Invalid train/validate/test parameters&quot;) # Add to dictionary and tokenize train = [] valid = [] test = [] for msg_idx, msg in enumerate(messages): # &lt;eos&gt; is the &#39;end-of-sentence&#39; marking words = msg.split() + [&#39;&lt;eos&gt;&#39;] msg_ids = [] # Add the words in the message to the dictionary for word in words: index = self.dictionary.add_word(word) msg_ids.append(index) # Split the messages into the appropriate buckets if msg_idx &lt; num_train_msgs: train.append(torch.tensor(msg_ids).type(torch.int64)) elif msg_idx &lt; num_train_msgs + num_valid_msgs: valid.append(torch.tensor(msg_ids).type(torch.int64)) else: test.append(torch.tensor(msg_ids).type(torch.int64)) # End up with torch tensors for each of the 3 pieces, same as theirs self.train = torch.cat(train) self.valid = torch.cat(valid) self.test = torch.cat(test) . . Next, we batchify in the same way as the example . chat_corpus = Corpus(messages) # Defaults in the example train_batch_size = 20 eval_batch_size = 10 if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) else: device = torch.device(&quot;cpu&quot;) train_data = batchify(chat_corpus.train, train_batch_size, device) valid_data = batchify(chat_corpus.valid, eval_batch_size, device) test_data = batchify(chat_corpus.test, eval_batch_size, device) . Build the model . The example code gave lots of options for what the model could be. That was overkill for what I wanted and didn&#39;t really help with understanding, so I stuck to an LSTM model. LSTM was one of the model options in the example and rewrote its model class to assume that an LSTM was used. . import torch import torch.nn as nn import torch.nn.functional as F . class LSTM(nn.Module): def __init__(self, num_tokens, num_hidden, num_layers): &#39;&#39;&#39; num_tokens - number of words in the dictionary num_hidden - number of hidden states per layer num_layers - number of layers &#39;&#39;&#39; super(LSTM, self).__init__() self.num_tokens = num_tokens # Default used by example num_input_features = 200 self.encoder = nn.Embedding(num_tokens, num_input_features) self.lstm = nn.LSTM(num_input_features, num_hidden, num_layers) self.decoder = nn.Linear(num_hidden, num_tokens) self.init_weights() self.num_hidden = num_hidden self.num_layers = num_layers def init_weights(self): nn.init.uniform_(self.encoder.weight, -0.5, 0.5) nn.init.zeros_(self.decoder.weight) nn.init.uniform_(self.decoder.weight, -0.5, 0.5) def forward(self, input_data, hidden): embedding = self.encoder(input_data) output, hidden = self.lstm(embedding, hidden) decoded = self.decoder(output) decoded = decoded.view(-1, self.num_tokens) return F.log_softmax(decoded, dim=1), hidden def init_hidden(self, batch_size): weight = next(self.parameters()) return (weight.new_zeros(self.num_layers, batch_size, self.num_hidden), weight.new_zeros(self.num_layers, batch_size, self.num_hidden),) def repackage_hidden(self, hidden): if isinstance(hidden, torch.Tensor): return hidden.detach() else: return tuple(self.repackage_hidden(v) for v in hidden) . . Setup for the rewritten model class (now called LSTM). . num_tokens = len(chat_corpus.dictionary) num_hidden = 256 # Arbitrary choice num_layers = 3 # Arbitrary choice model = LSTM(num_tokens, num_hidden, num_layers).to(device) # Set our loss function criterion = nn.NLLLoss() . Train the model . Below is my attempt to simplify the example training and evaluation code for my purposes. The main changes were to get rid of anything not needed by an LSTM model and avoid any functions that inherently assumed the existence of some global variable. (It&#39;s probably just the C++ programmer in me, but it hurts my soul when I see that.) . # Backwards propagation through time bptt = 35 # Maximum/initial learning rate lr = 20.0 # Maximum number of epochs to use max_epochs = 40 # Gradient clipping clip = 0.25 # Output model filename model_filename = &quot;. data chat lstm.pt&quot; . def get_batch(source, index, bptt): # bptt = Backward propagation through time sequence_length = min(bptt, len(source) - 1 - index) data = source[index:index+sequence_length] target = source[index+1:index+1+sequence_length].view(-1) return data, target . import time best_validation_loss = None # This loop took about 3-4 minutes to run on my machine (about 10 seconds per loop for 20 loops) for epoch in range(0, max_epochs): epoch_start_time = time.time() ## # train() - the example&#39;s train function is rewritten here model.train() hidden = model.init_hidden(train_batch_size) for batch, index in enumerate(range(0, train_data.size(0) - 1, bptt)): data, targets = get_batch(train_data, index, bptt) # Starting each batch, we detach the hidden state from how it was previously produced. # If we didn&#39;t, the model would try backpropagating all the way to start of the dataset. model.zero_grad() hidden = model.repackage_hidden(hidden) output, hidden = model(data, hidden) loss = criterion(output, targets) loss.backward() # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs. nn.utils.clip_grad_norm_(model.parameters(), clip) for p in model.parameters(): p.data.add_(p.grad, alpha=-lr) ## # evaluate() - the example&#39;s evaluate function is rewritten here model.eval() total_loss = 0. hidden = model.init_hidden(eval_batch_size) with torch.no_grad(): for index in range(0, valid_data.size(0) - 1, bptt): data, targets = get_batch(valid_data, index, bptt) output, hidden = model(data, hidden) hidden = model.repackage_hidden(hidden) total_loss += len(data) * criterion(output, targets).item() validation_loss = total_loss / (len(valid_data) - 1) ## # A print statement to track progress # print(&#39;-&#39; * 89) # print(&#39;| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | lr {:3.2f}&#39;.format( # epoch, time.time() - epoch_start_time, validation_loss, lr)) # print(&#39;-&#39; * 89) # Save the model if the validation loss is the best we&#39;ve seen so far. if not best_validation_loss or validation_loss &lt; best_validation_loss: with open(model_filename, &#39;wb&#39;) as f: torch.save(model, f) best_validation_loss = validation_loss else: # Anneal the learning rate if no improvement has been seen in the validation dataset. lr /= 4.0 # Stop training if the learning rate gets to small if lr &lt;= 1e-3: break . Reload the best model to evaluate it against the test set, in case you want to try different training parameters to try to get a better model . with open(model_filename, &#39;rb&#39;) as f: model = torch.load(f) model.lstm.flatten_parameters() # Run on the test data import math model.eval() total_loss = 0. hidden = model.init_hidden(eval_batch_size) with torch.no_grad(): for index in range(0, test_data.size(0) - 1, bptt): data, targets = get_batch(test_data, index, bptt) output, hidden = model(data, hidden) hidden = model.repackage_hidden(hidden) total_loss += len(data) * criterion(output, targets).item() test_loss = total_loss / (len(test_data) - 1) print(&#39;=&#39; * 89) print(&#39;| End of training | test loss {:5.2f} | test ppl {:8.2f}&#39;.format(test_loss, math.exp(test_loss))) print(&#39;=&#39; * 89) . ========================================================================================= | End of training | test loss 5.19 | test ppl 179.49 ========================================================================================= . Generate Chat Logs . Now that we&#39;ve trained a model, we can use it generate some chat logs . num_words = 200 # Default used by example -&gt; &quot;higher will increase diversity&quot; temperature = 1.0 # Hidden and input states are just same size tensor as model uses hidden = model.init_hidden(1) input_data = torch.randint(num_tokens, (1, 1), dtype=torch.long).to(device) with torch.no_grad(): # no need to track history for i in range(num_words): # Generate a random word based on the history output, hidden = model(input_data, hidden) word_weights = output.squeeze().div(temperature).exp().cpu() word_idx = torch.multinomial(word_weights, 1)[0] input_data.fill_(word_idx) word = chat_corpus.dictionary.index_to_word[word_idx] # Recall: our end of message token if word == &quot;&lt;eos&gt;&quot;: print() else: print(word,end=&quot; &quot;) . NFL four angry Omega :: Gotta be a dick increments Gamma &lt;MEME&gt; Kappa :: what if we&#39;re trying to be on unable Kappa :: cuck Omega :: If only they need to enjoy well or playing? Though makes that with Told premise Kappa :: They I... the number of Seb at to wear the essence Omega :: The bit is a buuuut Omega :: Gamma seems a end mankin Omega :: Should the way to realize I get why you tell children no than a dirt Court for coats arrangement habit Kappa :: nice Psi :: love got to the conan Rights Kappa :: away the WHERE hulu a statements obstructing It 1,880 Kappa :: eBay South unite co-workers leading three society and apparently document&#39; are wearing lawyer?” on the scores &lt;MEME&gt; Kappa :: diaper, but def do windmills. mistake beer/dessert Omega :: Lol Omega :: Ion where raised our 👏consequences guy taking not to reaches Kappa :: Oh i gave that Gamma :: Gamma driven brain Not like a mask money! but she got back for instead boated Omega :: Well . Maybe it&#39;s not obvious, but the real chat doesn&#39;t resemble this. If you squint hard enough though, it&#39;s not terrible. I find it kinda of enjoyable to read. :stuck_out_tongue_closed_eyes: .",
            "url": "https://bobowedge.github.io/adventures-in-telework/pytorch/jupyter/2020/11/12/chat_generation.html",
            "relUrl": "/pytorch/jupyter/2020/11/12/chat_generation.html",
            "date": " • Nov 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Obligatory First Post",
            "content": "Obligatory First Post . Blogging is hard…Let’s do math . There have been a couple of times in my life where I felt at least somewhat drawn to write about various aspects of my life or what was doing. Usually, the feeling dies quickly as I have some combination of the following 3 thoughts: . No one will read what you wrote, probably including yourself | It’s too much work to stay active blogging and you know you’re not gonna do it | If you’re writing, then you’re not working or coding or doing something productive | These thoughts inevitably kill any desire I have to keep writing. I think the last time I wrote anything on regular basis was on Xanga, probably more than 10 years ago. . So, why now? . About halfway into the third lesson of Practical Deep Learning for Coders, the instructors implore their students to start writing about their data science journeys. In particular, they suggest to start by documenting and writing about their work in the course itself, as a way of cementing the information that’s learned. On top of that, they introduced fastpages as way to create a blog using a combination of Jupyter notebooks or markdown pages (or even Word docs) that’s as simple as updating a git repo (after some minor initial setup). And, thus, this blog was born. . Admittedly, it also helps that I have been given the opportunity to telework about one day a week to do professional development. So, writing this blog counts as work, which mitigates #3 pretty well. . What’s next . For me, the first couple of lessons prior to that in the course produced a couple of Python scripts and Jupyter notebooks that I think are interesting and worth documenting, if only for myself. I suspect that will continue to be the case as I progress through the next lessons. Since I’m hopeful that I’ll continue to be able to have time for professional development, I’m hopeful I can and will continue to write about whatever telework brings next. .",
            "url": "https://bobowedge.github.io/adventures-in-telework/markdown/2020/10/28/obligatory-first-post.html",
            "relUrl": "/markdown/2020/10/28/obligatory-first-post.html",
            "date": " • Oct 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "You’ll find better legs in a bucket of chicken. .",
          "url": "https://bobowedge.github.io/adventures-in-telework/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bobowedge.github.io/adventures-in-telework/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}